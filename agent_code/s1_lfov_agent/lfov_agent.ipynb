{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bbe88005-be9f-46e7-8600-019ed0bc7dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "sys.path.append(r\"../..\")\n",
    "\n",
    "\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from callbacks import state_to_features, ACTION_MAP, ACTION_MAP_INV\n",
    "from networks import AgentNet\n",
    "\n",
    "from events import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "198f4ee7-8582-4580-8cb9-fbccd2d77655",
   "metadata": {},
   "outputs": [],
   "source": [
    "EVENT_HORIZON = {\n",
    "    MOVED_LEFT: 1,\n",
    "    MOVED_RIGHT: 1,\n",
    "    MOVED_UP: 1,\n",
    "    MOVED_DOWN: 1,\n",
    "    WAITED: 2,\n",
    "    INVALID_ACTION: 1,\n",
    "    BOMB_DROPPED: 2,\n",
    "    BOMB_EXPLODED: 2,\n",
    "    CRATE_DESTROYED: 8,\n",
    "    COIN_FOUND: 3,\n",
    "    COIN_COLLECTED: 5,\n",
    "    KILLED_OPPONENT: 8,\n",
    "    KILLED_SELF: 3,\n",
    "    GOT_KILLED: 3,\n",
    "    OPPONENT_ELIMINATED: 0,\n",
    "    SURVIVED_ROUND: 8\n",
    "}\n",
    "\"\"\"\n",
    "REWARDS = {\n",
    "    MOVED_LEFT: 1,\n",
    "    MOVED_RIGHT: 1,\n",
    "    MOVED_UP: 1,\n",
    "    MOVED_DOWN: 1,\n",
    "    WAITED: 0,\n",
    "    INVALID_ACTION: -5,\n",
    "    BOMB_DROPPED: 1,\n",
    "    BOMB_EXPLODED: 0,\n",
    "    CRATE_DESTROYED: 4,\n",
    "    COIN_FOUND: 0,\n",
    "    COIN_COLLECTED: 5,\n",
    "    KILLED_OPPONENT: 5,\n",
    "    KILLED_SELF: -8,\n",
    "    GOT_KILLED: -8,\n",
    "    OPPONENT_ELIMINATED: 0,\n",
    "    SURVIVED_ROUND: 0\n",
    "}\n",
    "\"\"\"\n",
    "REWARDS = {\n",
    "    MOVED_LEFT: 100,\n",
    "    MOVED_RIGHT: 100,\n",
    "    MOVED_UP: 100,\n",
    "    MOVED_DOWN: 100,\n",
    "    WAITED: -100,\n",
    "    INVALID_ACTION: -10,\n",
    "    BOMB_DROPPED: 0,\n",
    "    BOMB_EXPLODED: 0,\n",
    "    CRATE_DESTROYED: 0,\n",
    "    COIN_FOUND: 0,\n",
    "    COIN_COLLECTED: 0,\n",
    "    KILLED_OPPONENT: 0,\n",
    "    KILLED_SELF: 0,\n",
    "    GOT_KILLED: 0,\n",
    "    OPPONENT_ELIMINATED: 0,\n",
    "    SURVIVED_ROUND: 0\n",
    "}\n",
    "\n",
    "# REWARDS = {k: v + 10 for k, v in REWARDS.items()}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7369f91a-b21e-407c-bced-63e441be02a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class BombermanDataset(Dataset):\n",
    "    def __init__(self, states_dir, max_game_step=200, min_reward=1):\n",
    "        self.features = []\n",
    "        self.actions = []\n",
    "        self.cum_rewards = []\n",
    "        self.total_number = 0\n",
    "        for states_file in os.listdir(states_dir):\n",
    "\n",
    "            if not states_file.endswith('.pickle'):\n",
    "                continue\n",
    "            with open(os.path.join(states_dir, states_file), \"rb\") as f:\n",
    "                data = pickle.load(f)\n",
    "            self.total_number += len(data['game_state'])\n",
    "            last_round = -1\n",
    "            running_horizons = []\n",
    "            running_rewards = []\n",
    "            for i in range(len(data['game_state'])-1,-1,-1):\n",
    "                game_state = data['game_state'][i]               \n",
    "\n",
    "                    \n",
    "                if last_round != game_state['round']:\n",
    "                    last_round = game_state['round']\n",
    "                    running_horizons = []\n",
    "                    running_rewards = []\n",
    "                if game_state['step'] > max_game_step:\n",
    "                    continue\n",
    "                    \n",
    "                # determine if action should be included in training set\n",
    "                \n",
    "                horizons = [EVENT_HORIZON[e] for e in data['events'][i]]\n",
    "                rewards = [REWARDS[e] for e in data['events'][i]]\n",
    "                cr = np.where(data['events'] == \"CRATE_DESTROYED\")\n",
    "                #print(cr)\n",
    "                #rewards[np.array(cr[-1:], dtype = \"int\")] == 0\n",
    "                #for j in cr[1:]:\n",
    "                #    rewards[j] = 0             \n",
    "\n",
    "                running_rewards = [r for j,r in enumerate(running_rewards) if running_horizons[j]>1]\n",
    "                running_horizons = [h-1 for h in running_horizons if h > 1]\n",
    "                running_rewards.extend(rewards)\n",
    "                running_horizons.extend(horizons)\n",
    "                #if \"INVALID_ACTION\" in data['events'][i]:\n",
    "                #    continue   \n",
    "                self.cum_rewards.append(np.sum(running_rewards))\n",
    "\n",
    "                last_action = None if game_state['step'] == 1 else data['action'][i-1]\n",
    "                #print(\"LAST ACTION:\")\n",
    "                #print(last_action)\n",
    "                \n",
    "                channels, features, act_map = state_to_features(game_state, last_action=last_action)\n",
    "                action = ACTION_MAP[act_map[data['action'][i]]]\n",
    "                self.features.append((\n",
    "                    torch.tensor(channels[np.newaxis], dtype=torch.float),\n",
    "                    torch.tensor(features, dtype=torch.float)\n",
    "                ))\n",
    "                self.actions.append(action)\n",
    "                \n",
    "                    \n",
    "        self.cum_rewards = torch.tensor(self.cum_rewards, dtype=torch.float)\n",
    "        self.n = np.array([np.sum([t == i for t in self.actions]) for i in range(6)])\n",
    "        self.weights = np.array([1/self.n[t] for t in self.actions])\n",
    "        self.proportion_selected = len(self.actions)/self.total_number\n",
    "        print(f\"Loaded {len(self.actions)} actions.\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.features[idx], self.actions[idx], self.cum_rewards[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "944b4018-e9a4-45ce-825b-41f22809126e",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "data_path = f\"../data/\"\n",
    "weight_path = \"models/model_weights_pretrained.pth\"\n",
    "num_epoch_per_run = 1\n",
    "\n",
    "# optimizer = optim.SGD(net.parameters(), momentum=0.9, lr=0.001)\n",
    "net = AgentNet()\n",
    "#net.load_state_dict(torch.load(weight_path))\n",
    "optimizer = optim.AdamW(net.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7b090f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.AdamW(net.parameters(), lr=0.001)\n",
    "batch_size = 128\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a87534bb-9e02-4b62-8053-194d219d0ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "EVENT_HORIZON = {\n",
    "    MOVED_LEFT: 1,\n",
    "    MOVED_RIGHT: 1,\n",
    "    MOVED_UP: 1,\n",
    "    MOVED_DOWN: 1,\n",
    "    WAITED: 1,\n",
    "    INVALID_ACTION: 1,\n",
    "    BOMB_DROPPED: 1,\n",
    "    BOMB_EXPLODED: 1,\n",
    "    CRATE_DESTROYED: 6,\n",
    "    COIN_FOUND: 1,\n",
    "    COIN_COLLECTED: 5,\n",
    "    KILLED_OPPONENT: 6,\n",
    "    KILLED_SELF: 5,\n",
    "    GOT_KILLED: 6,\n",
    "    OPPONENT_ELIMINATED: 4,\n",
    "    SURVIVED_ROUND: 100\n",
    "}\n",
    "\n",
    "\n",
    "REWARDS = {\n",
    "    MOVED_LEFT: 5, #1\n",
    "    MOVED_RIGHT: 5, #1\n",
    "    MOVED_UP: 5,   #1\n",
    "    MOVED_DOWN: 5,  #1\n",
    "    WAITED: 5,  #1\n",
    "    INVALID_ACTION: -120, #-7\n",
    "    BOMB_DROPPED: 5,  #1\n",
    "    BOMB_EXPLODED: 0,\n",
    "    CRATE_DESTROYED: 120,\n",
    "    COIN_FOUND: 0,\n",
    "    COIN_COLLECTED: 120,\n",
    "    KILLED_OPPONENT: 60,\n",
    "    KILLED_SELF: -120, #-12\n",
    "    GOT_KILLED: -120,\n",
    "    OPPONENT_ELIMINATED: 5,\n",
    "    SURVIVED_ROUND: 200 #20\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2f25651b-d431-4812-be1b-1419aeccbf0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [01:02<00:00,  1.57s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rule_based_agent_0    511\n",
      "rule_based_agent_1    478\n",
      "rule_based_agent_2    524\n",
      "rule_based_agent_3    441\n",
      "Loaded 528784 actions.\n",
      "528784 actions in the training set:\n",
      "UP: 19.59% (103597)\n",
      "DOWN: 25.18% (133160)\n",
      "LEFT: 19.49% (103045)\n",
      "RIGHT: 25.55% (135121)\n",
      "BOMB: 8.60% (45464)\n",
      "WAIT: 1.59% (8397)\n",
      "[1, 1,   200] loss: 271.523\n",
      "0.04817089775787467\n",
      "[1, 1,   400] loss: 139.790\n",
      "0.09658386032860299\n",
      "[1, 1,   600] loss: 118.204\n",
      "0.1449968228993313\n",
      "[1, 1] loss: 101.384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:55<00:00,  1.39s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rule_based_agent_0    496\n",
      "rule_based_agent_1    452\n",
      "rule_based_agent_2    465\n",
      "rule_based_agent_3    500\n",
      "Loaded 510991 actions.\n",
      "510991 actions in the training set:\n",
      "UP: 19.38% (99032)\n",
      "DOWN: 25.37% (129626)\n",
      "LEFT: 19.66% (100450)\n",
      "RIGHT: 25.12% (128360)\n",
      "BOMB: 8.68% (44347)\n",
      "WAIT: 1.80% (9176)\n",
      "[2, 1,   200] loss: 118.307\n",
      "0.04984823607460797\n",
      "[2, 1,   400] loss: 103.313\n",
      "0.09994696579783205\n",
      "[2, 1,   600] loss: 93.825\n",
      "0.15004569552105615\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [01:21<00:00,  2.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rule_based_agent_0    463\n",
      "rule_based_agent_1    431\n",
      "rule_based_agent_2    543\n",
      "rule_based_agent_3    486\n",
      "Loaded 514550 actions.\n",
      "514550 actions in the training set:\n",
      "UP: 19.83% (102055)\n",
      "DOWN: 25.25% (129948)\n",
      "LEFT: 19.39% (99784)\n",
      "RIGHT: 25.32% (130278)\n",
      "BOMB: 8.43% (43397)\n",
      "WAIT: 1.77% (9088)\n",
      "[3, 1,   200] loss: 111.844\n",
      "0.04950344961616947\n",
      "[3, 1,   400] loss: 98.572\n",
      "0.09925566028568653\n",
      "[3, 1,   600] loss: 89.211\n",
      "0.14900787095520357\n",
      "[3, 1] loss: 97.197\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [01:25<00:00,  2.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rule_based_agent_0    475\n",
      "rule_based_agent_1    509\n",
      "rule_based_agent_2    465\n",
      "rule_based_agent_3    469\n",
      "Loaded 504696 actions.\n",
      "504696 actions in the training set:\n",
      "UP: 19.47% (98248)\n",
      "DOWN: 25.24% (127391)\n",
      "LEFT: 19.55% (98655)\n",
      "RIGHT: 25.46% (128490)\n",
      "BOMB: 8.58% (43321)\n",
      "WAIT: 1.70% (8591)\n",
      "[4, 1,   200] loss: 102.938\n",
      "0.05046998589249766\n",
      "[4, 1,   400] loss: 85.305\n",
      "0.10119358980455562\n",
      "[4, 1] loss: 78.836\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [01:24<00:00,  2.11s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rule_based_agent_0    485\n",
      "rule_based_agent_1    480\n",
      "rule_based_agent_2    499\n",
      "rule_based_agent_3    485\n",
      "Loaded 558522 actions.\n",
      "558522 actions in the training set:\n",
      "UP: 19.53% (109078)\n",
      "DOWN: 24.93% (139254)\n",
      "LEFT: 20.04% (111904)\n",
      "RIGHT: 25.29% (141255)\n",
      "BOMB: 8.64% (48266)\n",
      "WAIT: 1.57% (8765)\n",
      "[5, 1,   200] loss: 102.070\n",
      "0.045606081765803316\n",
      "[5, 1,   400] loss: 88.812\n",
      "0.09144133982188705\n",
      "[5, 1,   600] loss: 76.876\n",
      "0.13727659787797078\n",
      "[5, 1] loss: 78.121\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:59<00:00,  1.49s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rule_based_agent_0    464\n",
      "rule_based_agent_1    459\n",
      "rule_based_agent_2    506\n",
      "rule_based_agent_3    523\n",
      "Loaded 512145 actions.\n",
      "512145 actions in the training set:\n",
      "UP: 19.51% (99945)\n",
      "DOWN: 24.79% (126978)\n",
      "LEFT: 19.71% (100964)\n",
      "RIGHT: 25.68% (131542)\n",
      "BOMB: 8.54% (43762)\n",
      "WAIT: 1.75% (8954)\n",
      "[6, 1,   200] loss: 101.376\n",
      "0.049735914633551044\n",
      "[6, 1,   400] loss: 84.654\n",
      "0.09972175848636616\n",
      "[6, 1,   600] loss: 74.339\n",
      "0.1497076023391813\n",
      "[6, 1] loss: 70.495\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:57<00:00,  1.44s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rule_based_agent_0    483\n",
      "rule_based_agent_1    509\n",
      "rule_based_agent_2    508\n",
      "rule_based_agent_3    419\n",
      "Loaded 540884 actions.\n",
      "540884 actions in the training set:\n",
      "UP: 19.56% (105783)\n",
      "DOWN: 25.20% (136326)\n",
      "LEFT: 19.71% (106607)\n",
      "RIGHT: 25.17% (136141)\n",
      "BOMB: 8.74% (47280)\n",
      "WAIT: 1.62% (8747)\n",
      "[7, 1,   200] loss: 93.473\n",
      "0.04709327693183751\n",
      "[7, 1,   400] loss: 78.871\n",
      "0.09442320349649833\n",
      "[7, 1,   600] loss: 69.165\n",
      "0.14175313006115914\n",
      "[7, 1] loss: 66.582\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:55<00:00,  1.39s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rule_based_agent_0    486\n",
      "rule_based_agent_1    449\n",
      "rule_based_agent_2    517\n",
      "rule_based_agent_3    467\n",
      "Loaded 514745 actions.\n",
      "514745 actions in the training set:\n",
      "UP: 19.56% (100665)\n",
      "DOWN: 24.95% (128407)\n",
      "LEFT: 19.82% (102026)\n",
      "RIGHT: 25.38% (130651)\n",
      "BOMB: 8.68% (44685)\n",
      "WAIT: 1.61% (8311)\n",
      "[8, 1,   200] loss: 95.278\n",
      "0.049484696305937895\n",
      "[8, 1,   400] loss: 78.088\n",
      "0.09921805942748352\n",
      "[8, 1,   600] loss: 74.317\n",
      "0.14895142254902913\n",
      "[8, 1] loss: 70.503\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:55<00:00,  1.39s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rule_based_agent_0    544\n",
      "rule_based_agent_1    478\n",
      "rule_based_agent_2    410\n",
      "rule_based_agent_3    468\n",
      "Loaded 513918 actions.\n",
      "513918 actions in the training set:\n",
      "UP: 19.56% (100499)\n",
      "DOWN: 25.49% (131002)\n",
      "LEFT: 19.46% (99996)\n",
      "RIGHT: 25.32% (130138)\n",
      "BOMB: 8.53% (43848)\n",
      "WAIT: 1.64% (8435)\n",
      "[9, 1,   200] loss: 90.847\n",
      "0.04956432738296771\n",
      "[9, 1,   400] loss: 75.577\n",
      "0.09937772173770913\n",
      "[9, 1,   600] loss: 67.661\n",
      "0.14919111609245056\n",
      "[9, 1] loss: 46.900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:54<00:00,  1.37s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rule_based_agent_0    501\n",
      "rule_based_agent_1    517\n",
      "rule_based_agent_2    522\n",
      "rule_based_agent_3    440\n",
      "Loaded 514822 actions.\n",
      "514822 actions in the training set:\n",
      "UP: 19.57% (100732)\n",
      "DOWN: 24.95% (128455)\n",
      "LEFT: 19.59% (100868)\n",
      "RIGHT: 25.62% (131877)\n",
      "BOMB: 8.60% (44295)\n",
      "WAIT: 1.67% (8595)\n",
      "[10, 1,   200] loss: 92.429\n",
      "0.04947729506509046\n",
      "[10, 1,   400] loss: 75.591\n",
      "0.09920321975362359\n",
      "[10, 1,   600] loss: 66.345\n",
      "0.1489291444421567\n",
      "[10, 1] loss: 70.222\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "for run in range(10):\n",
    "    # os.system(\"rm -rf ../rule_based_agent/data\")\n",
    "    os.system(\"rm -rf ../data\")\n",
    "    os.system(\"cd ../..; python main.py play --no-gui --agents rule_based_agent rule_based_agent rule_based_agent rule_based_agent --train 4 --n-rounds 40 --scenario loot-crate\")\n",
    "\n",
    "    trainset = BombermanDataset(data_path)\n",
    "    N = len(trainset)\n",
    "    print(f\"{N} actions in the training set:\")\n",
    "    for i in range(6):\n",
    "        n = np.sum([t == i for t in trainset.actions])\n",
    "        print(f\"{ACTION_MAP_INV[i]}: {n/N*100:.2f}% ({n})\")\n",
    "    # sampler = BatchSampler(WeightedRandomSampler(trainset.weights, len(trainset), replacement=True,), batch_size, False)\n",
    "    # trainloader = torch.utils.data.DataLoader(trainset, batch_sampler=sampler)\n",
    "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    for epoch in range(num_epoch_per_run):\n",
    "        running_loss = 0.0\n",
    "        running = 0\n",
    "        for i, batch in enumerate(trainloader, 0):\n",
    "        # for i, batch in enumerate(sampler, 0):\n",
    "            print(i, end=\"\\r\")\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "            (channels, features), actions, rewards = batch\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "    \n",
    "            # forward + backward + optimize\n",
    "            outputs = net(channels, features)\n",
    "            # loss = criterion(outputs, labels)\n",
    "            log_logits = F.log_softmax(outputs, dim=-1)\n",
    "            log_probs = log_logits.gather(1, actions[:,np.newaxis])\n",
    "            \n",
    "            loss = torch.mean(-log_probs * rewards)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "            # print statistics\n",
    "            running_loss += loss.item()\n",
    "            running += 1\n",
    "            if i % 200 == 199:    # print every 2000 mini-batches\n",
    "                print(f'[{run + 1}, {epoch + 1}, {i + 1:5d}] loss: {running_loss / running:.3f}')\n",
    "                torch.save(net.state_dict(), weight_path)\n",
    "                running_loss = 0.0\n",
    "                running = 0\n",
    "                #print(log_probs)\n",
    "                #print(rewards)\n",
    "                print(i*batch_size/N)\n",
    "            if i*batch_size/N > 0.15:\n",
    "                break\n",
    "        if running > 0:\n",
    "            print(f'[{run + 1}, {epoch + 1}] loss: {running_loss / running:.3f}')\n",
    "            \n",
    "    torch.save(net.state_dict(), weight_path)\n",
    "\n",
    "print('Finished Training') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "500a7874",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(net.state_dict(), \"models/model_weights_pre\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b32258df",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.AdamW(net.parameters(), lr=0.0001)\n",
    "batch_size = 256\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9d7f345c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [01:14<00:00,  1.49s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rule_based_agent_0    603\n",
      "rule_based_agent_1    586\n",
      "rule_based_agent_2    599\n",
      "rule_based_agent_3    588\n",
      "Loaded 822204 actions.\n",
      "822204 actions in the training set:\n",
      "UP: 19.61% (161249)\n",
      "DOWN: 25.23% (207447)\n",
      "LEFT: 19.47% (160121)\n",
      "RIGHT: 25.35% (208467)\n",
      "BOMB: 8.63% (70983)\n",
      "WAIT: 1.70% (13937)\n",
      "[1, 1,   200] loss: 91.364\n",
      "0.0619602920929599\n",
      "[1, 1,   400] loss: 81.923\n",
      "0.12423194243764321\n",
      "[1, 1,   600] loss: 77.439\n",
      "0.18650359278232653\n",
      "[1, 1,   800] loss: 71.511\n",
      "0.24877524312700985\n",
      "[1, 1] loss: 72.563\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [01:24<00:00,  1.70s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rule_based_agent_0    578\n",
      "rule_based_agent_1    541\n",
      "rule_based_agent_2    606\n",
      "rule_based_agent_3    654\n",
      "Loaded 847121 actions.\n",
      "847121 actions in the training set:\n",
      "UP: 19.25% (163068)\n",
      "DOWN: 24.86% (210609)\n",
      "LEFT: 20.00% (169398)\n",
      "RIGHT: 25.75% (218125)\n",
      "BOMB: 8.55% (72397)\n",
      "WAIT: 1.60% (13524)\n",
      "[2, 1,   200] loss: 92.761\n",
      "0.060137807940069954\n",
      "[2, 1,   400] loss: 85.659\n",
      "0.12057781592003976\n",
      "[2, 1,   600] loss: 79.833\n",
      "0.18101782390000956\n",
      "[2, 1,   800] loss: 73.716\n",
      "0.24145783187997938\n",
      "[2, 1] loss: 72.354\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [01:13<00:00,  1.47s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rule_based_agent_0    642\n",
      "rule_based_agent_1    557\n",
      "rule_based_agent_2    620\n",
      "rule_based_agent_3    573\n",
      "Loaded 776815 actions.\n",
      "776815 actions in the training set:\n",
      "UP: 19.80% (153819)\n",
      "DOWN: 25.63% (199062)\n",
      "LEFT: 19.27% (149729)\n",
      "RIGHT: 25.26% (196186)\n",
      "BOMB: 8.49% (65931)\n",
      "WAIT: 1.56% (12088)\n",
      "[3, 1,   200] loss: 87.313\n",
      "0.06558060799546868\n",
      "[3, 1,   400] loss: 75.107\n",
      "0.13149076678488444\n",
      "[3, 1,   600] loss: 70.318\n",
      "0.1974009255743002\n",
      "[3, 1] loss: 65.128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [01:40<00:00,  2.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rule_based_agent_0    576\n",
      "rule_based_agent_1    663\n",
      "rule_based_agent_2    548\n",
      "rule_based_agent_3    596\n",
      "Loaded 808447 actions.\n",
      "808447 actions in the training set:\n",
      "UP: 19.70% (159271)\n",
      "DOWN: 25.35% (204949)\n",
      "LEFT: 19.62% (158614)\n",
      "RIGHT: 25.23% (203992)\n",
      "BOMB: 8.54% (69059)\n",
      "WAIT: 1.55% (12562)\n",
      "[4, 1,   200] loss: 86.266\n",
      "0.0630146441263311\n",
      "[4, 1,   400] loss: 78.057\n",
      "0.1263459447558096\n",
      "[4, 1,   600] loss: 73.214\n",
      "0.1896772453852881\n",
      "[4, 1] loss: 67.609\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [01:26<00:00,  1.73s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rule_based_agent_0    611\n",
      "rule_based_agent_1    636\n",
      "rule_based_agent_2    612\n",
      "rule_based_agent_3    626\n",
      "Loaded 855260 actions.\n",
      "855260 actions in the training set:\n",
      "UP: 19.65% (168075)\n",
      "DOWN: 25.45% (217650)\n",
      "LEFT: 19.52% (166973)\n",
      "RIGHT: 25.26% (216041)\n",
      "BOMB: 8.61% (73598)\n",
      "WAIT: 1.51% (12923)\n",
      "[5, 1,   200] loss: 91.453\n",
      "0.059565512241891355\n",
      "[5, 1,   400] loss: 81.927\n",
      "0.11943034866590277\n",
      "[5, 1,   600] loss: 75.920\n",
      "0.17929518508991418\n",
      "[5, 1,   800] loss: 70.973\n",
      "0.2391600215139256\n",
      "[5, 1] loss: 65.879\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [01:12<00:00,  1.44s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rule_based_agent_0    561\n",
      "rule_based_agent_1    633\n",
      "rule_based_agent_2    515\n",
      "rule_based_agent_3    664\n",
      "Loaded 803166 actions.\n",
      "803166 actions in the training set:\n",
      "UP: 19.76% (158744)\n",
      "DOWN: 25.76% (206911)\n",
      "LEFT: 19.46% (156305)\n",
      "RIGHT: 24.79% (199102)\n",
      "BOMB: 8.60% (69112)\n",
      "WAIT: 1.62% (12992)\n",
      "[6, 1,   200] loss: 86.538\n",
      "0.06342897981239246\n",
      "[6, 1,   400] loss: 76.485\n",
      "0.127176698216807\n",
      "[6, 1,   600] loss: 69.810\n",
      "0.1909244166212215\n",
      "[6, 1] loss: 64.309\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [01:29<00:00,  1.79s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rule_based_agent_0    594\n",
      "rule_based_agent_1    638\n",
      "rule_based_agent_2    623\n",
      "rule_based_agent_3    621\n",
      "Loaded 822476 actions.\n",
      "822476 actions in the training set:\n",
      "UP: 19.59% (161156)\n",
      "DOWN: 25.21% (207337)\n",
      "LEFT: 19.50% (160423)\n",
      "RIGHT: 25.30% (208061)\n",
      "BOMB: 8.54% (70270)\n",
      "WAIT: 1.85% (15229)\n",
      "[7, 1,   200] loss: 81.298\n",
      "0.06193980128295537\n",
      "[7, 1,   400] loss: 71.906\n",
      "0.12419085784873966\n",
      "[7, 1,   600] loss: 66.140\n",
      "0.18644191441452396\n",
      "[7, 1,   800] loss: 60.735\n",
      "0.24869297098030824\n",
      "[7, 1] loss: 56.948\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [01:20<00:00,  1.61s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rule_based_agent_0    587\n",
      "rule_based_agent_1    554\n",
      "rule_based_agent_2    631\n",
      "rule_based_agent_3    622\n",
      "Loaded 806193 actions.\n",
      "806193 actions in the training set:\n",
      "UP: 19.45% (156790)\n",
      "DOWN: 24.97% (201276)\n",
      "LEFT: 19.68% (158657)\n",
      "RIGHT: 25.56% (206035)\n",
      "BOMB: 8.65% (69774)\n",
      "WAIT: 1.69% (13661)\n",
      "[8, 1,   200] loss: 84.761\n",
      "0.06319082403345104\n",
      "[8, 1,   400] loss: 74.705\n",
      "0.12669918989621592\n",
      "[8, 1,   600] loss: 67.596\n",
      "0.1902075557589808\n",
      "[8, 1] loss: 62.399\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [01:20<00:00,  1.62s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rule_based_agent_0    585\n",
      "rule_based_agent_1    620\n",
      "rule_based_agent_2    542\n",
      "rule_based_agent_3    646\n",
      "Loaded 846507 actions.\n",
      "846507 actions in the training set:\n",
      "UP: 19.62% (166055)\n",
      "DOWN: 25.47% (215599)\n",
      "LEFT: 19.61% (165966)\n",
      "RIGHT: 25.07% (212243)\n",
      "BOMB: 8.57% (72534)\n",
      "WAIT: 1.67% (14110)\n",
      "[9, 1,   200] loss: 87.836\n",
      "0.06018142791494931\n",
      "[9, 1,   400] loss: 76.854\n",
      "0.12066527506565214\n",
      "[9, 1,   600] loss: 70.995\n",
      "0.18114912221635499\n",
      "[9, 1,   800] loss: 65.990\n",
      "0.2416329693670578\n",
      "[9, 1] loss: 63.369\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [01:23<00:00,  1.67s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rule_based_agent_0    550\n",
      "rule_based_agent_1    601\n",
      "rule_based_agent_2    640\n",
      "rule_based_agent_3    678\n",
      "Loaded 821429 actions.\n",
      "821429 actions in the training set:\n",
      "UP: 19.63% (161236)\n",
      "DOWN: 25.13% (206458)\n",
      "LEFT: 19.76% (162351)\n",
      "RIGHT: 25.36% (208325)\n",
      "BOMB: 8.54% (70137)\n",
      "WAIT: 1.57% (12922)\n",
      "[10, 1,   200] loss: 83.702\n",
      "0.06201875025108682\n",
      "[10, 1,   400] loss: 75.458\n",
      "0.12434915251348565\n",
      "[10, 1,   600] loss: 68.025\n",
      "0.18667955477588447\n",
      "[10, 1,   800] loss: 61.395\n",
      "0.2490099570382833\n",
      "[10, 1] loss: 57.100\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "for run in range(10):\n",
    "    # os.system(\"rm -rf ../rule_based_agent/data\")\n",
    "    os.system(\"rm -rf ../data\")\n",
    "    os.system(\"cd ../..; python main.py play --no-gui --agents rule_based_agent rule_based_agent rule_based_agent rule_based_agent --train 4 --n-rounds 50 --scenario loot-crate\")\n",
    "\n",
    "    trainset = BombermanDataset(data_path)\n",
    "    N = len(trainset)\n",
    "    print(f\"{N} actions in the training set:\")\n",
    "    for i in range(6):\n",
    "        n = np.sum([t == i for t in trainset.actions])\n",
    "        print(f\"{ACTION_MAP_INV[i]}: {n/N*100:.2f}% ({n})\")\n",
    "    # sampler = BatchSampler(WeightedRandomSampler(trainset.weights, len(trainset), replacement=True,), batch_size, False)\n",
    "    # trainloader = torch.utils.data.DataLoader(trainset, batch_sampler=sampler)\n",
    "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    for epoch in range(num_epoch_per_run):\n",
    "        running_loss = 0.0\n",
    "        running = 0\n",
    "        for i, batch in enumerate(trainloader, 0):\n",
    "        # for i, batch in enumerate(sampler, 0):\n",
    "            print(i, end=\"\\r\")\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "            (channels, features), actions, rewards = batch\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "    \n",
    "            # forward + backward + optimize\n",
    "            outputs = net(channels, features)\n",
    "            # loss = criterion(outputs, labels)\n",
    "            log_logits = F.log_softmax(outputs, dim=-1)\n",
    "            log_probs = log_logits.gather(1, actions[:,np.newaxis])\n",
    "            \n",
    "            loss = torch.mean(-log_probs * rewards)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "            # print statistics\n",
    "            running_loss += loss.item()\n",
    "            running += 1\n",
    "            if i % 200 == 199:    # print every 2000 mini-batches\n",
    "                print(f'[{run + 1}, {epoch + 1}, {i + 1:5d}] loss: {running_loss / running:.3f}')\n",
    "                torch.save(net.state_dict(), weight_path)\n",
    "                running_loss = 0.0\n",
    "                running = 0\n",
    "                #print(log_probs)\n",
    "                #print(rewards)\n",
    "                print(i*batch_size/N)\n",
    "            if i*batch_size/N > 0.25:\n",
    "                break\n",
    "        if running > 0:\n",
    "            print(f'[{run + 1}, {epoch + 1}] loss: {running_loss / running:.3f}')\n",
    "            \n",
    "    torch.save(net.state_dict(), weight_path)\n",
    "\n",
    "print('Finished Training') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c0857423",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(net.state_dict(), \"models/model_weights_fine\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "46cab078",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path = f\"../data/\"\n",
    "weight_path = \"models/model_weights.pth\"\n",
    "num_epoch_per_run = 1\n",
    "\n",
    "# optimizer = optim.SGD(net.parameters(), momentum=0.9, lr=0.001)\n",
    "net = AgentNet()\n",
    "net.load_state_dict(torch.load(\"models/model_weights_fine\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fe98dd07",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.AdamW(net.parameters(), lr=0.00001)\n",
    "batch_size = 256\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0231f6b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60/60 [01:28<00:00,  1.47s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lfov_agent    690\n",
      "rule_based_agent_0    767\n",
      "rule_based_agent_1    785\n",
      "rule_based_agent_2    686\n",
      "Loaded 296379 actions.\n",
      "296379 actions in the training set:\n",
      "UP: 19.33% (57281)\n",
      "DOWN: 25.83% (76543)\n",
      "LEFT: 19.53% (57892)\n",
      "RIGHT: 25.42% (75331)\n",
      "BOMB: 8.62% (25541)\n",
      "WAIT: 1.28% (3791)\n",
      "[1, 1,   200] loss: 93.594\n",
      "0.17188802175592738\n",
      "[1, 1,   400] loss: 88.953\n",
      "0.34463980241515085\n",
      "[1, 1] loss: 84.614\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60/60 [01:26<00:00,  1.44s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lfov_agent    648\n",
      "rule_based_agent_0    698\n",
      "rule_based_agent_1    720\n",
      "rule_based_agent_2    861\n",
      "Loaded 300310 actions.\n",
      "300310 actions in the training set:\n",
      "UP: 19.39% (58231)\n",
      "DOWN: 25.46% (76446)\n",
      "LEFT: 19.64% (58991)\n",
      "RIGHT: 25.81% (77515)\n",
      "BOMB: 8.38% (25161)\n",
      "WAIT: 1.32% (3966)\n",
      "[2, 1,   200] loss: 83.185\n",
      "0.16963804069128569\n",
      "[2, 1,   400] loss: 82.310\n",
      "0.3401285338483567\n",
      "[2, 1] loss: 80.394\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60/60 [01:29<00:00,  1.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lfov_agent    692\n",
      "rule_based_agent_0    672\n",
      "rule_based_agent_1    773\n",
      "rule_based_agent_2    725\n",
      "Loaded 299652 actions.\n",
      "299652 actions in the training set:\n",
      "UP: 19.78% (59261)\n",
      "DOWN: 26.03% (77998)\n",
      "LEFT: 19.31% (57863)\n",
      "RIGHT: 25.19% (75488)\n",
      "BOMB: 8.37% (25083)\n",
      "WAIT: 1.32% (3959)\n",
      "[3, 1,   200] loss: 84.440\n",
      "0.1700105455661901\n",
      "[3, 1,   400] loss: 84.410\n",
      "0.3408754154819591\n",
      "[3, 1] loss: 82.549\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60/60 [01:29<00:00,  1.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lfov_agent    681\n",
      "rule_based_agent_0    761\n",
      "rule_based_agent_1    723\n",
      "rule_based_agent_2    739\n",
      "Loaded 301606 actions.\n",
      "301606 actions in the training set:\n",
      "UP: 19.20% (57910)\n",
      "DOWN: 26.08% (78645)\n",
      "LEFT: 19.48% (58754)\n",
      "RIGHT: 25.50% (76914)\n",
      "BOMB: 8.40% (25349)\n",
      "WAIT: 1.34% (4034)\n",
      "[4, 1,   200] loss: 79.566\n",
      "0.1689091065827603\n",
      "[4, 1,   400] loss: 79.261\n",
      "0.33866700264583594\n",
      "[4, 1] loss: 78.342\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60/60 [01:26<00:00,  1.44s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lfov_agent    610\n",
      "rule_based_agent_0    738\n",
      "rule_based_agent_1    769\n",
      "rule_based_agent_2    788\n",
      "Loaded 292092 actions.\n",
      "292092 actions in the training set:\n",
      "UP: 19.22% (56143)\n",
      "DOWN: 25.78% (75299)\n",
      "LEFT: 19.53% (57031)\n",
      "RIGHT: 25.73% (75161)\n",
      "BOMB: 8.42% (24591)\n",
      "WAIT: 1.32% (3867)\n",
      "[5, 1,   200] loss: 80.779\n",
      "0.17441080207605822\n",
      "[5, 1,   400] loss: 78.057\n",
      "0.3496980403434534\n",
      "[5, 1] loss: 78.392\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60/60 [01:26<00:00,  1.44s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lfov_agent    682\n",
      "rule_based_agent_0    784\n",
      "rule_based_agent_1    673\n",
      "rule_based_agent_2    690\n",
      "Loaded 293564 actions.\n",
      "293564 actions in the training set:\n",
      "UP: 19.55% (57381)\n",
      "DOWN: 26.00% (76336)\n",
      "LEFT: 19.16% (56249)\n",
      "RIGHT: 25.91% (76075)\n",
      "BOMB: 8.13% (23860)\n",
      "WAIT: 1.25% (3663)\n",
      "[6, 1,   200] loss: 82.561\n",
      "0.17353626466460464\n",
      "[6, 1,   400] loss: 80.592\n",
      "0.3479445708601872\n",
      "[6, 1] loss: 78.853\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60/60 [01:25<00:00,  1.43s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lfov_agent    599\n",
      "rule_based_agent_0    745\n",
      "rule_based_agent_1    750\n",
      "rule_based_agent_2    763\n",
      "Loaded 294842 actions.\n",
      "294842 actions in the training set:\n",
      "UP: 19.40% (57195)\n",
      "DOWN: 25.44% (75017)\n",
      "LEFT: 19.49% (57450)\n",
      "RIGHT: 25.82% (76125)\n",
      "BOMB: 8.57% (25274)\n",
      "WAIT: 1.28% (3781)\n",
      "[7, 1,   200] loss: 80.875\n",
      "0.17278406739881022\n",
      "[7, 1,   400] loss: 78.554\n",
      "0.3464363964428406\n",
      "[7, 1] loss: 76.612\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60/60 [01:25<00:00,  1.43s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lfov_agent    620\n",
      "rule_based_agent_0    764\n",
      "rule_based_agent_1    761\n",
      "rule_based_agent_2    723\n",
      "Loaded 287542 actions.\n",
      "287542 actions in the training set:\n",
      "UP: 19.53% (56157)\n",
      "DOWN: 25.68% (73838)\n",
      "LEFT: 19.43% (55881)\n",
      "RIGHT: 25.69% (73877)\n",
      "BOMB: 8.42% (24199)\n",
      "WAIT: 1.25% (3590)\n",
      "[8, 1,   200] loss: 81.135\n",
      "0.1771706394196326\n",
      "[8, 1,   400] loss: 79.417\n",
      "0.35523158355996687\n",
      "[8, 1] loss: 78.882\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60/60 [01:25<00:00,  1.43s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lfov_agent    701\n",
      "rule_based_agent_0    755\n",
      "rule_based_agent_1    704\n",
      "rule_based_agent_2    762\n",
      "Loaded 295418 actions.\n",
      "295418 actions in the training set:\n",
      "UP: 19.54% (57722)\n",
      "DOWN: 26.22% (77453)\n",
      "LEFT: 19.22% (56790)\n",
      "RIGHT: 25.28% (74685)\n",
      "BOMB: 8.35% (24662)\n",
      "WAIT: 1.39% (4106)\n",
      "[9, 1,   200] loss: 80.564\n",
      "0.1724471765430678\n",
      "[9, 1,   400] loss: 78.910\n",
      "0.34576092181248264\n",
      "[9, 1] loss: 77.739\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60/60 [01:29<00:00,  1.50s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lfov_agent    594\n",
      "rule_based_agent_0    686\n",
      "rule_based_agent_1    695\n",
      "rule_based_agent_2    784\n",
      "Loaded 295330 actions.\n",
      "295330 actions in the training set:\n",
      "UP: 19.59% (57856)\n",
      "DOWN: 26.09% (77046)\n",
      "LEFT: 19.32% (57069)\n",
      "RIGHT: 25.27% (74630)\n",
      "BOMB: 8.41% (24828)\n",
      "WAIT: 1.32% (3901)\n",
      "[10, 1,   200] loss: 79.371\n",
      "0.17249856093183896\n",
      "[10, 1,   400] loss: 79.762\n",
      "0.3458639488030339\n",
      "[10, 1] loss: 78.461\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60/60 [01:29<00:00,  1.49s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lfov_agent    682\n",
      "rule_based_agent_0    774\n",
      "rule_based_agent_1    699\n",
      "rule_based_agent_2    743\n",
      "Loaded 293655 actions.\n",
      "293655 actions in the training set:\n",
      "UP: 19.18% (56312)\n",
      "DOWN: 26.15% (76790)\n",
      "LEFT: 19.42% (57034)\n",
      "RIGHT: 25.73% (75546)\n",
      "BOMB: 8.20% (24079)\n",
      "WAIT: 1.33% (3894)\n",
      "[11, 1,   200] loss: 78.615\n",
      "0.17348248795355092\n",
      "[11, 1,   400] loss: 77.862\n",
      "0.3478367472033509\n",
      "[11, 1] loss: 75.530\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60/60 [01:26<00:00,  1.44s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lfov_agent    639\n",
      "rule_based_agent_0    802\n",
      "rule_based_agent_1    667\n",
      "rule_based_agent_2    750\n",
      "Loaded 284726 actions.\n",
      "284726 actions in the training set:\n",
      "UP: 19.17% (54578)\n",
      "DOWN: 26.11% (74338)\n",
      "LEFT: 19.35% (55084)\n",
      "RIGHT: 25.68% (73121)\n",
      "BOMB: 8.41% (23941)\n",
      "WAIT: 1.29% (3664)\n",
      "[12, 1,   200] loss: 76.943\n",
      "0.1789228942913538\n",
      "[12, 1,   400] loss: 75.507\n",
      "0.35874489860427217\n",
      "[12, 1] loss: 75.901\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60/60 [01:30<00:00,  1.51s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lfov_agent    706\n",
      "rule_based_agent_0    761\n",
      "rule_based_agent_1    764\n",
      "rule_based_agent_2    689\n",
      "Loaded 300739 actions.\n",
      "300739 actions in the training set:\n",
      "UP: 19.31% (58075)\n",
      "DOWN: 25.58% (76931)\n",
      "LEFT: 19.64% (59060)\n",
      "RIGHT: 25.80% (77602)\n",
      "BOMB: 8.36% (25148)\n",
      "WAIT: 1.30% (3923)\n",
      "[13, 1,   200] loss: 79.963\n",
      "0.1693960543860291\n",
      "[13, 1,   400] loss: 79.615\n",
      "0.33964334522625933\n",
      "[13, 1] loss: 77.691\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60/60 [01:27<00:00,  1.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lfov_agent    663\n",
      "rule_based_agent_0    707\n",
      "rule_based_agent_1    791\n",
      "rule_based_agent_2    737\n",
      "Loaded 293376 actions.\n",
      "293376 actions in the training set:\n",
      "UP: 19.26% (56514)\n",
      "DOWN: 26.24% (76993)\n",
      "LEFT: 19.28% (56551)\n",
      "RIGHT: 25.59% (75072)\n",
      "BOMB: 8.37% (24554)\n",
      "WAIT: 1.26% (3692)\n",
      "[14, 1,   200] loss: 78.516\n",
      "0.1736474694589878\n",
      "[14, 1,   400] loss: 79.554\n",
      "0.3481675392670157\n",
      "[14, 1] loss: 78.189\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60/60 [01:21<00:00,  1.37s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lfov_agent    660\n",
      "rule_based_agent_0    731\n",
      "rule_based_agent_1    800\n",
      "rule_based_agent_2    718\n",
      "Loaded 283413 actions.\n",
      "283413 actions in the training set:\n",
      "UP: 19.23% (54492)\n",
      "DOWN: 25.57% (72465)\n",
      "LEFT: 19.64% (55670)\n",
      "RIGHT: 25.74% (72962)\n",
      "BOMB: 8.53% (24164)\n",
      "WAIT: 1.29% (3660)\n",
      "[15, 1,   200] loss: 77.611\n",
      "0.1797518109613885\n",
      "[15, 1,   400] loss: 77.159\n",
      "0.3604068973547438\n",
      "[15, 1] loss: 75.116\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60/60 [01:26<00:00,  1.45s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lfov_agent    650\n",
      "rule_based_agent_0    796\n",
      "rule_based_agent_1    742\n",
      "rule_based_agent_2    676\n",
      "Loaded 295542 actions.\n",
      "295542 actions in the training set:\n",
      "UP: 19.71% (58246)\n",
      "DOWN: 25.85% (76405)\n",
      "LEFT: 19.41% (57363)\n",
      "RIGHT: 25.16% (74372)\n",
      "BOMB: 8.58% (25349)\n",
      "WAIT: 1.29% (3807)\n",
      "[16, 1,   200] loss: 77.549\n",
      "0.1723748232061771\n",
      "[16, 1,   400] loss: 76.359\n",
      "0.34561585155409386\n",
      "[16, 1] loss: 76.783\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60/60 [01:28<00:00,  1.47s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lfov_agent    610\n",
      "rule_based_agent_0    711\n",
      "rule_based_agent_1    762\n",
      "rule_based_agent_2    776\n",
      "Loaded 295737 actions.\n",
      "295737 actions in the training set:\n",
      "UP: 19.40% (57376)\n",
      "DOWN: 25.79% (76281)\n",
      "LEFT: 19.65% (58122)\n",
      "RIGHT: 25.57% (75617)\n",
      "BOMB: 8.38% (24769)\n",
      "WAIT: 1.21% (3572)\n",
      "[17, 1,   200] loss: 77.975\n",
      "0.1722611644806027\n",
      "[17, 1,   400] loss: 78.863\n",
      "0.3453879629535702\n",
      "[17, 1] loss: 77.602\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60/60 [01:28<00:00,  1.47s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lfov_agent    668\n",
      "rule_based_agent_0    785\n",
      "rule_based_agent_1    688\n",
      "rule_based_agent_2    702\n",
      "Loaded 284315 actions.\n",
      "284315 actions in the training set:\n",
      "UP: 19.63% (55813)\n",
      "DOWN: 25.98% (73853)\n",
      "LEFT: 19.15% (54452)\n",
      "RIGHT: 25.51% (72520)\n",
      "BOMB: 8.37% (23789)\n",
      "WAIT: 1.37% (3888)\n",
      "[18, 1,   200] loss: 78.883\n",
      "0.17918154159998592\n",
      "[18, 1,   400] loss: 76.349\n",
      "0.3592634929567557\n",
      "[18, 1] loss: 75.507\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60/60 [01:35<00:00,  1.60s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lfov_agent    670\n",
      "rule_based_agent_0    667\n",
      "rule_based_agent_1    726\n",
      "rule_based_agent_2    778\n",
      "Loaded 292050 actions.\n",
      "292050 actions in the training set:\n",
      "UP: 19.28% (56307)\n",
      "DOWN: 26.11% (76243)\n",
      "LEFT: 19.39% (56625)\n",
      "RIGHT: 25.64% (74882)\n",
      "BOMB: 8.35% (24377)\n",
      "WAIT: 1.24% (3616)\n",
      "[19, 1,   200] loss: 78.297\n",
      "0.17443588426639275\n",
      "[19, 1,   400] loss: 77.295\n",
      "0.3497483307652799\n",
      "[19, 1] loss: 77.027\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60/60 [01:26<00:00,  1.45s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lfov_agent    548\n",
      "rule_based_agent_0    750\n",
      "rule_based_agent_1    719\n",
      "rule_based_agent_2    818\n",
      "Loaded 291567 actions.\n",
      "291567 actions in the training set:\n",
      "UP: 19.34% (56386)\n",
      "DOWN: 25.23% (73551)\n",
      "LEFT: 19.92% (58072)\n",
      "RIGHT: 25.85% (75384)\n",
      "BOMB: 8.43% (24581)\n",
      "WAIT: 1.23% (3593)\n",
      "[20, 1,   200] loss: 76.469\n",
      "0.17472484883405873\n",
      "[20, 1,   400] loss: 75.179\n",
      "0.3503277119838665\n",
      "[20, 1] loss: 74.905\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "for run in range(20):\n",
    "    # os.system(\"rm -rf ../rule_based_agent/data\")\n",
    "    os.system(\"rm -rf ../data\")\n",
    "    os.system(\"cd ../..; python main.py play --no-gui --agents lfov_agent rule_based_agent rule_based_agent rule_based_agent --train 4 --n-rounds 60 --scenario loot-crate --save-winner-game True\")\n",
    "\n",
    "    trainset = BombermanDataset(data_path)\n",
    "    N = len(trainset)\n",
    "    print(f\"{N} actions in the training set:\")\n",
    "    for i in range(6):\n",
    "        n = np.sum([t == i for t in trainset.actions])\n",
    "        print(f\"{ACTION_MAP_INV[i]}: {n/N*100:.2f}% ({n})\")\n",
    "    # sampler = BatchSampler(WeightedRandomSampler(trainset.weights, len(trainset), replacement=True,), batch_size, False)\n",
    "    # trainloader = torch.utils.data.DataLoader(trainset, batch_sampler=sampler)\n",
    "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    for epoch in range(num_epoch_per_run):\n",
    "        running_loss = 0.0\n",
    "        running = 0\n",
    "        for i, batch in enumerate(trainloader, 0):\n",
    "        # for i, batch in enumerate(sampler, 0):\n",
    "            print(i, end=\"\\r\")\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "            (channels, features), actions, rewards = batch\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "    \n",
    "            # forward + backward + optimize\n",
    "            outputs = net(channels, features)\n",
    "            # loss = criterion(outputs, labels)\n",
    "            log_logits = F.log_softmax(outputs, dim=-1)\n",
    "            log_probs = log_logits.gather(1, actions[:,np.newaxis])\n",
    "            \n",
    "            loss = torch.mean(-log_probs * rewards)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "            # print statistics\n",
    "            running_loss += loss.item()\n",
    "            running += 1\n",
    "            if i % 200 == 199:    # print every 2000 mini-batches\n",
    "                print(f'[{run + 1}, {epoch + 1}, {i + 1:5d}] loss: {running_loss / running:.3f}')\n",
    "                torch.save(net.state_dict(), weight_path)\n",
    "                running_loss = 0.0\n",
    "                running = 0\n",
    "                #print(log_probs)\n",
    "                #print(rewards)\n",
    "                print(i*batch_size/N)\n",
    "            if i*batch_size/N > 0.5:\n",
    "                break\n",
    "        if running > 0:\n",
    "            print(f'[{run + 1}, {epoch + 1}] loss: {running_loss / running:.3f}')\n",
    "            \n",
    "    torch.save(net.state_dict(), weight_path)\n",
    "\n",
    "print('Finished Training') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "25463da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(net.state_dict(), \"models/model_weights_win.pth\")\n",
    "\n",
    "\n",
    "#weight_path_ref = \"models/model_weights_pretrained_ref.pth\"\n",
    "#torch.save(net.state_dict(),weight_path_ref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "0136c04c-7b35-43b8-a5e1-9a83f30dc43c",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "data_path = f\"../data/\"\n",
    "weight_path = \"models/model_weights.pth\"\n",
    "pretrained_weight_path = \"models/model_weights_pretrained.pth\"\n",
    "num_epoch_per_run = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "5ab0b0cd-1b41-460c-b3d5-bfb026b34e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pretrained_net = AgentNet()\n",
    "# pretrained_net.load_state_dict(torch.load(pretrained_weight_path))\n",
    "# torch.save(pretrained_net.state_dict(), weigth_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "7f5db558-2318-4b20-8385-a775144067cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = AgentNet()\n",
    "pretrained_net = AgentNet()\n",
    "pretrained_net.load_state_dict(torch.load(pretrained_weight_path))\n",
    "net.cnn.load_state_dict(pretrained_net.cnn.state_dict())\n",
    "torch.save(net.state_dict(), weight_path)\n",
    "optimizer = optim.AdamW(net.mlp.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "4fea4551-3f68-4965-bf24-adca6abf13f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.AdamW(net.mlp.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e02d0f",
   "metadata": {},
   "source": [
    "EVENT_HORIZON = {\n",
    "    MOVED_LEFT: 1,\n",
    "    MOVED_RIGHT: 1,\n",
    "    MOVED_UP: 1,\n",
    "    MOVED_DOWN: 1,\n",
    "    WAITED: 1,\n",
    "    INVALID_ACTION: 1,\n",
    "    BOMB_DROPPED: 1,\n",
    "    BOMB_EXPLODED: 1,\n",
    "    CRATE_DESTROYED: 5,\n",
    "    COIN_FOUND: 3,\n",
    "    COIN_COLLECTED: 8,\n",
    "    KILLED_OPPONENT: 8,\n",
    "    KILLED_SELF: 4,\n",
    "    GOT_KILLED: 3,\n",
    "    OPPONENT_ELIMINATED: 7,\n",
    "    SURVIVED_ROUND: 5\n",
    "}\n",
    "\n",
    "\n",
    "REWARDS = {\n",
    "    MOVED_LEFT: -0.5,  #-0.2\n",
    "    MOVED_RIGHT: -0.5, #-0.2\n",
    "    MOVED_UP: -0.5,  #-0.2\n",
    "    MOVED_DOWN: -0.5,  #-0.2    \n",
    "    WAITED: 4,\n",
    "    INVALID_ACTION: -2,\n",
    "    BOMB_DROPPED: 0,\n",
    "    BOMB_EXPLODED: 1,\n",
    "    CRATE_DESTROYED: 2,\n",
    "    COIN_FOUND: 0,\n",
    "    COIN_COLLECTED: 6,\n",
    "    KILLED_OPPONENT: 1,\n",
    "    KILLED_SELF: -10,\n",
    "    GOT_KILLED: -1,\n",
    "    OPPONENT_ELIMINATED: 2,\n",
    "    SURVIVED_ROUND: 9\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "77764d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "action_probs = np.zeros((6,500))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "7ee713ae-2c19-4c00-9eb0-daf9d05fd540",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:58<00:00,  1.94s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 298683 actions.\n",
      "298683 actions in the training set:\n",
      "UP: 15.22% (45452)\n",
      "DOWN: 34.32% (102520)\n",
      "LEFT: 16.40% (48982)\n",
      "RIGHT: 26.48% (79098)\n",
      "BOMB: 6.45% (19274)\n",
      "WAIT: 1.12% (3357)\n",
      "[1, 1,   200] loss: 20.545\n",
      "[1, 1,   400] loss: 17.363\n",
      "[1, 1,   600] loss: 15.881\n",
      "[1, 1,   800] loss: 15.734\n",
      "[1, 1,  1000] loss: 14.239\n",
      "[1, 1,  1200] loss: 13.750\n",
      "[1, 1,  1400] loss: 13.503\n",
      "[1, 1,  1600] loss: 13.106\n",
      "[1, 1,  1800] loss: 12.162\n",
      "[1, 1,  2000] loss: 12.178\n",
      "[1, 1,  2200] loss: 11.820\n",
      "[1, 1,  2400] loss: 11.852\n",
      "[1, 1,  2600] loss: 11.281\n",
      "[1, 1,  2800] loss: 11.148\n",
      "[1, 1,  3000] loss: 10.654\n",
      "[1, 1,  3200] loss: 11.071\n",
      "[1, 1,  3400] loss: 10.446\n",
      "[1, 1,  3600] loss: 10.402\n",
      "[1, 1,  3800] loss: 9.904\n",
      "[1, 1,  4000] loss: 9.635\n",
      "[1, 1,  4200] loss: 9.458\n",
      "[1, 1,  4400] loss: 9.511\n",
      "[1, 1,  4600] loss: 9.639\n",
      "[1, 1] loss: 9.649\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:58<00:00,  1.94s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 275019 actions.\n",
      "275019 actions in the training set:\n",
      "UP: 14.84% (40818)\n",
      "DOWN: 35.45% (97483)\n",
      "LEFT: 16.15% (44411)\n",
      "RIGHT: 26.31% (72357)\n",
      "BOMB: 6.03% (16597)\n",
      "WAIT: 1.22% (3353)\n",
      "[2, 1,   200] loss: 15.642\n",
      "[2, 1,   400] loss: 12.522\n",
      "[2, 1,   600] loss: 10.814\n",
      "[2, 1,   800] loss: 10.197\n",
      "[2, 1,  1000] loss: 9.779\n",
      "[2, 1,  1200] loss: 8.961\n",
      "[2, 1,  1400] loss: 8.457\n",
      "[2, 1,  1600] loss: 8.409\n",
      "[2, 1,  1800] loss: 8.007\n",
      "[2, 1,  2000] loss: 7.834\n",
      "[2, 1,  2200] loss: 7.641\n",
      "[2, 1,  2400] loss: 7.661\n",
      "[2, 1,  2600] loss: 7.253\n",
      "[2, 1,  2800] loss: 6.938\n",
      "[2, 1,  3000] loss: 6.859\n",
      "[2, 1,  3200] loss: 7.091\n",
      "[2, 1,  3400] loss: 7.851\n",
      "[2, 1,  3600] loss: 6.596\n",
      "[2, 1,  3800] loss: 7.056\n",
      "[2, 1,  4000] loss: 6.786\n",
      "[2, 1,  4200] loss: 6.336\n",
      "[2, 1] loss: 5.841\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [01:02<00:00,  2.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 296943 actions.\n",
      "296943 actions in the training set:\n",
      "UP: 14.74% (43777)\n",
      "DOWN: 34.50% (102442)\n",
      "LEFT: 15.75% (46779)\n",
      "RIGHT: 27.90% (82858)\n",
      "BOMB: 5.79% (17200)\n",
      "WAIT: 1.31% (3887)\n",
      "[3, 1,   200] loss: 13.117\n",
      "[3, 1,   400] loss: 10.529\n",
      "[3, 1,   600] loss: 8.514\n",
      "[3, 1,   800] loss: 7.821\n",
      "[3, 1,  1000] loss: 7.589\n",
      "[3, 1,  1200] loss: 7.152\n",
      "[3, 1,  1400] loss: 6.555\n",
      "[3, 1,  1600] loss: 6.769\n",
      "[3, 1,  1800] loss: 6.426\n",
      "[3, 1,  2000] loss: 6.444\n",
      "[3, 1,  2200] loss: 6.472\n",
      "[3, 1,  2400] loss: 6.159\n",
      "[3, 1,  2600] loss: 5.700\n",
      "[3, 1,  2800] loss: 6.195\n",
      "[3, 1,  3000] loss: 6.146\n",
      "[3, 1,  3200] loss: 6.141\n",
      "[3, 1,  3400] loss: 5.716\n",
      "[3, 1,  3600] loss: 5.535\n",
      "[3, 1,  3800] loss: 5.579\n",
      "[3, 1,  4000] loss: 5.579\n",
      "[3, 1,  4200] loss: 6.154\n",
      "[3, 1,  4400] loss: 5.372\n",
      "[3, 1,  4600] loss: 5.222\n",
      "[3, 1] loss: 6.239\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:53<00:00,  1.80s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 288673 actions.\n",
      "288673 actions in the training set:\n",
      "UP: 14.39% (41553)\n",
      "DOWN: 41.43% (119600)\n",
      "LEFT: 15.19% (43854)\n",
      "RIGHT: 22.47% (64863)\n",
      "BOMB: 5.37% (15504)\n",
      "WAIT: 1.14% (3299)\n",
      "[4, 1,   200] loss: 10.355\n",
      "[4, 1,   400] loss: 8.255\n",
      "[4, 1,   600] loss: 7.762\n",
      "[4, 1,   800] loss: 6.706\n",
      "[4, 1,  1000] loss: 6.530\n",
      "[4, 1,  1200] loss: 6.148\n",
      "[4, 1,  1400] loss: 5.509\n",
      "[4, 1,  1600] loss: 5.585\n",
      "[4, 1,  1800] loss: 5.632\n",
      "[4, 1,  2000] loss: 5.073\n",
      "[4, 1,  2200] loss: 4.864\n",
      "[4, 1,  2400] loss: 5.536\n",
      "[4, 1,  2600] loss: 5.197\n",
      "[4, 1,  2800] loss: 4.650\n",
      "[4, 1,  3000] loss: 5.106\n",
      "[4, 1,  3200] loss: 5.531\n",
      "[4, 1,  3400] loss: 4.926\n",
      "[4, 1,  3600] loss: 4.624\n",
      "[4, 1,  3800] loss: 4.378\n",
      "[4, 1,  4000] loss: 4.987\n",
      "[4, 1,  4200] loss: 4.764\n",
      "[4, 1,  4400] loss: 4.636\n",
      "[4, 1] loss: 4.888\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:52<00:00,  1.76s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 278948 actions.\n",
      "278948 actions in the training set:\n",
      "UP: 13.04% (36378)\n",
      "DOWN: 39.66% (110624)\n",
      "LEFT: 14.77% (41196)\n",
      "RIGHT: 26.79% (74744)\n",
      "BOMB: 4.77% (13313)\n",
      "WAIT: 0.97% (2693)\n",
      "[5, 1,   200] loss: 11.025\n",
      "[5, 1,   400] loss: 8.949\n",
      "[5, 1,   600] loss: 7.630\n",
      "[5, 1,   800] loss: 7.161\n",
      "[5, 1,  1000] loss: 6.631\n",
      "[5, 1,  1200] loss: 6.287\n",
      "[5, 1,  1400] loss: 6.397\n",
      "[5, 1,  1600] loss: 5.681\n",
      "[5, 1,  1800] loss: 6.000\n",
      "[5, 1,  2000] loss: 6.066\n",
      "[5, 1,  2200] loss: 5.989\n",
      "[5, 1,  2400] loss: 5.975\n",
      "[5, 1,  2600] loss: 5.652\n",
      "[5, 1,  2800] loss: 5.766\n",
      "[5, 1,  3000] loss: 5.277\n",
      "[5, 1,  3200] loss: 5.607\n",
      "[5, 1,  3400] loss: 5.522\n",
      "[5, 1,  3600] loss: 5.495\n",
      "[5, 1,  3800] loss: 5.514\n",
      "[5, 1,  4000] loss: 5.363\n",
      "[5, 1,  4200] loss: 5.473\n",
      "[5, 1] loss: 5.809\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:51<00:00,  1.71s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 261623 actions.\n",
      "261623 actions in the training set:\n",
      "UP: 15.45% (40432)\n",
      "DOWN: 39.18% (102493)\n",
      "LEFT: 14.66% (38343)\n",
      "RIGHT: 23.93% (62594)\n",
      "BOMB: 5.39% (14107)\n",
      "WAIT: 1.40% (3654)\n",
      "[6, 1,   200] loss: 10.541\n",
      "[6, 1,   400] loss: 8.960\n",
      "[6, 1,   600] loss: 7.116\n",
      "[6, 1,   800] loss: 6.318\n",
      "[6, 1,  1000] loss: 6.269\n",
      "[6, 1,  1200] loss: 5.886\n",
      "[6, 1,  1400] loss: 6.083\n",
      "[6, 1,  1600] loss: 5.624\n",
      "[6, 1,  1800] loss: 5.623\n",
      "[6, 1,  2000] loss: 5.538\n",
      "[6, 1,  2200] loss: 5.588\n",
      "[6, 1,  2400] loss: 5.382\n",
      "[6, 1,  2600] loss: 5.600\n",
      "[6, 1,  2800] loss: 5.469\n",
      "[6, 1,  3000] loss: 5.517\n",
      "[6, 1,  3200] loss: 5.241\n",
      "[6, 1,  3400] loss: 5.374\n",
      "[6, 1,  3600] loss: 5.240\n",
      "[6, 1,  3800] loss: 5.201\n",
      "[6, 1,  4000] loss: 5.039\n",
      "[6, 1] loss: 5.375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:55<00:00,  1.83s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 266770 actions.\n",
      "266770 actions in the training set:\n",
      "UP: 15.18% (40502)\n",
      "DOWN: 41.63% (111067)\n",
      "LEFT: 13.93% (37161)\n",
      "RIGHT: 22.58% (60247)\n",
      "BOMB: 5.43% (14481)\n",
      "WAIT: 1.24% (3312)\n",
      "[7, 1,   200] loss: 10.172\n",
      "[7, 1,   400] loss: 8.596\n",
      "[7, 1,   600] loss: 7.097\n",
      "[7, 1,   800] loss: 6.640\n",
      "[7, 1,  1000] loss: 6.194\n",
      "[7, 1,  1200] loss: 6.160\n",
      "[7, 1,  1400] loss: 5.763\n",
      "[7, 1,  1600] loss: 5.387\n",
      "[7, 1,  1800] loss: 5.189\n",
      "[7, 1,  2000] loss: 5.195\n",
      "[7, 1,  2200] loss: 5.018\n",
      "[7, 1,  2400] loss: 5.369\n",
      "[7, 1,  2600] loss: 4.919\n",
      "[7, 1,  2800] loss: 5.100\n",
      "[7, 1,  3000] loss: 5.002\n",
      "[7, 1,  3200] loss: 5.368\n",
      "[7, 1,  3400] loss: 4.856\n",
      "[7, 1,  3600] loss: 5.708\n",
      "[7, 1,  3800] loss: 5.186\n",
      "[7, 1,  4000] loss: 4.962\n",
      "[7, 1] loss: 4.603\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:52<00:00,  1.74s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 272686 actions.\n",
      "272686 actions in the training set:\n",
      "UP: 14.62% (39861)\n",
      "DOWN: 41.77% (113900)\n",
      "LEFT: 11.12% (30333)\n",
      "RIGHT: 26.36% (71867)\n",
      "BOMB: 4.95% (13500)\n",
      "WAIT: 1.18% (3225)\n",
      "[8, 1,   200] loss: 8.618\n",
      "[8, 1,   400] loss: 7.104\n",
      "[8, 1,   600] loss: 6.021\n",
      "[8, 1,   800] loss: 5.576\n",
      "[8, 1,  1000] loss: 5.072\n",
      "[8, 1,  1200] loss: 4.652\n",
      "[8, 1,  1400] loss: 4.711\n",
      "[8, 1,  1600] loss: 4.287\n",
      "[8, 1,  1800] loss: 4.299\n",
      "[8, 1,  2000] loss: 4.500\n",
      "[8, 1,  2200] loss: 4.388\n",
      "[8, 1,  2400] loss: 4.180\n",
      "[8, 1,  2600] loss: 4.070\n",
      "2624\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_28285/2862909088.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     43\u001b[0m             \"\"\"\n\u001b[1;32m     44\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlog_probs\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    485\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m             )\n\u001b[0;32m--> 487\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    488\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m         )\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    198\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    201\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for run in range(10):\n",
    "    # os.system(\"rm -rf ../rule_based_agent/data\")\n",
    "    os.system(\"rm -rf ../data\")\n",
    "    #os.system(\"cd ../..; python main.py play --no-gui --agents basic_agent basic_agent basic_agent basic_agent --train 4 --n-rounds 30 --scenario loot-crate\")\n",
    "    os.system(\"cd ../..; python main.py play --no-gui --agents basic_agent basic_agent basic_agent basic_agent  --train 4 --n-rounds 30 --scenario loot-crate\")\n",
    "\n",
    "    # os.system(\"cd ../..; python main.py play --no-gui --agents rule_based_agent rule_based_agent rule_based_agent rule_based_agent --train 4 --n-rounds 10 --scenario classic\")\n",
    "\n",
    "    trainset = BombermanDataset(data_path)\n",
    "    N = len(trainset)\n",
    "    print(f\"{N} actions in the training set:\")\n",
    "    for i in range(6):\n",
    "        n = np.sum([t == i for t in trainset.actions])\n",
    "        print(f\"{ACTION_MAP_INV[i]}: {n/N*100:.2f}% ({n})\")\n",
    "        action_probs[i,run] = n/N\n",
    "    \n",
    "    \n",
    "    # sampler = BatchSampler(WeightedRandomSampler(trainset.weights, len(trainset), replacement=True,), batch_size, False)\n",
    "    # trainloader = torch.utils.data.DataLoader(trainset, batch_sampler=sampler)\n",
    "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    for epoch in range(num_epoch_per_run):\n",
    "        running_loss = 0.0\n",
    "        running = 0\n",
    "        for i, batch in enumerate(trainloader, 0):\n",
    "        # for i, batch in enumerate(sampler, 0):\n",
    "            print(i, end=\"\\r\")\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "            (channels, features), actions, rewards = batch\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "    \n",
    "            # forward + backward + optimize\n",
    "            outputs = net(channels, features)\n",
    "            # loss = criterion(outputs, labels)\n",
    "            log_logits = F.log_softmax(outputs, dim=-1)\n",
    "            log_probs = log_logits.gather(1, actions[:,np.newaxis])\n",
    "            \"\"\"\n",
    "            if i == 0:\n",
    "                print(actions, rewards, log_probs)\n",
    "                print(-log_probs * rewards)\n",
    "            \"\"\"\n",
    "            loss = torch.mean(-log_probs * rewards)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "            # print statistics\n",
    "            running_loss += loss.item()\n",
    "            running += 1\n",
    "            if i % 200 == 199:    # print every 2000 mini-batches\n",
    "                print(f'[{run + 1}, {epoch + 1}, {i + 1:5d}] loss: {running_loss / running:.3f}')\n",
    "                torch.save(net.state_dict(), weight_path)\n",
    "                running_loss = 0.0\n",
    "                running = 0\n",
    "        if running > 0:\n",
    "            print(f'[{run + 1}, {epoch + 1}] loss: {running_loss / running:.3f}')\n",
    "    torch.save(net.state_dict(), weight_path)\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1e729cf5-51d3-499c-a029-dcc14cc8954c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0. 0. 0. 0.]]\n",
      "[0. 0. 0. 0. 0. 0. 0.]\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "a = np.zeros((1,7))\n",
    "print(a)\n",
    "print(a[0])\n",
    "print(a[0,0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "2c53db6b-c671-4001-a638-de64bbee8438",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3 4 5 6]\n",
      "[1 2 3 4 5 6]\n"
     ]
    }
   ],
   "source": [
    "a = np.array([1,2,3,4,5,6,7])-1\n",
    "print(a)\n",
    "print(a[1:7])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd468b0e",
   "metadata": {},
   "source": [
    "\n",
    "    extensions = np.zeros(4)\n",
    "    if pos[0]+4 <= 16:             #field, box_map, explosion_map, others_map, coin_map\n",
    "        if field[pos[0]+4, pos[1]] == 1:\n",
    "            extensions[0] = 1\n",
    "        if box_map[pos[0]+4, pos[1]] ==1:\n",
    "            extensions[0] = 2\n",
    "        if explosion_map[pos[0]+4, pos[1]] ==1:\n",
    "            extensions[0] = 3\n",
    "        if others_map[pos[0]+4, pos[1]] == 1:\n",
    "            extensions[0] = 4\n",
    "        if coin_map[pos[0]+4, pos[1]] ==1:\n",
    "            extensions[0] =5\n",
    "\n",
    "    if pos[0]-4 >= 0:             #field, box_map, explosion_map, others_map, coin_map\n",
    "        if field[pos[0]-4, pos[1]] == 1:\n",
    "            extensions[1] = 1\n",
    "        if box_map[pos[0]-4, pos[1]] ==1:\n",
    "            extensions[1] = 2\n",
    "        if explosion_map[pos[0]+4, pos[1]] ==1:\n",
    "            extensions[1] = 3\n",
    "        if others_map[pos[0]-4, pos[1]] == 1:\n",
    "            extensions[1] = 4\n",
    "        if coin_map[pos[0]-4, pos[1]] ==1:\n",
    "            extensions[1] =5\n",
    "\n",
    "    if pos[1]+4 <= 16:             #field, box_map, explosion_map, others_map, coin_map\n",
    "        if field[pos[0], pos[1]+4] == 1:\n",
    "            extensions[2] = 1\n",
    "        if box_map[pos[0], pos[1]+4] ==1:\n",
    "            extensions[2] = 2\n",
    "        if explosion_map[pos[0], pos[1]+4] ==1:\n",
    "            extensions[2] = 3\n",
    "        if others_map[pos[0], pos[1]+4] == 1:\n",
    "            extensions[2] = 4\n",
    "        if coin_map[pos[0], pos[1]+4] ==1:\n",
    "            extensions[2] = 5\n",
    "\n",
    "    if pos[1]-4 >= 0:             #field, box_map, explosion_map, others_map, coin_map\n",
    "        if field[pos[0], pos[1]-4] == 1:\n",
    "            extensions[3] = 1\n",
    "        if box_map[pos[0], pos[1]-4] ==1:\n",
    "            extensions[3] = 2\n",
    "        if explosion_map[pos[0], pos[1]-4] ==1:\n",
    "            extensions[3] = 3\n",
    "        if others_map[pos[0], pos[1]-4] == 1:\n",
    "            extensions[3] = 4\n",
    "        if coin_map[pos[0], pos[1]-4] ==1:\n",
    "            extensions[3] = 5\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
