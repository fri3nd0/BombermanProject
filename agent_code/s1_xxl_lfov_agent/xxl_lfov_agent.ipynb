{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bbe88005-be9f-46e7-8600-019ed0bc7dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "sys.path.append(r\"../..\")\n",
    "\n",
    "\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from callbacks import state_to_features, ACTION_MAP, ACTION_MAP_INV\n",
    "from networks import AgentNet\n",
    "\n",
    "from events import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7369f91a-b21e-407c-bced-63e441be02a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class BombermanDataset(Dataset):\n",
    "    def __init__(self, states_dir, max_game_step=200, min_reward=1):\n",
    "        self.features = []\n",
    "        self.actions = []\n",
    "        self.cum_rewards = []\n",
    "        self.total_number = 0\n",
    "        for states_file in os.listdir(states_dir):\n",
    "\n",
    "            if not states_file.endswith('.pickle'):\n",
    "                continue\n",
    "            with open(os.path.join(states_dir, states_file), \"rb\") as f:\n",
    "                data = pickle.load(f)\n",
    "            self.total_number += len(data['game_state'])\n",
    "            last_round = -1\n",
    "            running_horizons = []\n",
    "            running_rewards = []\n",
    "            for i in range(len(data['game_state'])-1,-1,-1):\n",
    "                game_state = data['game_state'][i]               \n",
    "\n",
    "                    \n",
    "                if last_round != game_state['round']:\n",
    "                    last_round = game_state['round']\n",
    "                    running_horizons = []\n",
    "                    running_rewards = []\n",
    "                if game_state['step'] > max_game_step:\n",
    "                    continue\n",
    "                    \n",
    "                # determine if action should be included in training set\n",
    "                \n",
    "                horizons = [EVENT_HORIZON[e] for e in data['events'][i]]\n",
    "                rewards = [REWARDS[e] for e in data['events'][i]]\n",
    "                cr = np.where(data['events'] == \"CRATE_DESTROYED\")\n",
    "                #print(cr)\n",
    "                #rewards[np.array(cr[-1:], dtype = \"int\")] == 0\n",
    "                #for j in cr[1:]:\n",
    "                #    rewards[j] = 0             \n",
    "\n",
    "                running_rewards = [r for j,r in enumerate(running_rewards) if running_horizons[j]>1]\n",
    "                running_horizons = [h-1 for h in running_horizons if h > 1]\n",
    "                running_rewards.extend(rewards)\n",
    "                running_horizons.extend(horizons)\n",
    "                #if \"INVALID_ACTION\" in data['events'][i]:\n",
    "                #    continue   \n",
    "                self.cum_rewards.append(np.sum(running_rewards))\n",
    "\n",
    "                last_action = None if game_state['step'] == 1 else data['action'][i-1]\n",
    "                #print(\"LAST ACTION:\")\n",
    "                #print(last_action)\n",
    "                \n",
    "                channels, features, act_map = state_to_features(game_state, last_action=last_action)\n",
    "                action = ACTION_MAP[act_map[data['action'][i]]]\n",
    "                self.features.append((\n",
    "                    torch.tensor(channels[np.newaxis], dtype=torch.float),\n",
    "                    torch.tensor(features, dtype=torch.float)\n",
    "                ))\n",
    "                self.actions.append(action)\n",
    "                \n",
    "                    \n",
    "        self.cum_rewards = torch.tensor(self.cum_rewards, dtype=torch.float)\n",
    "        self.n = np.array([np.sum([t == i for t in self.actions]) for i in range(6)])\n",
    "        self.weights = np.array([1/self.n[t] for t in self.actions])\n",
    "        self.proportion_selected = len(self.actions)/self.total_number\n",
    "        print(f\"Loaded {len(self.actions)} actions.\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.features[idx], self.actions[idx], self.cum_rewards[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "944b4018-e9a4-45ce-825b-41f22809126e",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "data_path = f\"../data/\"\n",
    "weight_path = \"models/model_weights_pretrained.pth\"\n",
    "num_epoch_per_run = 1\n",
    "\n",
    "# optimizer = optim.SGD(net.parameters(), momentum=0.9, lr=0.001)\n",
    "net = AgentNet()\n",
    "#net.load_state_dict(torch.load(weight_path))\n",
    "optimizer = optim.AdamW(net.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7b090f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.AdamW(net.parameters(), lr=0.001)\n",
    "batch_size = 128\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a87534bb-9e02-4b62-8053-194d219d0ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "EVENT_HORIZON = {\n",
    "    MOVED_LEFT: 1,\n",
    "    MOVED_RIGHT: 1,\n",
    "    MOVED_UP: 1,\n",
    "    MOVED_DOWN: 1,\n",
    "    WAITED: 1,\n",
    "    INVALID_ACTION: 1,\n",
    "    BOMB_DROPPED: 1,\n",
    "    BOMB_EXPLODED: 1,\n",
    "    CRATE_DESTROYED: 6,\n",
    "    COIN_FOUND: 1,\n",
    "    COIN_COLLECTED: 5,\n",
    "    KILLED_OPPONENT: 6,\n",
    "    KILLED_SELF: 5,\n",
    "    GOT_KILLED: 6,\n",
    "    OPPONENT_ELIMINATED: 4,\n",
    "    SURVIVED_ROUND: 100\n",
    "}\n",
    "\n",
    "\n",
    "REWARDS = {\n",
    "    MOVED_LEFT: 5, #1\n",
    "    MOVED_RIGHT: 5, #1\n",
    "    MOVED_UP: 5,   #1\n",
    "    MOVED_DOWN: 5,  #1\n",
    "    WAITED: 5,  #1\n",
    "    INVALID_ACTION: -120, #-7\n",
    "    BOMB_DROPPED: 15,  #1\n",
    "    BOMB_EXPLODED: 0,\n",
    "    CRATE_DESTROYED: 120,\n",
    "    COIN_FOUND: 0,\n",
    "    COIN_COLLECTED: 120,\n",
    "    KILLED_OPPONENT: 60,\n",
    "    KILLED_SELF: -120, #-12\n",
    "    GOT_KILLED: -120,\n",
    "    OPPONENT_ELIMINATED: 5,\n",
    "    SURVIVED_ROUND: 200 #20\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2f25651b-d431-4812-be1b-1419aeccbf0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [01:58<00:00,  2.96s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rule_based_agent_0    505\n",
      "rule_based_agent_1    452\n",
      "rule_based_agent_2    442\n",
      "rule_based_agent_3    483\n",
      "Loaded 531586 actions.\n",
      "531586 actions in the training set:\n",
      "UP: 20.08% (106742)\n",
      "DOWN: 25.60% (136066)\n",
      "LEFT: 19.36% (102926)\n",
      "RIGHT: 24.77% (131652)\n",
      "BOMB: 8.50% (45192)\n",
      "WAIT: 1.69% (9008)\n",
      "[1, 1,   200] loss: 328.329\n",
      "0.04791698803203997\n",
      "[1, 1,   400] loss: 172.784\n",
      "0.09607476494866306\n",
      "[1, 1,   600] loss: 136.952\n",
      "0.14423254186528614\n",
      "[1, 1,   800] loss: 117.749\n",
      "0.19239031878190924\n",
      "[1, 1] loss: 111.130\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [01:30<00:00,  2.26s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rule_based_agent_0    482\n",
      "rule_based_agent_1    511\n",
      "rule_based_agent_2    452\n",
      "rule_based_agent_3    519\n",
      "Loaded 515270 actions.\n",
      "515270 actions in the training set:\n",
      "UP: 19.30% (99437)\n",
      "DOWN: 24.91% (128335)\n",
      "LEFT: 19.89% (102496)\n",
      "RIGHT: 25.41% (130916)\n",
      "BOMB: 8.69% (44776)\n",
      "WAIT: 1.81% (9310)\n",
      "[2, 1,   200] loss: 117.750\n",
      "0.04943427717507326\n",
      "[2, 1,   400] loss: 101.821\n",
      "0.0991169678032876\n",
      "[2, 1,   600] loss: 92.666\n",
      "0.14879965843150192\n",
      "[2, 1,   800] loss: 83.491\n",
      "0.19848234905971626\n",
      "[2, 1] loss: 85.855\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:58<00:00,  1.47s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rule_based_agent_0    450\n",
      "rule_based_agent_1    495\n",
      "rule_based_agent_2    484\n",
      "rule_based_agent_3    489\n",
      "Loaded 545312 actions.\n",
      "545312 actions in the training set:\n",
      "UP: 19.82% (108092)\n",
      "DOWN: 24.91% (135840)\n",
      "LEFT: 19.65% (107135)\n",
      "RIGHT: 25.32% (138082)\n",
      "BOMB: 8.57% (46735)\n",
      "WAIT: 1.73% (9428)\n",
      "[3, 1,   200] loss: 107.201\n",
      "0.046710873775013206\n",
      "[3, 1,   400] loss: 94.656\n",
      "0.09365647555894607\n",
      "[3, 1,   600] loss: 86.712\n",
      "0.14060207734287894\n",
      "[3, 1,   800] loss: 78.471\n",
      "0.1875476791268118\n",
      "[3, 1] loss: 73.658\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:58<00:00,  1.45s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rule_based_agent_0    447\n",
      "rule_based_agent_1    505\n",
      "rule_based_agent_2    528\n",
      "rule_based_agent_3    449\n",
      "Loaded 515085 actions.\n",
      "515085 actions in the training set:\n",
      "UP: 19.78% (101873)\n",
      "DOWN: 25.39% (130759)\n",
      "LEFT: 19.41% (99991)\n",
      "RIGHT: 24.88% (128147)\n",
      "BOMB: 8.82% (45427)\n",
      "WAIT: 1.73% (8888)\n",
      "[4, 1,   200] loss: 99.308\n",
      "0.04945203218886203\n",
      "[4, 1,   400] loss: 85.310\n",
      "0.09915256705203995\n",
      "[4, 1,   600] loss: 76.573\n",
      "0.1488531019152179\n",
      "[4, 1,   800] loss: 68.866\n",
      "0.1985536367783958\n",
      "[4, 1] loss: 77.572\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [01:25<00:00,  2.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rule_based_agent_0    557\n",
      "rule_based_agent_1    507\n",
      "rule_based_agent_2    437\n",
      "rule_based_agent_3    479\n",
      "Loaded 516209 actions.\n",
      "516209 actions in the training set:\n",
      "UP: 19.57% (101019)\n",
      "DOWN: 25.29% (130540)\n",
      "LEFT: 19.64% (101358)\n",
      "RIGHT: 25.03% (129212)\n",
      "BOMB: 8.73% (45065)\n",
      "WAIT: 1.75% (9015)\n",
      "[5, 1,   200] loss: 95.081\n",
      "0.04934435470904227\n",
      "[5, 1,   400] loss: 80.185\n",
      "0.09893667099953701\n",
      "[5, 1,   600] loss: 72.555\n",
      "0.14852898729003175\n",
      "[5, 1,   800] loss: 61.559\n",
      "0.1981213035805265\n",
      "[5, 1] loss: 64.301\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [01:08<00:00,  1.72s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rule_based_agent_0    438\n",
      "rule_based_agent_1    519\n",
      "rule_based_agent_2    497\n",
      "rule_based_agent_3    531\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_155639/3043398580.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cd ../..; python main.py play --no-gui --agents rule_based_agent rule_based_agent rule_based_agent rule_based_agent --train 4 --n-rounds 40 --scenario loot-crate\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mtrainset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBombermanDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mN\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{N} actions in the training set:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_155639/3527033403.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, states_dir, max_game_step, min_reward)\u001b[0m\n\u001b[1;32m     33\u001b[0m                 \u001b[0mhorizons\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mEVENT_HORIZON\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'events'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m                 \u001b[0mrewards\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mREWARDS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'events'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m                 \u001b[0mcr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'events'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"CRATE_DESTROYED\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m                 \u001b[0;31m#print(cr)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m                 \u001b[0;31m#rewards[np.array(cr[-1:], dtype = \"int\")] == 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mwhere\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for run in range(10):\n",
    "    # os.system(\"rm -rf ../rule_based_agent/data\")\n",
    "    os.system(\"rm -rf ../data\")\n",
    "    os.system(\"cd ../..; python main.py play --no-gui --agents rule_based_agent rule_based_agent rule_based_agent rule_based_agent --train 4 --n-rounds 40 --scenario loot-crate\")\n",
    "\n",
    "    trainset = BombermanDataset(data_path)\n",
    "    N = len(trainset)\n",
    "    print(f\"{N} actions in the training set:\")\n",
    "    for i in range(6):\n",
    "        n = np.sum([t == i for t in trainset.actions])\n",
    "        print(f\"{ACTION_MAP_INV[i]}: {n/N*100:.2f}% ({n})\")\n",
    "    # sampler = BatchSampler(WeightedRandomSampler(trainset.weights, len(trainset), replacement=True,), batch_size, False)\n",
    "    # trainloader = torch.utils.data.DataLoader(trainset, batch_sampler=sampler)\n",
    "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    for epoch in range(num_epoch_per_run):\n",
    "        running_loss = 0.0\n",
    "        running = 0\n",
    "        for i, batch in enumerate(trainloader, 0):\n",
    "        # for i, batch in enumerate(sampler, 0):\n",
    "            print(i, end=\"\\r\")\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "            (channels, features), actions, rewards = batch\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "    \n",
    "            # forward + backward + optimize\n",
    "            outputs = net(channels, features)\n",
    "            # loss = criterion(outputs, labels)\n",
    "            log_logits = F.log_softmax(outputs, dim=-1)\n",
    "            log_probs = log_logits.gather(1, actions[:,np.newaxis])\n",
    "            \n",
    "            loss = torch.mean(-log_probs * rewards)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "            # print statistics\n",
    "            running_loss += loss.item()\n",
    "            running += 1\n",
    "            if i % 200 == 199:    # print every 2000 mini-batches\n",
    "                print(f'[{run + 1}, {epoch + 1}, {i + 1:5d}] loss: {running_loss / running:.3f}')\n",
    "                torch.save(net.state_dict(), weight_path)\n",
    "                running_loss = 0.0\n",
    "                running = 0\n",
    "                #print(log_probs)\n",
    "                #print(rewards)\n",
    "                print(i*batch_size/N)\n",
    "            if i*batch_size/N > 0.2:\n",
    "                break\n",
    "        if running > 0:\n",
    "            print(f'[{run + 1}, {epoch + 1}] loss: {running_loss / running:.3f}')\n",
    "            \n",
    "    torch.save(net.state_dict(), weight_path)\n",
    "\n",
    "print('Finished Training') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "25463da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(net.state_dict(), \"models/model_weights.pth\")\n",
    "\n",
    "\n",
    "#weight_path_ref = \"models/model_weights_pretrained_ref.pth\"\n",
    "#torch.save(net.state_dict(),weight_path_ref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "0136c04c-7b35-43b8-a5e1-9a83f30dc43c",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "data_path = f\"../data/\"\n",
    "weight_path = \"models/model_weights.pth\"\n",
    "pretrained_weight_path = \"models/model_weights_pretrained.pth\"\n",
    "num_epoch_per_run = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "5ab0b0cd-1b41-460c-b3d5-bfb026b34e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pretrained_net = AgentNet()\n",
    "# pretrained_net.load_state_dict(torch.load(pretrained_weight_path))\n",
    "# torch.save(pretrained_net.state_dict(), weigth_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "7f5db558-2318-4b20-8385-a775144067cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = AgentNet()\n",
    "pretrained_net = AgentNet()\n",
    "pretrained_net.load_state_dict(torch.load(pretrained_weight_path))\n",
    "net.cnn.load_state_dict(pretrained_net.cnn.state_dict())\n",
    "torch.save(net.state_dict(), weight_path)\n",
    "optimizer = optim.AdamW(net.mlp.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "4fea4551-3f68-4965-bf24-adca6abf13f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.AdamW(net.mlp.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e02d0f",
   "metadata": {},
   "source": [
    "EVENT_HORIZON = {\n",
    "    MOVED_LEFT: 1,\n",
    "    MOVED_RIGHT: 1,\n",
    "    MOVED_UP: 1,\n",
    "    MOVED_DOWN: 1,\n",
    "    WAITED: 1,\n",
    "    INVALID_ACTION: 1,\n",
    "    BOMB_DROPPED: 1,\n",
    "    BOMB_EXPLODED: 1,\n",
    "    CRATE_DESTROYED: 5,\n",
    "    COIN_FOUND: 3,\n",
    "    COIN_COLLECTED: 8,\n",
    "    KILLED_OPPONENT: 8,\n",
    "    KILLED_SELF: 4,\n",
    "    GOT_KILLED: 3,\n",
    "    OPPONENT_ELIMINATED: 7,\n",
    "    SURVIVED_ROUND: 5\n",
    "}\n",
    "\n",
    "\n",
    "REWARDS = {\n",
    "    MOVED_LEFT: -0.5,  #-0.2\n",
    "    MOVED_RIGHT: -0.5, #-0.2\n",
    "    MOVED_UP: -0.5,  #-0.2\n",
    "    MOVED_DOWN: -0.5,  #-0.2    \n",
    "    WAITED: 4,\n",
    "    INVALID_ACTION: -2,\n",
    "    BOMB_DROPPED: 0,\n",
    "    BOMB_EXPLODED: 1,\n",
    "    CRATE_DESTROYED: 2,\n",
    "    COIN_FOUND: 0,\n",
    "    COIN_COLLECTED: 6,\n",
    "    KILLED_OPPONENT: 1,\n",
    "    KILLED_SELF: -10,\n",
    "    GOT_KILLED: -1,\n",
    "    OPPONENT_ELIMINATED: 2,\n",
    "    SURVIVED_ROUND: 9\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "77764d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "action_probs = np.zeros((6,500))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "7ee713ae-2c19-4c00-9eb0-daf9d05fd540",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:58<00:00,  1.94s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 298683 actions.\n",
      "298683 actions in the training set:\n",
      "UP: 15.22% (45452)\n",
      "DOWN: 34.32% (102520)\n",
      "LEFT: 16.40% (48982)\n",
      "RIGHT: 26.48% (79098)\n",
      "BOMB: 6.45% (19274)\n",
      "WAIT: 1.12% (3357)\n",
      "[1, 1,   200] loss: 20.545\n",
      "[1, 1,   400] loss: 17.363\n",
      "[1, 1,   600] loss: 15.881\n",
      "[1, 1,   800] loss: 15.734\n",
      "[1, 1,  1000] loss: 14.239\n",
      "[1, 1,  1200] loss: 13.750\n",
      "[1, 1,  1400] loss: 13.503\n",
      "[1, 1,  1600] loss: 13.106\n",
      "[1, 1,  1800] loss: 12.162\n",
      "[1, 1,  2000] loss: 12.178\n",
      "[1, 1,  2200] loss: 11.820\n",
      "[1, 1,  2400] loss: 11.852\n",
      "[1, 1,  2600] loss: 11.281\n",
      "[1, 1,  2800] loss: 11.148\n",
      "[1, 1,  3000] loss: 10.654\n",
      "[1, 1,  3200] loss: 11.071\n",
      "[1, 1,  3400] loss: 10.446\n",
      "[1, 1,  3600] loss: 10.402\n",
      "[1, 1,  3800] loss: 9.904\n",
      "[1, 1,  4000] loss: 9.635\n",
      "[1, 1,  4200] loss: 9.458\n",
      "[1, 1,  4400] loss: 9.511\n",
      "[1, 1,  4600] loss: 9.639\n",
      "[1, 1] loss: 9.649\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:58<00:00,  1.94s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 275019 actions.\n",
      "275019 actions in the training set:\n",
      "UP: 14.84% (40818)\n",
      "DOWN: 35.45% (97483)\n",
      "LEFT: 16.15% (44411)\n",
      "RIGHT: 26.31% (72357)\n",
      "BOMB: 6.03% (16597)\n",
      "WAIT: 1.22% (3353)\n",
      "[2, 1,   200] loss: 15.642\n",
      "[2, 1,   400] loss: 12.522\n",
      "[2, 1,   600] loss: 10.814\n",
      "[2, 1,   800] loss: 10.197\n",
      "[2, 1,  1000] loss: 9.779\n",
      "[2, 1,  1200] loss: 8.961\n",
      "[2, 1,  1400] loss: 8.457\n",
      "[2, 1,  1600] loss: 8.409\n",
      "[2, 1,  1800] loss: 8.007\n",
      "[2, 1,  2000] loss: 7.834\n",
      "[2, 1,  2200] loss: 7.641\n",
      "[2, 1,  2400] loss: 7.661\n",
      "[2, 1,  2600] loss: 7.253\n",
      "[2, 1,  2800] loss: 6.938\n",
      "[2, 1,  3000] loss: 6.859\n",
      "[2, 1,  3200] loss: 7.091\n",
      "[2, 1,  3400] loss: 7.851\n",
      "[2, 1,  3600] loss: 6.596\n",
      "[2, 1,  3800] loss: 7.056\n",
      "[2, 1,  4000] loss: 6.786\n",
      "[2, 1,  4200] loss: 6.336\n",
      "[2, 1] loss: 5.841\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [01:02<00:00,  2.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 296943 actions.\n",
      "296943 actions in the training set:\n",
      "UP: 14.74% (43777)\n",
      "DOWN: 34.50% (102442)\n",
      "LEFT: 15.75% (46779)\n",
      "RIGHT: 27.90% (82858)\n",
      "BOMB: 5.79% (17200)\n",
      "WAIT: 1.31% (3887)\n",
      "[3, 1,   200] loss: 13.117\n",
      "[3, 1,   400] loss: 10.529\n",
      "[3, 1,   600] loss: 8.514\n",
      "[3, 1,   800] loss: 7.821\n",
      "[3, 1,  1000] loss: 7.589\n",
      "[3, 1,  1200] loss: 7.152\n",
      "[3, 1,  1400] loss: 6.555\n",
      "[3, 1,  1600] loss: 6.769\n",
      "[3, 1,  1800] loss: 6.426\n",
      "[3, 1,  2000] loss: 6.444\n",
      "[3, 1,  2200] loss: 6.472\n",
      "[3, 1,  2400] loss: 6.159\n",
      "[3, 1,  2600] loss: 5.700\n",
      "[3, 1,  2800] loss: 6.195\n",
      "[3, 1,  3000] loss: 6.146\n",
      "[3, 1,  3200] loss: 6.141\n",
      "[3, 1,  3400] loss: 5.716\n",
      "[3, 1,  3600] loss: 5.535\n",
      "[3, 1,  3800] loss: 5.579\n",
      "[3, 1,  4000] loss: 5.579\n",
      "[3, 1,  4200] loss: 6.154\n",
      "[3, 1,  4400] loss: 5.372\n",
      "[3, 1,  4600] loss: 5.222\n",
      "[3, 1] loss: 6.239\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:53<00:00,  1.80s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 288673 actions.\n",
      "288673 actions in the training set:\n",
      "UP: 14.39% (41553)\n",
      "DOWN: 41.43% (119600)\n",
      "LEFT: 15.19% (43854)\n",
      "RIGHT: 22.47% (64863)\n",
      "BOMB: 5.37% (15504)\n",
      "WAIT: 1.14% (3299)\n",
      "[4, 1,   200] loss: 10.355\n",
      "[4, 1,   400] loss: 8.255\n",
      "[4, 1,   600] loss: 7.762\n",
      "[4, 1,   800] loss: 6.706\n",
      "[4, 1,  1000] loss: 6.530\n",
      "[4, 1,  1200] loss: 6.148\n",
      "[4, 1,  1400] loss: 5.509\n",
      "[4, 1,  1600] loss: 5.585\n",
      "[4, 1,  1800] loss: 5.632\n",
      "[4, 1,  2000] loss: 5.073\n",
      "[4, 1,  2200] loss: 4.864\n",
      "[4, 1,  2400] loss: 5.536\n",
      "[4, 1,  2600] loss: 5.197\n",
      "[4, 1,  2800] loss: 4.650\n",
      "[4, 1,  3000] loss: 5.106\n",
      "[4, 1,  3200] loss: 5.531\n",
      "[4, 1,  3400] loss: 4.926\n",
      "[4, 1,  3600] loss: 4.624\n",
      "[4, 1,  3800] loss: 4.378\n",
      "[4, 1,  4000] loss: 4.987\n",
      "[4, 1,  4200] loss: 4.764\n",
      "[4, 1,  4400] loss: 4.636\n",
      "[4, 1] loss: 4.888\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:52<00:00,  1.76s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 278948 actions.\n",
      "278948 actions in the training set:\n",
      "UP: 13.04% (36378)\n",
      "DOWN: 39.66% (110624)\n",
      "LEFT: 14.77% (41196)\n",
      "RIGHT: 26.79% (74744)\n",
      "BOMB: 4.77% (13313)\n",
      "WAIT: 0.97% (2693)\n",
      "[5, 1,   200] loss: 11.025\n",
      "[5, 1,   400] loss: 8.949\n",
      "[5, 1,   600] loss: 7.630\n",
      "[5, 1,   800] loss: 7.161\n",
      "[5, 1,  1000] loss: 6.631\n",
      "[5, 1,  1200] loss: 6.287\n",
      "[5, 1,  1400] loss: 6.397\n",
      "[5, 1,  1600] loss: 5.681\n",
      "[5, 1,  1800] loss: 6.000\n",
      "[5, 1,  2000] loss: 6.066\n",
      "[5, 1,  2200] loss: 5.989\n",
      "[5, 1,  2400] loss: 5.975\n",
      "[5, 1,  2600] loss: 5.652\n",
      "[5, 1,  2800] loss: 5.766\n",
      "[5, 1,  3000] loss: 5.277\n",
      "[5, 1,  3200] loss: 5.607\n",
      "[5, 1,  3400] loss: 5.522\n",
      "[5, 1,  3600] loss: 5.495\n",
      "[5, 1,  3800] loss: 5.514\n",
      "[5, 1,  4000] loss: 5.363\n",
      "[5, 1,  4200] loss: 5.473\n",
      "[5, 1] loss: 5.809\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:51<00:00,  1.71s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 261623 actions.\n",
      "261623 actions in the training set:\n",
      "UP: 15.45% (40432)\n",
      "DOWN: 39.18% (102493)\n",
      "LEFT: 14.66% (38343)\n",
      "RIGHT: 23.93% (62594)\n",
      "BOMB: 5.39% (14107)\n",
      "WAIT: 1.40% (3654)\n",
      "[6, 1,   200] loss: 10.541\n",
      "[6, 1,   400] loss: 8.960\n",
      "[6, 1,   600] loss: 7.116\n",
      "[6, 1,   800] loss: 6.318\n",
      "[6, 1,  1000] loss: 6.269\n",
      "[6, 1,  1200] loss: 5.886\n",
      "[6, 1,  1400] loss: 6.083\n",
      "[6, 1,  1600] loss: 5.624\n",
      "[6, 1,  1800] loss: 5.623\n",
      "[6, 1,  2000] loss: 5.538\n",
      "[6, 1,  2200] loss: 5.588\n",
      "[6, 1,  2400] loss: 5.382\n",
      "[6, 1,  2600] loss: 5.600\n",
      "[6, 1,  2800] loss: 5.469\n",
      "[6, 1,  3000] loss: 5.517\n",
      "[6, 1,  3200] loss: 5.241\n",
      "[6, 1,  3400] loss: 5.374\n",
      "[6, 1,  3600] loss: 5.240\n",
      "[6, 1,  3800] loss: 5.201\n",
      "[6, 1,  4000] loss: 5.039\n",
      "[6, 1] loss: 5.375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:55<00:00,  1.83s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 266770 actions.\n",
      "266770 actions in the training set:\n",
      "UP: 15.18% (40502)\n",
      "DOWN: 41.63% (111067)\n",
      "LEFT: 13.93% (37161)\n",
      "RIGHT: 22.58% (60247)\n",
      "BOMB: 5.43% (14481)\n",
      "WAIT: 1.24% (3312)\n",
      "[7, 1,   200] loss: 10.172\n",
      "[7, 1,   400] loss: 8.596\n",
      "[7, 1,   600] loss: 7.097\n",
      "[7, 1,   800] loss: 6.640\n",
      "[7, 1,  1000] loss: 6.194\n",
      "[7, 1,  1200] loss: 6.160\n",
      "[7, 1,  1400] loss: 5.763\n",
      "[7, 1,  1600] loss: 5.387\n",
      "[7, 1,  1800] loss: 5.189\n",
      "[7, 1,  2000] loss: 5.195\n",
      "[7, 1,  2200] loss: 5.018\n",
      "[7, 1,  2400] loss: 5.369\n",
      "[7, 1,  2600] loss: 4.919\n",
      "[7, 1,  2800] loss: 5.100\n",
      "[7, 1,  3000] loss: 5.002\n",
      "[7, 1,  3200] loss: 5.368\n",
      "[7, 1,  3400] loss: 4.856\n",
      "[7, 1,  3600] loss: 5.708\n",
      "[7, 1,  3800] loss: 5.186\n",
      "[7, 1,  4000] loss: 4.962\n",
      "[7, 1] loss: 4.603\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:52<00:00,  1.74s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 272686 actions.\n",
      "272686 actions in the training set:\n",
      "UP: 14.62% (39861)\n",
      "DOWN: 41.77% (113900)\n",
      "LEFT: 11.12% (30333)\n",
      "RIGHT: 26.36% (71867)\n",
      "BOMB: 4.95% (13500)\n",
      "WAIT: 1.18% (3225)\n",
      "[8, 1,   200] loss: 8.618\n",
      "[8, 1,   400] loss: 7.104\n",
      "[8, 1,   600] loss: 6.021\n",
      "[8, 1,   800] loss: 5.576\n",
      "[8, 1,  1000] loss: 5.072\n",
      "[8, 1,  1200] loss: 4.652\n",
      "[8, 1,  1400] loss: 4.711\n",
      "[8, 1,  1600] loss: 4.287\n",
      "[8, 1,  1800] loss: 4.299\n",
      "[8, 1,  2000] loss: 4.500\n",
      "[8, 1,  2200] loss: 4.388\n",
      "[8, 1,  2400] loss: 4.180\n",
      "[8, 1,  2600] loss: 4.070\n",
      "2624\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_28285/2862909088.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     43\u001b[0m             \"\"\"\n\u001b[1;32m     44\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlog_probs\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    485\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m             )\n\u001b[0;32m--> 487\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    488\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m         )\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    198\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    201\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for run in range(10):\n",
    "    # os.system(\"rm -rf ../rule_based_agent/data\")\n",
    "    os.system(\"rm -rf ../data\")\n",
    "    #os.system(\"cd ../..; python main.py play --no-gui --agents basic_agent basic_agent basic_agent basic_agent --train 4 --n-rounds 30 --scenario loot-crate\")\n",
    "    os.system(\"cd ../..; python main.py play --no-gui --agents basic_agent basic_agent basic_agent basic_agent  --train 4 --n-rounds 30 --scenario loot-crate\")\n",
    "\n",
    "    # os.system(\"cd ../..; python main.py play --no-gui --agents rule_based_agent rule_based_agent rule_based_agent rule_based_agent --train 4 --n-rounds 10 --scenario classic\")\n",
    "\n",
    "    trainset = BombermanDataset(data_path)\n",
    "    N = len(trainset)\n",
    "    print(f\"{N} actions in the training set:\")\n",
    "    for i in range(6):\n",
    "        n = np.sum([t == i for t in trainset.actions])\n",
    "        print(f\"{ACTION_MAP_INV[i]}: {n/N*100:.2f}% ({n})\")\n",
    "        action_probs[i,run] = n/N\n",
    "    \n",
    "    \n",
    "    # sampler = BatchSampler(WeightedRandomSampler(trainset.weights, len(trainset), replacement=True,), batch_size, False)\n",
    "    # trainloader = torch.utils.data.DataLoader(trainset, batch_sampler=sampler)\n",
    "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    for epoch in range(num_epoch_per_run):\n",
    "        running_loss = 0.0\n",
    "        running = 0\n",
    "        for i, batch in enumerate(trainloader, 0):\n",
    "        # for i, batch in enumerate(sampler, 0):\n",
    "            print(i, end=\"\\r\")\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "            (channels, features), actions, rewards = batch\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "    \n",
    "            # forward + backward + optimize\n",
    "            outputs = net(channels, features)\n",
    "            # loss = criterion(outputs, labels)\n",
    "            log_logits = F.log_softmax(outputs, dim=-1)\n",
    "            log_probs = log_logits.gather(1, actions[:,np.newaxis])\n",
    "            \"\"\"\n",
    "            if i == 0:\n",
    "                print(actions, rewards, log_probs)\n",
    "                print(-log_probs * rewards)\n",
    "            \"\"\"\n",
    "            loss = torch.mean(-log_probs * rewards)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "            # print statistics\n",
    "            running_loss += loss.item()\n",
    "            running += 1\n",
    "            if i % 200 == 199:    # print every 2000 mini-batches\n",
    "                print(f'[{run + 1}, {epoch + 1}, {i + 1:5d}] loss: {running_loss / running:.3f}')\n",
    "                torch.save(net.state_dict(), weight_path)\n",
    "                running_loss = 0.0\n",
    "                running = 0\n",
    "        if running > 0:\n",
    "            print(f'[{run + 1}, {epoch + 1}] loss: {running_loss / running:.3f}')\n",
    "    torch.save(net.state_dict(), weight_path)\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1e729cf5-51d3-499c-a029-dcc14cc8954c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0. 0. 0. 0.]]\n",
      "[0. 0. 0. 0. 0. 0. 0.]\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "a = np.zeros((1,7))\n",
    "print(a)\n",
    "print(a[0])\n",
    "print(a[0,0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "2c53db6b-c671-4001-a638-de64bbee8438",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3 4 5 6]\n",
      "[1 2 3 4 5 6]\n"
     ]
    }
   ],
   "source": [
    "a = np.array([1,2,3,4,5,6,7])-1\n",
    "print(a)\n",
    "print(a[1:7])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd468b0e",
   "metadata": {},
   "source": [
    "\n",
    "    extensions = np.zeros(4)\n",
    "    if pos[0]+4 <= 16:             #field, box_map, explosion_map, others_map, coin_map\n",
    "        if field[pos[0]+4, pos[1]] == 1:\n",
    "            extensions[0] = 1\n",
    "        if box_map[pos[0]+4, pos[1]] ==1:\n",
    "            extensions[0] = 2\n",
    "        if explosion_map[pos[0]+4, pos[1]] ==1:\n",
    "            extensions[0] = 3\n",
    "        if others_map[pos[0]+4, pos[1]] == 1:\n",
    "            extensions[0] = 4\n",
    "        if coin_map[pos[0]+4, pos[1]] ==1:\n",
    "            extensions[0] =5\n",
    "\n",
    "    if pos[0]-4 >= 0:             #field, box_map, explosion_map, others_map, coin_map\n",
    "        if field[pos[0]-4, pos[1]] == 1:\n",
    "            extensions[1] = 1\n",
    "        if box_map[pos[0]-4, pos[1]] ==1:\n",
    "            extensions[1] = 2\n",
    "        if explosion_map[pos[0]+4, pos[1]] ==1:\n",
    "            extensions[1] = 3\n",
    "        if others_map[pos[0]-4, pos[1]] == 1:\n",
    "            extensions[1] = 4\n",
    "        if coin_map[pos[0]-4, pos[1]] ==1:\n",
    "            extensions[1] =5\n",
    "\n",
    "    if pos[1]+4 <= 16:             #field, box_map, explosion_map, others_map, coin_map\n",
    "        if field[pos[0], pos[1]+4] == 1:\n",
    "            extensions[2] = 1\n",
    "        if box_map[pos[0], pos[1]+4] ==1:\n",
    "            extensions[2] = 2\n",
    "        if explosion_map[pos[0], pos[1]+4] ==1:\n",
    "            extensions[2] = 3\n",
    "        if others_map[pos[0], pos[1]+4] == 1:\n",
    "            extensions[2] = 4\n",
    "        if coin_map[pos[0], pos[1]+4] ==1:\n",
    "            extensions[2] = 5\n",
    "\n",
    "    if pos[1]-4 >= 0:             #field, box_map, explosion_map, others_map, coin_map\n",
    "        if field[pos[0], pos[1]-4] == 1:\n",
    "            extensions[3] = 1\n",
    "        if box_map[pos[0], pos[1]-4] ==1:\n",
    "            extensions[3] = 2\n",
    "        if explosion_map[pos[0], pos[1]-4] ==1:\n",
    "            extensions[3] = 3\n",
    "        if others_map[pos[0], pos[1]-4] == 1:\n",
    "            extensions[3] = 4\n",
    "        if coin_map[pos[0], pos[1]-4] ==1:\n",
    "            extensions[3] = 5\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
