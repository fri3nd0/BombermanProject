{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bbe88005-be9f-46e7-8600-019ed0bc7dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "sys.path.append(r\"../..\")\n",
    "\n",
    "\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from callbacks import state_to_features, ACTION_MAP, ACTION_MAP_INV\n",
    "from networks import AgentNet\n",
    "\n",
    "from events import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "198f4ee7-8582-4580-8cb9-fbccd2d77655",
   "metadata": {},
   "outputs": [],
   "source": [
    "EVENT_HORIZON = {\n",
    "    MOVED_LEFT: 1,\n",
    "    MOVED_RIGHT: 1,\n",
    "    MOVED_UP: 1,\n",
    "    MOVED_DOWN: 1,\n",
    "    WAITED: 2,\n",
    "    INVALID_ACTION: 1,\n",
    "    BOMB_DROPPED: 2,\n",
    "    BOMB_EXPLODED: 2,\n",
    "    CRATE_DESTROYED: 8,\n",
    "    COIN_FOUND: 3,\n",
    "    COIN_COLLECTED: 5,\n",
    "    KILLED_OPPONENT: 8,\n",
    "    KILLED_SELF: 3,\n",
    "    GOT_KILLED: 3,\n",
    "    OPPONENT_ELIMINATED: 0,\n",
    "    SURVIVED_ROUND: 8\n",
    "}\n",
    "\"\"\"\n",
    "REWARDS = {\n",
    "    MOVED_LEFT: 1,\n",
    "    MOVED_RIGHT: 1,\n",
    "    MOVED_UP: 1,\n",
    "    MOVED_DOWN: 1,\n",
    "    WAITED: 0,\n",
    "    INVALID_ACTION: -5,\n",
    "    BOMB_DROPPED: 1,\n",
    "    BOMB_EXPLODED: 0,\n",
    "    CRATE_DESTROYED: 4,\n",
    "    COIN_FOUND: 0,\n",
    "    COIN_COLLECTED: 5,\n",
    "    KILLED_OPPONENT: 5,\n",
    "    KILLED_SELF: -8,\n",
    "    GOT_KILLED: -8,\n",
    "    OPPONENT_ELIMINATED: 0,\n",
    "    SURVIVED_ROUND: 0\n",
    "}\n",
    "\"\"\"\n",
    "REWARDS = {\n",
    "    MOVED_LEFT: 100,\n",
    "    MOVED_RIGHT: 100,\n",
    "    MOVED_UP: 100,\n",
    "    MOVED_DOWN: 100,\n",
    "    WAITED: -100,\n",
    "    INVALID_ACTION: -10,\n",
    "    BOMB_DROPPED: 0,\n",
    "    BOMB_EXPLODED: 0,\n",
    "    CRATE_DESTROYED: 0,\n",
    "    COIN_FOUND: 0,\n",
    "    COIN_COLLECTED: 0,\n",
    "    KILLED_OPPONENT: 0,\n",
    "    KILLED_SELF: 0,\n",
    "    GOT_KILLED: 0,\n",
    "    OPPONENT_ELIMINATED: 0,\n",
    "    SURVIVED_ROUND: 0\n",
    "}\n",
    "\n",
    "# REWARDS = {k: v + 10 for k, v in REWARDS.items()}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b315449-5fd7-449a-95fd-f0ad35deebbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify directory with training data\n",
    "data_path = \"../data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7369f91a-b21e-407c-bced-63e441be02a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class BombermanDataset(Dataset):\n",
    "    def __init__(self, states_dir, max_game_step=200, min_reward=1):\n",
    "        self.features = []\n",
    "        self.actions = []\n",
    "        self.cum_rewards = []\n",
    "        self.total_number = 0\n",
    "        for states_file in os.listdir(states_dir):\n",
    "\n",
    "            if not states_file.endswith('.pickle'):\n",
    "                continue\n",
    "            with open(os.path.join(states_dir, states_file), \"rb\") as f:\n",
    "                data = pickle.load(f)\n",
    "            self.total_number += len(data['game_state'])\n",
    "            last_round = -1\n",
    "            running_horizons = []\n",
    "            running_rewards = []\n",
    "            for i in range(len(data['game_state'])-1,-1,-1):\n",
    "                game_state = data['game_state'][i]               \n",
    "\n",
    "                    \n",
    "                if last_round != game_state['round']:\n",
    "                    last_round = game_state['round']\n",
    "                    running_horizons = []\n",
    "                    running_rewards = []\n",
    "                if game_state['step'] > max_game_step:\n",
    "                    continue\n",
    "                    \n",
    "                # determine if action should be included in training set\n",
    "                \n",
    "                horizons = [EVENT_HORIZON[e] for e in data['events'][i]]\n",
    "                rewards = [REWARDS[e] for e in data['events'][i]]\n",
    "                cr = np.where(data['events'] == \"CRATE_DESTROYED\")\n",
    "                #print(cr)\n",
    "                #rewards[np.array(cr[-1:], dtype = \"int\")] == 0\n",
    "                #for j in cr[1:]:\n",
    "                #    rewards[j] = 0             \n",
    "\n",
    "                running_rewards = [r for j,r in enumerate(running_rewards) if running_horizons[j]>1]\n",
    "                running_horizons = [h-1 for h in running_horizons if h > 1]\n",
    "                running_rewards.extend(rewards)\n",
    "                running_horizons.extend(horizons)\n",
    "                #if \"INVALID_ACTION\" in data['events'][i]:\n",
    "                #    continue   \n",
    "                self.cum_rewards.append(np.sum(running_rewards))\n",
    "\n",
    "                last_action = None if game_state['step'] == 1 else data['action'][i-1]\n",
    "                #print(\"LAST ACTION:\")\n",
    "                #print(last_action)\n",
    "                \n",
    "                channels, features, act_map = state_to_features(game_state, last_action=last_action)\n",
    "                action = ACTION_MAP[act_map[data['action'][i]]]\n",
    "                self.features.append((\n",
    "                    torch.tensor(channels[np.newaxis], dtype=torch.float),\n",
    "                    torch.tensor(features, dtype=torch.float)\n",
    "                ))\n",
    "                self.actions.append(action)\n",
    "                \n",
    "                    \n",
    "        self.cum_rewards = torch.tensor(self.cum_rewards, dtype=torch.float)\n",
    "        self.n = np.array([np.sum([t == i for t in self.actions]) for i in range(6)])\n",
    "        self.weights = np.array([1/self.n[t] for t in self.actions])\n",
    "        self.proportion_selected = len(self.actions)/self.total_number\n",
    "        print(f\"Loaded {len(self.actions)} actions.\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.features[idx], self.actions[idx], self.cum_rewards[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "944b4018-e9a4-45ce-825b-41f22809126e",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "data_path = f\"../data/\"\n",
    "weight_path = \"models/model_weights_pretrained.pth\"\n",
    "num_epoch_per_run = 1\n",
    "\n",
    "# optimizer = optim.SGD(net.parameters(), momentum=0.9, lr=0.001)\n",
    "net = AgentNet()\n",
    "torch.save(net.state_dict(), weight_path)\n",
    "optimizer = optim.AdamW(net.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7b090f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.AdamW(net.parameters(), lr=0.001)\n",
    "batch_size = 128\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a87534bb-9e02-4b62-8053-194d219d0ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "EVENT_HORIZON = {\n",
    "    MOVED_LEFT: 1,\n",
    "    MOVED_RIGHT: 1,\n",
    "    MOVED_UP: 1,\n",
    "    MOVED_DOWN: 1,\n",
    "    WAITED: 1,\n",
    "    INVALID_ACTION: 1,\n",
    "    BOMB_DROPPED: 1,\n",
    "    BOMB_EXPLODED: 1,\n",
    "    CRATE_DESTROYED: 6,\n",
    "    COIN_FOUND: 1,\n",
    "    COIN_COLLECTED: 5,\n",
    "    KILLED_OPPONENT: 6,\n",
    "    KILLED_SELF: 5,\n",
    "    GOT_KILLED: 6,\n",
    "    OPPONENT_ELIMINATED: 4,\n",
    "    SURVIVED_ROUND: 100\n",
    "}\n",
    "\n",
    "\n",
    "REWARDS = {\n",
    "    MOVED_LEFT: 5, #1\n",
    "    MOVED_RIGHT: 5, #1\n",
    "    MOVED_UP: 5,   #1\n",
    "    MOVED_DOWN: 5,  #1\n",
    "    WAITED: 5,  #1\n",
    "    INVALID_ACTION: -120, #-7\n",
    "    BOMB_DROPPED: 5,  #1\n",
    "    BOMB_EXPLODED: 0,\n",
    "    CRATE_DESTROYED: 120,\n",
    "    COIN_FOUND: 0,\n",
    "    COIN_COLLECTED: 120,\n",
    "    KILLED_OPPONENT: 60,\n",
    "    KILLED_SELF: -120, #-12\n",
    "    GOT_KILLED: -120,\n",
    "    OPPONENT_ELIMINATED: 5,\n",
    "    SURVIVED_ROUND: 200 #20\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2f25651b-d431-4812-be1b-1419aeccbf0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [01:04<00:00,  1.62s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rule_based_agent_0    467\n",
      "rule_based_agent_1    471\n",
      "rule_based_agent_2    485\n",
      "rule_based_agent_3    483\n",
      "Loaded 529282 actions.\n",
      "529282 actions in the training set:\n",
      "UP: 19.57% (103561)\n",
      "DOWN: 25.01% (132377)\n",
      "LEFT: 19.84% (104996)\n",
      "RIGHT: 25.32% (134024)\n",
      "BOMB: 8.66% (45840)\n",
      "WAIT: 1.60% (8484)\n",
      "[1, 1,   200] loss: 318.752\n",
      "[1, 1,   400] loss: 153.964\n",
      "[1, 1,   600] loss: 138.011\n",
      "[1, 1] loss: 117.551\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:59<00:00,  1.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rule_based_agent_0    468\n",
      "rule_based_agent_1    472\n",
      "rule_based_agent_2    488\n",
      "rule_based_agent_3    481\n",
      "Loaded 517242 actions.\n",
      "517242 actions in the training set:\n",
      "UP: 19.52% (100969)\n",
      "DOWN: 25.07% (129665)\n",
      "LEFT: 19.66% (101676)\n",
      "RIGHT: 25.42% (131482)\n",
      "BOMB: 8.68% (44871)\n",
      "WAIT: 1.66% (8579)\n",
      "[2, 1,   200] loss: 128.109\n",
      "[2, 1,   400] loss: 116.567\n",
      "[2, 1,   600] loss: 107.399\n",
      "[2, 1] loss: 96.754\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [01:03<00:00,  1.58s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rule_based_agent_0    484\n",
      "rule_based_agent_1    487\n",
      "rule_based_agent_2    515\n",
      "rule_based_agent_3    466\n",
      "Loaded 532165 actions.\n",
      "532165 actions in the training set:\n",
      "UP: 19.48% (103672)\n",
      "DOWN: 24.90% (132490)\n",
      "LEFT: 19.97% (106288)\n",
      "RIGHT: 25.49% (135629)\n",
      "BOMB: 8.64% (45984)\n",
      "WAIT: 1.52% (8102)\n",
      "[3, 1,   200] loss: 120.316\n",
      "[3, 1,   400] loss: 111.276\n",
      "[3, 1,   600] loss: 103.465\n",
      "[3, 1] loss: 96.600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [01:02<00:00,  1.55s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rule_based_agent_0    511\n",
      "rule_based_agent_1    514\n",
      "rule_based_agent_2    440\n",
      "rule_based_agent_3    463\n",
      "Loaded 545976 actions.\n",
      "545976 actions in the training set:\n",
      "UP: 19.88% (108550)\n",
      "DOWN: 25.34% (138351)\n",
      "LEFT: 19.68% (107432)\n",
      "RIGHT: 24.87% (135770)\n",
      "BOMB: 8.58% (46830)\n",
      "WAIT: 1.66% (9043)\n",
      "[4, 1,   200] loss: 109.717\n",
      "[4, 1,   400] loss: 102.905\n",
      "[4, 1,   600] loss: 96.635\n",
      "[4, 1] loss: 92.557\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [01:29<00:00,  2.23s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rule_based_agent_0    494\n",
      "rule_based_agent_1    512\n",
      "rule_based_agent_2    516\n",
      "rule_based_agent_3    449\n",
      "Loaded 521097 actions.\n",
      "521097 actions in the training set:\n",
      "UP: 19.36% (100892)\n",
      "DOWN: 25.10% (130794)\n",
      "LEFT: 19.78% (103064)\n",
      "RIGHT: 25.50% (132905)\n",
      "BOMB: 8.56% (44620)\n",
      "WAIT: 1.69% (8822)\n",
      "[5, 1,   200] loss: 106.748\n",
      "[5, 1,   400] loss: 96.791\n",
      "[5, 1,   600] loss: 91.922\n",
      "[5, 1] loss: 88.345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [01:24<00:00,  2.12s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rule_based_agent_0    473\n",
      "rule_based_agent_1    498\n",
      "rule_based_agent_2    502\n",
      "rule_based_agent_3    480\n",
      "Loaded 510504 actions.\n",
      "510504 actions in the training set:\n",
      "UP: 19.40% (99063)\n",
      "DOWN: 24.98% (127509)\n",
      "LEFT: 19.86% (101374)\n",
      "RIGHT: 25.64% (130870)\n",
      "BOMB: 8.56% (43702)\n",
      "WAIT: 1.56% (7986)\n",
      "[6, 1,   200] loss: 100.977\n",
      "[6, 1,   400] loss: 91.007\n",
      "[6, 1,   600] loss: 85.719\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [01:25<00:00,  2.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rule_based_agent_0    472\n",
      "rule_based_agent_1    468\n",
      "rule_based_agent_2    562\n",
      "rule_based_agent_3    475\n",
      "Loaded 524460 actions.\n",
      "524460 actions in the training set:\n",
      "UP: 19.40% (101723)\n",
      "DOWN: 24.98% (131019)\n",
      "LEFT: 19.75% (103564)\n",
      "RIGHT: 25.50% (133735)\n",
      "BOMB: 8.74% (45817)\n",
      "WAIT: 1.64% (8602)\n",
      "[7, 1,   200] loss: 102.720\n",
      "[7, 1,   400] loss: 91.762\n",
      "[7, 1,   600] loss: 83.406\n",
      "[7, 1] loss: 81.811\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [01:47<00:00,  2.68s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rule_based_agent_0    530\n",
      "rule_based_agent_1    457\n",
      "rule_based_agent_2    513\n",
      "rule_based_agent_3    453\n",
      "Loaded 524907 actions.\n",
      "524907 actions in the training set:\n",
      "UP: 19.25% (101040)\n",
      "DOWN: 25.37% (133179)\n",
      "LEFT: 19.78% (103842)\n",
      "RIGHT: 25.50% (133838)\n",
      "BOMB: 8.54% (44845)\n",
      "WAIT: 1.56% (8163)\n",
      "[8, 1,   200] loss: 96.309\n",
      "[8, 1,   400] loss: 88.023\n",
      "[8, 1,   600] loss: 83.030\n",
      "[8, 1] loss: 75.959\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [01:45<00:00,  2.65s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rule_based_agent_0    577\n",
      "rule_based_agent_1    496\n",
      "rule_based_agent_2    442\n",
      "rule_based_agent_3    428\n",
      "Loaded 520692 actions.\n",
      "520692 actions in the training set:\n",
      "UP: 20.03% (104279)\n",
      "DOWN: 25.72% (133910)\n",
      "LEFT: 19.19% (99937)\n",
      "RIGHT: 25.01% (130203)\n",
      "BOMB: 8.49% (44219)\n",
      "WAIT: 1.56% (8144)\n",
      "[9, 1,   200] loss: 100.961\n",
      "[9, 1,   400] loss: 86.638\n",
      "[9, 1,   600] loss: 79.860\n",
      "[9, 1] loss: 69.878\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [01:07<00:00,  1.69s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rule_based_agent_0    465\n",
      "rule_based_agent_1    501\n",
      "rule_based_agent_2    499\n",
      "rule_based_agent_3    508\n",
      "Loaded 506860 actions.\n",
      "506860 actions in the training set:\n",
      "UP: 19.83% (100530)\n",
      "DOWN: 25.10% (127240)\n",
      "LEFT: 19.36% (98129)\n",
      "RIGHT: 25.36% (128537)\n",
      "BOMB: 8.70% (44083)\n",
      "WAIT: 1.65% (8341)\n",
      "[10, 1,   200] loss: 96.255\n",
      "[10, 1,   400] loss: 82.778\n",
      "[10, 1] loss: 76.884\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "for run in range(10):\n",
    "    # os.system(\"rm -rf ../rule_based_agent/data\")\n",
    "    os.system(\"rm -rf ../data\")\n",
    "    os.system(\"cd ../..; python main.py play --no-gui --agents rule_based_agent rule_based_agent rule_based_agent rule_based_agent --train 4 --n-rounds 40 --scenario loot-crate\")\n",
    "\n",
    "    trainset = BombermanDataset(data_path)\n",
    "    N = len(trainset)\n",
    "    print(f\"{N} actions in the training set:\")\n",
    "    for i in range(6):\n",
    "        n = np.sum([t == i for t in trainset.actions])\n",
    "        print(f\"{ACTION_MAP_INV[i]}: {n/N*100:.2f}% ({n})\")\n",
    "    # sampler = BatchSampler(WeightedRandomSampler(trainset.weights, len(trainset), replacement=True,), batch_size, False)\n",
    "    # trainloader = torch.utils.data.DataLoader(trainset, batch_sampler=sampler)\n",
    "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    for epoch in range(num_epoch_per_run):\n",
    "        running_loss = 0.0\n",
    "        running = 0\n",
    "        for i, batch in enumerate(trainloader, 0):\n",
    "        # for i, batch in enumerate(sampler, 0):\n",
    "            print(i, end=\"\\r\")\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "            (channels, features), actions, rewards = batch\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "    \n",
    "            # forward + backward + optimize\n",
    "            outputs = net(channels, features)\n",
    "            # loss = criterion(outputs, labels)\n",
    "            log_logits = F.log_softmax(outputs, dim=-1)\n",
    "            log_probs = log_logits.gather(1, actions[:,np.newaxis])\n",
    "            \n",
    "            loss = torch.mean(-log_probs * rewards)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "            # print statistics\n",
    "            running_loss += loss.item()\n",
    "            running += 1\n",
    "            if i % 200 == 199:    # print every 2000 mini-batches\n",
    "                print(f'[{run + 1}, {epoch + 1}, {i + 1:5d}] loss: {running_loss / running:.3f}')\n",
    "                torch.save(net.state_dict(), weight_path)\n",
    "                running_loss = 0.0\n",
    "                running = 0\n",
    "                #print(log_probs)\n",
    "                #print(rewards)\n",
    "            if i*batch_size/N > 0.15:\n",
    "                break\n",
    "        if running > 0:\n",
    "            print(f'[{run + 1}, {epoch + 1}] loss: {running_loss / running:.3f}')\n",
    "    torch.save(net.state_dict(), weight_path)\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "edd27a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(net.state_dict(), \"models/model_weights_pre\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3859dd3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.AdamW(net.parameters(), lr=0.0001)\n",
    "batch_size = 256 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "999bc580",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [01:15<00:00,  1.51s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rule_based_agent_0    638\n",
      "rule_based_agent_1    580\n",
      "rule_based_agent_2    561\n",
      "rule_based_agent_3    633\n",
      "Loaded 822416 actions.\n",
      "822416 actions in the training set:\n",
      "UP: 19.91% (163734)\n",
      "DOWN: 25.51% (209824)\n",
      "LEFT: 19.23% (158179)\n",
      "RIGHT: 24.89% (204718)\n",
      "BOMB: 8.64% (71082)\n",
      "WAIT: 1.81% (14879)\n",
      "[1, 1,   200] loss: 102.332\n",
      "[1, 1,   400] loss: 93.699\n",
      "[1, 1,   600] loss: 90.158\n",
      "[1, 1,   800] loss: 84.334\n",
      "[1, 1] loss: 84.447\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [01:16<00:00,  1.53s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rule_based_agent_0    645\n",
      "rule_based_agent_1    602\n",
      "rule_based_agent_2    603\n",
      "rule_based_agent_3    579\n",
      "Loaded 826954 actions.\n",
      "826954 actions in the training set:\n",
      "UP: 19.63% (162332)\n",
      "DOWN: 25.31% (209305)\n",
      "LEFT: 19.67% (162673)\n",
      "RIGHT: 25.15% (208009)\n",
      "BOMB: 8.66% (71576)\n",
      "WAIT: 1.58% (13059)\n",
      "[2, 1,   200] loss: 95.647\n",
      "[2, 1,   400] loss: 88.308\n",
      "[2, 1,   600] loss: 84.899\n",
      "[2, 1,   800] loss: 81.897\n",
      "[2, 1] loss: 76.017\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [01:14<00:00,  1.49s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rule_based_agent_0    597\n",
      "rule_based_agent_1    621\n",
      "rule_based_agent_2    571\n",
      "rule_based_agent_3    590\n",
      "Loaded 820584 actions.\n",
      "820584 actions in the training set:\n",
      "UP: 19.63% (161094)\n",
      "DOWN: 25.33% (207830)\n",
      "LEFT: 19.33% (158648)\n",
      "RIGHT: 25.48% (209124)\n",
      "BOMB: 8.66% (71065)\n",
      "WAIT: 1.56% (12823)\n",
      "[3, 1,   200] loss: 95.899\n",
      "[3, 1,   400] loss: 87.939\n",
      "[3, 1,   600] loss: 84.908\n",
      "[3, 1,   800] loss: 79.018\n",
      "[3, 1] loss: 87.280\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [01:17<00:00,  1.54s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rule_based_agent_0    571\n",
      "rule_based_agent_1    617\n",
      "rule_based_agent_2    597\n",
      "rule_based_agent_3    605\n",
      "Loaded 831811 actions.\n",
      "831811 actions in the training set:\n",
      "UP: 19.77% (164475)\n",
      "DOWN: 25.00% (207934)\n",
      "LEFT: 19.53% (162474)\n",
      "RIGHT: 25.45% (211686)\n",
      "BOMB: 8.64% (71857)\n",
      "WAIT: 1.61% (13385)\n",
      "[4, 1,   200] loss: 93.526\n",
      "[4, 1,   400] loss: 88.204\n",
      "[4, 1,   600] loss: 82.349\n",
      "[4, 1,   800] loss: 78.860\n",
      "[4, 1] loss: 74.927\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [01:25<00:00,  1.70s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rule_based_agent_0    576\n",
      "rule_based_agent_1    592\n",
      "rule_based_agent_2    584\n",
      "rule_based_agent_3    645\n",
      "Loaded 770813 actions.\n",
      "770813 actions in the training set:\n",
      "UP: 19.51% (150406)\n",
      "DOWN: 25.19% (194175)\n",
      "LEFT: 19.45% (149928)\n",
      "RIGHT: 25.36% (195470)\n",
      "BOMB: 8.71% (67151)\n",
      "WAIT: 1.78% (13683)\n",
      "[5, 1,   200] loss: 85.886\n",
      "[5, 1,   400] loss: 78.865\n",
      "[5, 1,   600] loss: 75.416\n",
      "[5, 1] loss: 72.070\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [02:26<00:00,  2.93s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rule_based_agent_0    617\n",
      "rule_based_agent_1    563\n",
      "rule_based_agent_2    572\n",
      "rule_based_agent_3    634\n",
      "Loaded 817075 actions.\n",
      "817075 actions in the training set:\n",
      "UP: 19.83% (162007)\n",
      "DOWN: 25.41% (207615)\n",
      "LEFT: 19.41% (158622)\n",
      "RIGHT: 24.98% (204074)\n",
      "BOMB: 8.67% (70805)\n",
      "WAIT: 1.71% (13952)\n",
      "[6, 1,   200] loss: 93.683\n",
      "[6, 1,   400] loss: 87.006\n",
      "[6, 1,   600] loss: 82.005\n",
      "[6, 1] loss: 76.662\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [02:45<00:00,  3.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rule_based_agent_0    636\n",
      "rule_based_agent_1    586\n",
      "rule_based_agent_2    527\n",
      "rule_based_agent_3    680\n",
      "Loaded 800842 actions.\n",
      "800842 actions in the training set:\n",
      "UP: 19.87% (159141)\n",
      "DOWN: 25.66% (205524)\n",
      "LEFT: 19.07% (152717)\n",
      "RIGHT: 25.04% (200542)\n",
      "BOMB: 8.60% (68852)\n",
      "WAIT: 1.76% (14066)\n",
      "[7, 1,   200] loss: 86.340\n",
      "[7, 1,   400] loss: 80.279\n",
      "[7, 1,   600] loss: 75.939\n",
      "[7, 1] loss: 71.134\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [02:58<00:00,  3.57s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rule_based_agent_0    641\n",
      "rule_based_agent_1    632\n",
      "rule_based_agent_2    584\n",
      "rule_based_agent_3    613\n",
      "Loaded 825463 actions.\n",
      "825463 actions in the training set:\n",
      "UP: 20.06% (165581)\n",
      "DOWN: 25.75% (212520)\n",
      "LEFT: 19.14% (157967)\n",
      "RIGHT: 24.73% (204165)\n",
      "BOMB: 8.70% (71808)\n",
      "WAIT: 1.63% (13422)\n",
      "[8, 1,   200] loss: 89.819\n",
      "[8, 1,   400] loss: 84.729\n",
      "[8, 1,   600] loss: 78.933\n",
      "[8, 1,   800] loss: 74.801\n",
      "[8, 1] loss: 73.780\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [03:00<00:00,  3.61s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rule_based_agent_0    618\n",
      "rule_based_agent_1    606\n",
      "rule_based_agent_2    581\n",
      "rule_based_agent_3    597\n",
      "Loaded 811923 actions.\n",
      "811923 actions in the training set:\n",
      "UP: 19.70% (159965)\n",
      "DOWN: 25.45% (206639)\n",
      "LEFT: 19.38% (157357)\n",
      "RIGHT: 25.13% (204045)\n",
      "BOMB: 8.65% (70239)\n",
      "WAIT: 1.68% (13678)\n",
      "[9, 1,   200] loss: 88.642\n",
      "[9, 1,   400] loss: 81.953\n",
      "[9, 1,   600] loss: 77.387\n",
      "[9, 1] loss: 73.486\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [02:34<00:00,  3.09s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rule_based_agent_0    625\n",
      "rule_based_agent_1    614\n",
      "rule_based_agent_2    557\n",
      "rule_based_agent_3    618\n",
      "Loaded 837987 actions.\n",
      "837987 actions in the training set:\n",
      "UP: 19.93% (166986)\n",
      "DOWN: 25.69% (215248)\n",
      "LEFT: 19.37% (162302)\n",
      "RIGHT: 24.72% (207122)\n",
      "BOMB: 8.67% (72661)\n",
      "WAIT: 1.63% (13668)\n",
      "[10, 1,   200] loss: 90.694\n",
      "[10, 1,   400] loss: 83.622\n",
      "[10, 1,   600] loss: 78.916\n",
      "[10, 1,   800] loss: 75.312\n",
      "[10, 1] loss: 72.207\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "for run in range(10):\n",
    "    # os.system(\"rm -rf ../rule_based_agent/data\")\n",
    "    os.system(\"rm -rf ../data\")\n",
    "    os.system(\"cd ../..; python main.py play --no-gui --agents rule_based_agent rule_based_agent rule_based_agent rule_based_agent --train 4 --n-rounds 50 --scenario loot-crate\")\n",
    "\n",
    "    trainset = BombermanDataset(data_path)\n",
    "    N = len(trainset)\n",
    "    print(f\"{N} actions in the training set:\")\n",
    "    for i in range(6):\n",
    "        n = np.sum([t == i for t in trainset.actions])\n",
    "        print(f\"{ACTION_MAP_INV[i]}: {n/N*100:.2f}% ({n})\")\n",
    "    # sampler = BatchSampler(WeightedRandomSampler(trainset.weights, len(trainset), replacement=True,), batch_size, False)\n",
    "    # trainloader = torch.utils.data.DataLoader(trainset, batch_sampler=sampler)\n",
    "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    for epoch in range(num_epoch_per_run):\n",
    "        running_loss = 0.0\n",
    "        running = 0\n",
    "        for i, batch in enumerate(trainloader, 0):\n",
    "        # for i, batch in enumerate(sampler, 0):\n",
    "            print(i, end=\"\\r\")\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "            (channels, features), actions, rewards = batch\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "    \n",
    "            # forward + backward + optimize\n",
    "            outputs = net(channels, features)\n",
    "            # loss = criterion(outputs, labels)\n",
    "            log_logits = F.log_softmax(outputs, dim=-1)\n",
    "            log_probs = log_logits.gather(1, actions[:,np.newaxis])\n",
    "            \n",
    "            loss = torch.mean(-log_probs * rewards)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "            # print statistics\n",
    "            running_loss += loss.item()\n",
    "            running += 1\n",
    "            if i % 200 == 199:    # print every 2000 mini-batches\n",
    "                print(f'[{run + 1}, {epoch + 1}, {i + 1:5d}] loss: {running_loss / running:.3f}')\n",
    "                torch.save(net.state_dict(), weight_path)\n",
    "                running_loss = 0.0\n",
    "                running = 0\n",
    "                #print(log_probs)\n",
    "                #print(rewards)\n",
    "            if i*batch_size/N > 0.25:\n",
    "                break\n",
    "        if running > 0:\n",
    "            print(f'[{run + 1}, {epoch + 1}] loss: {running_loss / running:.3f}')\n",
    "    torch.save(net.state_dict(), weight_path)\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7a3b3c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(net.state_dict(), \"models/model_weights_fine\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1197e789",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "data_path = f\"../data/\"\n",
    "weight_path = \"models/model_weights.pth\"\n",
    "num_epoch_per_run = 1\n",
    "\n",
    "# optimizer = optim.SGD(net.parameters(), momentum=0.9, lr=0.001)\n",
    "net = AgentNet()\n",
    "#torch.save(net.state_dict(), weight_path)\n",
    "#net.load_state_dict(torch.load(\"models/model_weights.pth\"))\n",
    "\n",
    "optimizer = optim.AdamW(net.parameters(), lr=0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0748e488",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60/60 [01:19<00:00,  1.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "basic_agent    609\n",
      "rule_based_agent_0    794\n",
      "rule_based_agent_1    752\n",
      "rule_based_agent_2    735\n",
      "Loaded 294534 actions.\n",
      "294534 actions in the training set:\n",
      "UP: 19.95% (58755)\n",
      "DOWN: 25.74% (75803)\n",
      "LEFT: 19.36% (57014)\n",
      "RIGHT: 25.08% (73856)\n",
      "BOMB: 8.48% (24976)\n",
      "WAIT: 1.40% (4130)\n",
      "[1, 1,   200] loss: 79.986\n",
      "[1, 1,   400] loss: 79.855\n",
      "[1, 1] loss: 78.929\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60/60 [01:22<00:00,  1.38s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "basic_agent    639\n",
      "rule_based_agent_0    725\n",
      "rule_based_agent_1    779\n",
      "rule_based_agent_2    754\n",
      "Loaded 284585 actions.\n",
      "284585 actions in the training set:\n",
      "UP: 19.54% (55620)\n",
      "DOWN: 26.09% (74247)\n",
      "LEFT: 19.28% (54855)\n",
      "RIGHT: 25.29% (71978)\n",
      "BOMB: 8.41% (23930)\n",
      "WAIT: 1.39% (3955)\n",
      "[2, 1,   200] loss: 86.051\n",
      "[2, 1,   400] loss: 86.843\n",
      "[2, 1] loss: 85.522\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60/60 [01:18<00:00,  1.31s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "basic_agent    584\n",
      "rule_based_agent_0    825\n",
      "rule_based_agent_1    767\n",
      "rule_based_agent_2    770\n",
      "Loaded 293086 actions.\n",
      "293086 actions in the training set:\n",
      "UP: 19.31% (56609)\n",
      "DOWN: 25.94% (76036)\n",
      "LEFT: 19.32% (56626)\n",
      "RIGHT: 25.49% (74721)\n",
      "BOMB: 8.58% (25154)\n",
      "WAIT: 1.34% (3940)\n",
      "[3, 1,   200] loss: 82.996\n",
      "[3, 1,   400] loss: 81.969\n",
      "[3, 1] loss: 81.688\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60/60 [01:26<00:00,  1.44s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "basic_agent    630\n",
      "rule_based_agent_0    677\n",
      "rule_based_agent_1    851\n",
      "rule_based_agent_2    742\n",
      "Loaded 295559 actions.\n",
      "295559 actions in the training set:\n",
      "UP: 19.08% (56406)\n",
      "DOWN: 25.40% (75073)\n",
      "LEFT: 19.87% (58738)\n",
      "RIGHT: 25.71% (76001)\n",
      "BOMB: 8.58% (25357)\n",
      "WAIT: 1.35% (3984)\n",
      "[4, 1,   200] loss: 82.551\n",
      "[4, 1,   400] loss: 83.323\n",
      "[4, 1] loss: 83.069\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "for run in range(4):\n",
    "    # os.system(\"rm -rf ../rule_based_agent/data\")\n",
    "    os.system(\"rm -rf ../data\")\n",
    "    os.system(\"cd ../..; python main.py play --no-gui --agents basic_agent rule_based_agent rule_based_agent rule_based_agent --train 4 --n-rounds 60 --scenario loot-crate --save-winner-game True\")\n",
    "\n",
    "    trainset = BombermanDataset(data_path)\n",
    "    N = len(trainset)\n",
    "    print(f\"{N} actions in the training set:\")\n",
    "    for i in range(6):\n",
    "        n = np.sum([t == i for t in trainset.actions])\n",
    "        print(f\"{ACTION_MAP_INV[i]}: {n/N*100:.2f}% ({n})\")\n",
    "    # sampler = BatchSampler(WeightedRandomSampler(trainset.weights, len(trainset), replacement=True,), batch_size, False)\n",
    "    # trainloader = torch.utils.data.DataLoader(trainset, batch_sampler=sampler)\n",
    "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    for epoch in range(num_epoch_per_run):\n",
    "        running_loss = 0.0\n",
    "        running = 0\n",
    "        for i, batch in enumerate(trainloader, 0):\n",
    "        # for i, batch in enumerate(sampler, 0):\n",
    "            print(i, end=\"\\r\")\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "            (channels, features), actions, rewards = batch\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "    \n",
    "            # forward + backward + optimize\n",
    "            outputs = net(channels, features)\n",
    "            # loss = criterion(outputs, labels)\n",
    "            log_logits = F.log_softmax(outputs, dim=-1)\n",
    "            log_probs = log_logits.gather(1, actions[:,np.newaxis])\n",
    "            \n",
    "            loss = torch.mean(-log_probs * rewards)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "            # print statistics\n",
    "            running_loss += loss.item()\n",
    "            running += 1\n",
    "            if i % 200 == 199:    # print every 2000 mini-batches\n",
    "                print(f'[{run + 1}, {epoch + 1}, {i + 1:5d}] loss: {running_loss / running:.3f}')\n",
    "                torch.save(net.state_dict(), weight_path)\n",
    "                running_loss = 0.0\n",
    "                running = 0\n",
    "                #print(log_probs)\n",
    "                #print(rewards)\n",
    "            if i*batch_size/N > 0.5:\n",
    "                break\n",
    "        if running > 0:\n",
    "            print(f'[{run + 1}, {epoch + 1}] loss: {running_loss / running:.3f}')\n",
    "    torch.save(net.state_dict(), weight_path)\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "25463da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(net.state_dict(), \"models/model_weights_winner.pth\")\n",
    "\n",
    "\n",
    "#weight_path_ref = \"models/model_weights_pretrained_ref.pth\"\n",
    "#torch.save(net.state_dict(),weight_path_ref)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06b297f6",
   "metadata": {},
   "source": [
    "## batch_size = 64\n",
    "data_path = f\"../data/\"\n",
    "weight_path = \"models/model_weights.pth\"\n",
    "pretrained_weight_path = \"models/model_weights_pretrained.pth\"\n",
    "num_epoch_per_run = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "5ab0b0cd-1b41-460c-b3d5-bfb026b34e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pretrained_net = AgentNet()\n",
    "# pretrained_net.load_state_dict(torch.load(pretrained_weight_path))\n",
    "# torch.save(pretrained_net.state_dict(), weigth_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "7f5db558-2318-4b20-8385-a775144067cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = AgentNet()\n",
    "pretrained_net = AgentNet()\n",
    "pretrained_net.load_state_dict(torch.load(pretrained_weight_path))\n",
    "net.cnn.load_state_dict(pretrained_net.cnn.state_dict())\n",
    "torch.save(net.state_dict(), weight_path)\n",
    "optimizer = optim.AdamW(net.mlp.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "4fea4551-3f68-4965-bf24-adca6abf13f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.AdamW(net.mlp.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e02d0f",
   "metadata": {},
   "source": [
    "EVENT_HORIZON = {\n",
    "    MOVED_LEFT: 1,\n",
    "    MOVED_RIGHT: 1,\n",
    "    MOVED_UP: 1,\n",
    "    MOVED_DOWN: 1,\n",
    "    WAITED: 1,\n",
    "    INVALID_ACTION: 1,\n",
    "    BOMB_DROPPED: 1,\n",
    "    BOMB_EXPLODED: 1,\n",
    "    CRATE_DESTROYED: 5,\n",
    "    COIN_FOUND: 3,\n",
    "    COIN_COLLECTED: 8,\n",
    "    KILLED_OPPONENT: 8,\n",
    "    KILLED_SELF: 4,\n",
    "    GOT_KILLED: 3,\n",
    "    OPPONENT_ELIMINATED: 7,\n",
    "    SURVIVED_ROUND: 5\n",
    "}\n",
    "\n",
    "\n",
    "REWARDS = {\n",
    "    MOVED_LEFT: -0.5,  #-0.2\n",
    "    MOVED_RIGHT: -0.5, #-0.2\n",
    "    MOVED_UP: -0.5,  #-0.2\n",
    "    MOVED_DOWN: -0.5,  #-0.2    \n",
    "    WAITED: 4,\n",
    "    INVALID_ACTION: -2,\n",
    "    BOMB_DROPPED: 0,\n",
    "    BOMB_EXPLODED: 1,\n",
    "    CRATE_DESTROYED: 2,\n",
    "    COIN_FOUND: 0,\n",
    "    COIN_COLLECTED: 6,\n",
    "    KILLED_OPPONENT: 1,\n",
    "    KILLED_SELF: -10,\n",
    "    GOT_KILLED: -1,\n",
    "    OPPONENT_ELIMINATED: 2,\n",
    "    SURVIVED_ROUND: 9\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "77764d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "action_probs = np.zeros((6,500))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "7ee713ae-2c19-4c00-9eb0-daf9d05fd540",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:58<00:00,  1.94s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 298683 actions.\n",
      "298683 actions in the training set:\n",
      "UP: 15.22% (45452)\n",
      "DOWN: 34.32% (102520)\n",
      "LEFT: 16.40% (48982)\n",
      "RIGHT: 26.48% (79098)\n",
      "BOMB: 6.45% (19274)\n",
      "WAIT: 1.12% (3357)\n",
      "[1, 1,   200] loss: 20.545\n",
      "[1, 1,   400] loss: 17.363\n",
      "[1, 1,   600] loss: 15.881\n",
      "[1, 1,   800] loss: 15.734\n",
      "[1, 1,  1000] loss: 14.239\n",
      "[1, 1,  1200] loss: 13.750\n",
      "[1, 1,  1400] loss: 13.503\n",
      "[1, 1,  1600] loss: 13.106\n",
      "[1, 1,  1800] loss: 12.162\n",
      "[1, 1,  2000] loss: 12.178\n",
      "[1, 1,  2200] loss: 11.820\n",
      "[1, 1,  2400] loss: 11.852\n",
      "[1, 1,  2600] loss: 11.281\n",
      "[1, 1,  2800] loss: 11.148\n",
      "[1, 1,  3000] loss: 10.654\n",
      "[1, 1,  3200] loss: 11.071\n",
      "[1, 1,  3400] loss: 10.446\n",
      "[1, 1,  3600] loss: 10.402\n",
      "[1, 1,  3800] loss: 9.904\n",
      "[1, 1,  4000] loss: 9.635\n",
      "[1, 1,  4200] loss: 9.458\n",
      "[1, 1,  4400] loss: 9.511\n",
      "[1, 1,  4600] loss: 9.639\n",
      "[1, 1] loss: 9.649\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:58<00:00,  1.94s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 275019 actions.\n",
      "275019 actions in the training set:\n",
      "UP: 14.84% (40818)\n",
      "DOWN: 35.45% (97483)\n",
      "LEFT: 16.15% (44411)\n",
      "RIGHT: 26.31% (72357)\n",
      "BOMB: 6.03% (16597)\n",
      "WAIT: 1.22% (3353)\n",
      "[2, 1,   200] loss: 15.642\n",
      "[2, 1,   400] loss: 12.522\n",
      "[2, 1,   600] loss: 10.814\n",
      "[2, 1,   800] loss: 10.197\n",
      "[2, 1,  1000] loss: 9.779\n",
      "[2, 1,  1200] loss: 8.961\n",
      "[2, 1,  1400] loss: 8.457\n",
      "[2, 1,  1600] loss: 8.409\n",
      "[2, 1,  1800] loss: 8.007\n",
      "[2, 1,  2000] loss: 7.834\n",
      "[2, 1,  2200] loss: 7.641\n",
      "[2, 1,  2400] loss: 7.661\n",
      "[2, 1,  2600] loss: 7.253\n",
      "[2, 1,  2800] loss: 6.938\n",
      "[2, 1,  3000] loss: 6.859\n",
      "[2, 1,  3200] loss: 7.091\n",
      "[2, 1,  3400] loss: 7.851\n",
      "[2, 1,  3600] loss: 6.596\n",
      "[2, 1,  3800] loss: 7.056\n",
      "[2, 1,  4000] loss: 6.786\n",
      "[2, 1,  4200] loss: 6.336\n",
      "[2, 1] loss: 5.841\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [01:02<00:00,  2.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 296943 actions.\n",
      "296943 actions in the training set:\n",
      "UP: 14.74% (43777)\n",
      "DOWN: 34.50% (102442)\n",
      "LEFT: 15.75% (46779)\n",
      "RIGHT: 27.90% (82858)\n",
      "BOMB: 5.79% (17200)\n",
      "WAIT: 1.31% (3887)\n",
      "[3, 1,   200] loss: 13.117\n",
      "[3, 1,   400] loss: 10.529\n",
      "[3, 1,   600] loss: 8.514\n",
      "[3, 1,   800] loss: 7.821\n",
      "[3, 1,  1000] loss: 7.589\n",
      "[3, 1,  1200] loss: 7.152\n",
      "[3, 1,  1400] loss: 6.555\n",
      "[3, 1,  1600] loss: 6.769\n",
      "[3, 1,  1800] loss: 6.426\n",
      "[3, 1,  2000] loss: 6.444\n",
      "[3, 1,  2200] loss: 6.472\n",
      "[3, 1,  2400] loss: 6.159\n",
      "[3, 1,  2600] loss: 5.700\n",
      "[3, 1,  2800] loss: 6.195\n",
      "[3, 1,  3000] loss: 6.146\n",
      "[3, 1,  3200] loss: 6.141\n",
      "[3, 1,  3400] loss: 5.716\n",
      "[3, 1,  3600] loss: 5.535\n",
      "[3, 1,  3800] loss: 5.579\n",
      "[3, 1,  4000] loss: 5.579\n",
      "[3, 1,  4200] loss: 6.154\n",
      "[3, 1,  4400] loss: 5.372\n",
      "[3, 1,  4600] loss: 5.222\n",
      "[3, 1] loss: 6.239\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:53<00:00,  1.80s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 288673 actions.\n",
      "288673 actions in the training set:\n",
      "UP: 14.39% (41553)\n",
      "DOWN: 41.43% (119600)\n",
      "LEFT: 15.19% (43854)\n",
      "RIGHT: 22.47% (64863)\n",
      "BOMB: 5.37% (15504)\n",
      "WAIT: 1.14% (3299)\n",
      "[4, 1,   200] loss: 10.355\n",
      "[4, 1,   400] loss: 8.255\n",
      "[4, 1,   600] loss: 7.762\n",
      "[4, 1,   800] loss: 6.706\n",
      "[4, 1,  1000] loss: 6.530\n",
      "[4, 1,  1200] loss: 6.148\n",
      "[4, 1,  1400] loss: 5.509\n",
      "[4, 1,  1600] loss: 5.585\n",
      "[4, 1,  1800] loss: 5.632\n",
      "[4, 1,  2000] loss: 5.073\n",
      "[4, 1,  2200] loss: 4.864\n",
      "[4, 1,  2400] loss: 5.536\n",
      "[4, 1,  2600] loss: 5.197\n",
      "[4, 1,  2800] loss: 4.650\n",
      "[4, 1,  3000] loss: 5.106\n",
      "[4, 1,  3200] loss: 5.531\n",
      "[4, 1,  3400] loss: 4.926\n",
      "[4, 1,  3600] loss: 4.624\n",
      "[4, 1,  3800] loss: 4.378\n",
      "[4, 1,  4000] loss: 4.987\n",
      "[4, 1,  4200] loss: 4.764\n",
      "[4, 1,  4400] loss: 4.636\n",
      "[4, 1] loss: 4.888\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:52<00:00,  1.76s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 278948 actions.\n",
      "278948 actions in the training set:\n",
      "UP: 13.04% (36378)\n",
      "DOWN: 39.66% (110624)\n",
      "LEFT: 14.77% (41196)\n",
      "RIGHT: 26.79% (74744)\n",
      "BOMB: 4.77% (13313)\n",
      "WAIT: 0.97% (2693)\n",
      "[5, 1,   200] loss: 11.025\n",
      "[5, 1,   400] loss: 8.949\n",
      "[5, 1,   600] loss: 7.630\n",
      "[5, 1,   800] loss: 7.161\n",
      "[5, 1,  1000] loss: 6.631\n",
      "[5, 1,  1200] loss: 6.287\n",
      "[5, 1,  1400] loss: 6.397\n",
      "[5, 1,  1600] loss: 5.681\n",
      "[5, 1,  1800] loss: 6.000\n",
      "[5, 1,  2000] loss: 6.066\n",
      "[5, 1,  2200] loss: 5.989\n",
      "[5, 1,  2400] loss: 5.975\n",
      "[5, 1,  2600] loss: 5.652\n",
      "[5, 1,  2800] loss: 5.766\n",
      "[5, 1,  3000] loss: 5.277\n",
      "[5, 1,  3200] loss: 5.607\n",
      "[5, 1,  3400] loss: 5.522\n",
      "[5, 1,  3600] loss: 5.495\n",
      "[5, 1,  3800] loss: 5.514\n",
      "[5, 1,  4000] loss: 5.363\n",
      "[5, 1,  4200] loss: 5.473\n",
      "[5, 1] loss: 5.809\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:51<00:00,  1.71s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 261623 actions.\n",
      "261623 actions in the training set:\n",
      "UP: 15.45% (40432)\n",
      "DOWN: 39.18% (102493)\n",
      "LEFT: 14.66% (38343)\n",
      "RIGHT: 23.93% (62594)\n",
      "BOMB: 5.39% (14107)\n",
      "WAIT: 1.40% (3654)\n",
      "[6, 1,   200] loss: 10.541\n",
      "[6, 1,   400] loss: 8.960\n",
      "[6, 1,   600] loss: 7.116\n",
      "[6, 1,   800] loss: 6.318\n",
      "[6, 1,  1000] loss: 6.269\n",
      "[6, 1,  1200] loss: 5.886\n",
      "[6, 1,  1400] loss: 6.083\n",
      "[6, 1,  1600] loss: 5.624\n",
      "[6, 1,  1800] loss: 5.623\n",
      "[6, 1,  2000] loss: 5.538\n",
      "[6, 1,  2200] loss: 5.588\n",
      "[6, 1,  2400] loss: 5.382\n",
      "[6, 1,  2600] loss: 5.600\n",
      "[6, 1,  2800] loss: 5.469\n",
      "[6, 1,  3000] loss: 5.517\n",
      "[6, 1,  3200] loss: 5.241\n",
      "[6, 1,  3400] loss: 5.374\n",
      "[6, 1,  3600] loss: 5.240\n",
      "[6, 1,  3800] loss: 5.201\n",
      "[6, 1,  4000] loss: 5.039\n",
      "[6, 1] loss: 5.375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:55<00:00,  1.83s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 266770 actions.\n",
      "266770 actions in the training set:\n",
      "UP: 15.18% (40502)\n",
      "DOWN: 41.63% (111067)\n",
      "LEFT: 13.93% (37161)\n",
      "RIGHT: 22.58% (60247)\n",
      "BOMB: 5.43% (14481)\n",
      "WAIT: 1.24% (3312)\n",
      "[7, 1,   200] loss: 10.172\n",
      "[7, 1,   400] loss: 8.596\n",
      "[7, 1,   600] loss: 7.097\n",
      "[7, 1,   800] loss: 6.640\n",
      "[7, 1,  1000] loss: 6.194\n",
      "[7, 1,  1200] loss: 6.160\n",
      "[7, 1,  1400] loss: 5.763\n",
      "[7, 1,  1600] loss: 5.387\n",
      "[7, 1,  1800] loss: 5.189\n",
      "[7, 1,  2000] loss: 5.195\n",
      "[7, 1,  2200] loss: 5.018\n",
      "[7, 1,  2400] loss: 5.369\n",
      "[7, 1,  2600] loss: 4.919\n",
      "[7, 1,  2800] loss: 5.100\n",
      "[7, 1,  3000] loss: 5.002\n",
      "[7, 1,  3200] loss: 5.368\n",
      "[7, 1,  3400] loss: 4.856\n",
      "[7, 1,  3600] loss: 5.708\n",
      "[7, 1,  3800] loss: 5.186\n",
      "[7, 1,  4000] loss: 4.962\n",
      "[7, 1] loss: 4.603\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:52<00:00,  1.74s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 272686 actions.\n",
      "272686 actions in the training set:\n",
      "UP: 14.62% (39861)\n",
      "DOWN: 41.77% (113900)\n",
      "LEFT: 11.12% (30333)\n",
      "RIGHT: 26.36% (71867)\n",
      "BOMB: 4.95% (13500)\n",
      "WAIT: 1.18% (3225)\n",
      "[8, 1,   200] loss: 8.618\n",
      "[8, 1,   400] loss: 7.104\n",
      "[8, 1,   600] loss: 6.021\n",
      "[8, 1,   800] loss: 5.576\n",
      "[8, 1,  1000] loss: 5.072\n",
      "[8, 1,  1200] loss: 4.652\n",
      "[8, 1,  1400] loss: 4.711\n",
      "[8, 1,  1600] loss: 4.287\n",
      "[8, 1,  1800] loss: 4.299\n",
      "[8, 1,  2000] loss: 4.500\n",
      "[8, 1,  2200] loss: 4.388\n",
      "[8, 1,  2400] loss: 4.180\n",
      "[8, 1,  2600] loss: 4.070\n",
      "2624\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_28285/2862909088.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     43\u001b[0m             \"\"\"\n\u001b[1;32m     44\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlog_probs\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    485\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m             )\n\u001b[0;32m--> 487\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    488\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m         )\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    198\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    201\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for run in range(10):\n",
    "    # os.system(\"rm -rf ../rule_based_agent/data\")\n",
    "    os.system(\"rm -rf ../data\")\n",
    "    #os.system(\"cd ../..; python main.py play --no-gui --agents basic_agent basic_agent basic_agent basic_agent --train 4 --n-rounds 30 --scenario loot-crate\")\n",
    "    os.system(\"cd ../..; python main.py play --no-gui --agents basic_agent basic_agent basic_agent basic_agent  --train 4 --n-rounds 30 --scenario loot-crate\")\n",
    "\n",
    "    # os.system(\"cd ../..; python main.py play --no-gui --agents rule_based_agent rule_based_agent rule_based_agent rule_based_agent --train 4 --n-rounds 10 --scenario classic\")\n",
    "\n",
    "    trainset = BombermanDataset(data_path)\n",
    "    N = len(trainset)\n",
    "    print(f\"{N} actions in the training set:\")\n",
    "    for i in range(6):\n",
    "        n = np.sum([t == i for t in trainset.actions])\n",
    "        print(f\"{ACTION_MAP_INV[i]}: {n/N*100:.2f}% ({n})\")\n",
    "        action_probs[i,run] = n/N\n",
    "    \n",
    "    \n",
    "    # sampler = BatchSampler(WeightedRandomSampler(trainset.weights, len(trainset), replacement=True,), batch_size, False)\n",
    "    # trainloader = torch.utils.data.DataLoader(trainset, batch_sampler=sampler)\n",
    "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    for epoch in range(num_epoch_per_run):\n",
    "        running_loss = 0.0\n",
    "        running = 0\n",
    "        for i, batch in enumerate(trainloader, 0):\n",
    "        # for i, batch in enumerate(sampler, 0):\n",
    "            print(i, end=\"\\r\")\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "            (channels, features), actions, rewards = batch\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "    \n",
    "            # forward + backward + optimize\n",
    "            outputs = net(channels, features)\n",
    "            # loss = criterion(outputs, labels)\n",
    "            log_logits = F.log_softmax(outputs, dim=-1)\n",
    "            log_probs = log_logits.gather(1, actions[:,np.newaxis])\n",
    "            \"\"\"\n",
    "            if i == 0:\n",
    "                print(actions, rewards, log_probs)\n",
    "                print(-log_probs * rewards)\n",
    "            \"\"\"\n",
    "            loss = torch.mean(-log_probs * rewards)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "            # print statistics\n",
    "            running_loss += loss.item()\n",
    "            running += 1\n",
    "            if i % 200 == 199:    # print every 2000 mini-batches\n",
    "                print(f'[{run + 1}, {epoch + 1}, {i + 1:5d}] loss: {running_loss / running:.3f}')\n",
    "                torch.save(net.state_dict(), weight_path)\n",
    "                running_loss = 0.0\n",
    "                running = 0\n",
    "        if running > 0:\n",
    "            print(f'[{run + 1}, {epoch + 1}] loss: {running_loss / running:.3f}')\n",
    "    torch.save(net.state_dict(), weight_path)\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1e729cf5-51d3-499c-a029-dcc14cc8954c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0. 0. 0. 0.]]\n",
      "[0. 0. 0. 0. 0. 0. 0.]\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "a = np.zeros((1,7))\n",
    "print(a)\n",
    "print(a[0])\n",
    "print(a[0,0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "2c53db6b-c671-4001-a638-de64bbee8438",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3 4 5 6]\n",
      "[1 2 3 4 5 6]\n"
     ]
    }
   ],
   "source": [
    "a = np.array([1,2,3,4,5,6,7])-1\n",
    "print(a)\n",
    "print(a[1:7])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d258b3d",
   "metadata": {},
   "source": [
    "\n",
    "    extensions = np.zeros(4)\n",
    "    if pos[0]+4 <= 16:             #field, box_map, explosion_map, others_map, coin_map\n",
    "        if field[pos[0]+4, pos[1]] == 1:\n",
    "            extensions[0] = 1\n",
    "        if box_map[pos[0]+4, pos[1]] ==1:\n",
    "            extensions[0] = 2\n",
    "        if explosion_map[pos[0]+4, pos[1]] ==1:\n",
    "            extensions[0] = 3\n",
    "        if others_map[pos[0]+4, pos[1]] == 1:\n",
    "            extensions[0] = 4\n",
    "        if coin_map[pos[0]+4, pos[1]] ==1:\n",
    "            extensions[0] =5\n",
    "\n",
    "    if pos[0]-4 >= 0:             #field, box_map, explosion_map, others_map, coin_map\n",
    "        if field[pos[0]-4, pos[1]] == 1:\n",
    "            extensions[1] = 1\n",
    "        if box_map[pos[0]-4, pos[1]] ==1:\n",
    "            extensions[1] = 2\n",
    "        if explosion_map[pos[0]+4, pos[1]] ==1:\n",
    "            extensions[1] = 3\n",
    "        if others_map[pos[0]-4, pos[1]] == 1:\n",
    "            extensions[1] = 4\n",
    "        if coin_map[pos[0]-4, pos[1]] ==1:\n",
    "            extensions[1] =5\n",
    "\n",
    "    if pos[1]+4 <= 16:             #field, box_map, explosion_map, others_map, coin_map\n",
    "        if field[pos[0], pos[1]+4] == 1:\n",
    "            extensions[2] = 1\n",
    "        if box_map[pos[0], pos[1]+4] ==1:\n",
    "            extensions[2] = 2\n",
    "        if explosion_map[pos[0], pos[1]+4] ==1:\n",
    "            extensions[2] = 3\n",
    "        if others_map[pos[0], pos[1]+4] == 1:\n",
    "            extensions[2] = 4\n",
    "        if coin_map[pos[0], pos[1]+4] ==1:\n",
    "            extensions[2] = 5\n",
    "\n",
    "    if pos[1]-4 >= 0:             #field, box_map, explosion_map, others_map, coin_map\n",
    "        if field[pos[0], pos[1]-4] == 1:\n",
    "            extensions[3] = 1\n",
    "        if box_map[pos[0], pos[1]-4] ==1:\n",
    "            extensions[3] = 2\n",
    "        if explosion_map[pos[0], pos[1]-4] ==1:\n",
    "            extensions[3] = 3\n",
    "        if others_map[pos[0], pos[1]-4] == 1:\n",
    "            extensions[3] = 4\n",
    "        if coin_map[pos[0], pos[1]-4] ==1:\n",
    "            extensions[3] = 5\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
