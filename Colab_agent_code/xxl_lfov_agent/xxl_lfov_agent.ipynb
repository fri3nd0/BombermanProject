{"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/gdrive')\n","%cd /content/gdrive/MyDrive/bomber_xxl_lfov/"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WcWQ0JLRsrM7","executionInfo":{"status":"ok","timestamp":1696182282466,"user_tz":-120,"elapsed":2448,"user":{"displayName":"Dominic Holst","userId":"11810639222428697240"}},"outputId":"2121d5bb-90a4-43c3-dff1-e800bdb6de19"},"id":"WcWQ0JLRsrM7","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n","/content/gdrive/MyDrive/bomber_xxl_lfov\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"vOcPdjcps_QS"},"id":"vOcPdjcps_QS","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"id":"bbe88005-be9f-46e7-8600-019ed0bc7dcd","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bbe88005-be9f-46e7-8600-019ed0bc7dcd","executionInfo":{"status":"ok","timestamp":1696182283456,"user_tz":-120,"elapsed":257,"user":{"displayName":"Dominic Holst","userId":"11810639222428697240"}},"outputId":"8b8ac43f-14f3-473c-a6bd-e957b16354fc"},"outputs":[{"output_type":"stream","name":"stdout","text":["The autoreload extension is already loaded. To reload it, use:\n","  %reload_ext autoreload\n"]}],"source":["%load_ext autoreload\n","%autoreload 2\n","import sys\n","sys.path.append(r\"../..\")\n","\n","\n","from torch import nn\n","from torch.nn import functional as F\n","import torch.optim as optim\n","import torch\n","import os\n","import pickle\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","from callbacks import state_to_features, ACTION_MAP, ACTION_MAP_INV\n","from networks import AgentNet\n","\n","from events import *"]},{"cell_type":"code","source":["%cd agent_code/xxl_lfov_agent\n","!ls"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Olg7Y6nYsqZZ","executionInfo":{"status":"ok","timestamp":1696182499578,"user_tz":-120,"elapsed":320,"user":{"displayName":"Dominic Holst","userId":"11810639222428697240"}},"outputId":"1e9d3b5c-edd7-458b-9637-c696e4e8832a"},"id":"Olg7Y6nYsqZZ","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/gdrive/MyDrive/bomber_xxl_lfov/agent_code/xxl_lfov_agent\n","callbacks.py  models\t   __pycache__\txxl_lfov_agent.ipynb\n","logs\t      networks.py  train.py\n"]}]},{"cell_type":"code","source":["data_path = f\"/mountpoint/data/\"\n","!sudo mkdir /mountpoint\n","!sudo mount -t tmpfs -o size=5000M none /mountpoint\n"],"metadata":{"id":"IPynSTtYsc6L"},"id":"IPynSTtYsc6L","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"id":"7369f91a-b21e-407c-bced-63e441be02a3","metadata":{"id":"7369f91a-b21e-407c-bced-63e441be02a3"},"outputs":[],"source":["from torch.utils.data import Dataset\n","\n","\n","class BombermanDataset(Dataset):\n","    def __init__(self, states_dir, max_game_step=200, min_reward=1):\n","        self.features = []\n","        self.actions = []\n","        self.cum_rewards = []\n","        self.total_number = 0\n","        for states_file in os.listdir(states_dir):\n","\n","            if not states_file.endswith('.pickle'):\n","                continue\n","            with open(os.path.join(states_dir, states_file), \"rb\") as f:\n","                data = pickle.load(f)\n","            self.total_number += len(data['game_state'])\n","            last_round = -1\n","            running_horizons = []\n","            running_rewards = []\n","            for i in range(len(data['game_state'])-1,-1,-1):\n","                game_state = data['game_state'][i]\n","\n","\n","                if last_round != game_state['round']:\n","                    last_round = game_state['round']\n","                    running_horizons = []\n","                    running_rewards = []\n","                if game_state['step'] > max_game_step:\n","                    continue\n","\n","                # determine if action should be included in training set\n","\n","                horizons = [EVENT_HORIZON[e] for e in data['events'][i]]\n","                rewards = [REWARDS[e] for e in data['events'][i]]\n","                cr = np.where(data['events'] == \"CRATE_DESTROYED\")\n","                #print(cr)\n","                #rewards[np.array(cr[-1:], dtype = \"int\")] == 0\n","                #for j in cr[1:]:\n","                #    rewards[j] = 0\n","\n","                running_rewards = [r for j,r in enumerate(running_rewards) if running_horizons[j]>1]\n","                running_horizons = [h-1 for h in running_horizons if h > 1]\n","                running_rewards.extend(rewards)\n","                running_horizons.extend(horizons)\n","                #if \"INVALID_ACTION\" in data['events'][i]:\n","                #    continue\n","                self.cum_rewards.append(np.sum(running_rewards))\n","\n","                last_action = None if game_state['step'] == 1 else data['action'][i-1]\n","                #print(\"LAST ACTION:\")\n","                #print(last_action)\n","\n","                channels, features, act_map = state_to_features(game_state, last_action=last_action)\n","                action = ACTION_MAP[act_map[data['action'][i]]]\n","                self.features.append((\n","                    torch.tensor(channels[np.newaxis], dtype=torch.float),\n","                    torch.tensor(features, dtype=torch.float)\n","                ))\n","                self.actions.append(action)\n","\n","\n","        self.cum_rewards = torch.tensor(self.cum_rewards, dtype=torch.float)\n","        self.n = np.array([np.sum([t == i for t in self.actions]) for i in range(6)])\n","        self.weights = np.array([1/self.n[t] for t in self.actions])\n","        self.proportion_selected = len(self.actions)/self.total_number\n","        print(f\"Loaded {len(self.actions)} actions.\")\n","\n","    def __len__(self):\n","        return len(self.features)\n","\n","    def __getitem__(self, idx):\n","        return self.features[idx], self.actions[idx], self.cum_rewards[idx]"]},{"cell_type":"code","execution_count":null,"id":"944b4018-e9a4-45ce-825b-41f22809126e","metadata":{"id":"944b4018-e9a4-45ce-825b-41f22809126e"},"outputs":[],"source":["batch_size = 64\n","data_path = f\"/mountpoint/data/\"\n","weight_path = \"models/model_weights_pretrained.pth\"\n","num_epoch_per_run = 1\n","\n","# optimizer = optim.SGD(net.parameters(), momentum=0.9, lr=0.001)\n","net = AgentNet().to(\"cuda\")\n","net.load_state_dict(torch.load(weight_path))\n","optimizer = optim.AdamW(net.parameters(), lr=0.001)"]},{"cell_type":"code","execution_count":null,"id":"7b090f68","metadata":{"id":"7b090f68"},"outputs":[],"source":["optimizer = optim.AdamW(net.parameters(), lr=0.001)\n","batch_size = 128\n"]},{"cell_type":"code","execution_count":null,"id":"a87534bb-9e02-4b62-8053-194d219d0ff0","metadata":{"id":"a87534bb-9e02-4b62-8053-194d219d0ff0"},"outputs":[],"source":["EVENT_HORIZON = {\n","    MOVED_LEFT: 1,\n","    MOVED_RIGHT: 1,\n","    MOVED_UP: 1,\n","    MOVED_DOWN: 1,\n","    WAITED: 1,\n","    INVALID_ACTION: 1,\n","    BOMB_DROPPED: 1,\n","    BOMB_EXPLODED: 1,\n","    CRATE_DESTROYED: 6,\n","    COIN_FOUND: 1,\n","    COIN_COLLECTED: 5,\n","    KILLED_OPPONENT: 6,\n","    KILLED_SELF: 5,\n","    GOT_KILLED: 6,\n","    OPPONENT_ELIMINATED: 4,\n","    SURVIVED_ROUND: 100\n","}\n","\n","\n","REWARDS = {\n","    MOVED_LEFT: 5, #1\n","    MOVED_RIGHT: 5, #1\n","    MOVED_UP: 5,   #1\n","    MOVED_DOWN: 5,  #1\n","    WAITED: 5,  #1\n","    INVALID_ACTION: -120, #-7\n","    BOMB_DROPPED: 15,  #1\n","    BOMB_EXPLODED: 0,\n","    CRATE_DESTROYED: 120,\n","    COIN_FOUND: 0,\n","    COIN_COLLECTED: 120,\n","    KILLED_OPPONENT: 60,\n","    KILLED_SELF: -120, #-12\n","    GOT_KILLED: -120,\n","    OPPONENT_ELIMINATED: 5,\n","    SURVIVED_ROUND: 200 #20\n","}"]},{"cell_type":"code","execution_count":null,"id":"2f25651b-d431-4812-be1b-1419aeccbf0b","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2f25651b-d431-4812-be1b-1419aeccbf0b","executionInfo":{"status":"ok","timestamp":1695822549774,"user_tz":-120,"elapsed":362692,"user":{"displayName":"Dominic Holst","userId":"11810639222428697240"}},"outputId":"d0810a53-07b6-4e39-f57a-848bf16b5824"},"outputs":[{"output_type":"stream","name":"stdout","text":["Loaded 529109 actions.\n","529109 actions in the training set:\n","UP: 19.96% (105602)\n","DOWN: 25.46% (134736)\n","LEFT: 19.29% (102054)\n","RIGHT: 25.01% (132326)\n","BOMB: 8.70% (46047)\n","WAIT: 1.58% (8344)\n","[1, 1,   200] loss: 324.458\n","0.048141309257638785\n","[1, 1,   400] loss: 160.018\n","0.09652453464220039\n","[1, 1,   600] loss: 133.981\n","0.14490776002676198\n","[1, 1] loss: 129.101\n","Loaded 513058 actions.\n","513058 actions in the training set:\n","UP: 19.47% (99918)\n","DOWN: 25.08% (128692)\n","LEFT: 19.53% (100225)\n","RIGHT: 25.49% (130782)\n","BOMB: 8.60% (44100)\n","WAIT: 1.82% (9341)\n","[2, 1,   200] loss: 130.920\n","0.04964740828522311\n","[2, 1,   400] loss: 116.345\n","0.09954430103419107\n","[2, 1,   600] loss: 107.156\n","0.149441193783159\n","[2, 1] loss: 106.879\n","Loaded 495144 actions.\n","495144 actions in the training set:\n","UP: 19.28% (95468)\n","DOWN: 25.21% (124812)\n","LEFT: 19.49% (96481)\n","RIGHT: 25.62% (126833)\n","BOMB: 8.71% (43107)\n","WAIT: 1.71% (8443)\n","[3, 1,   200] loss: 108.114\n","0.051443620441730085\n","[3, 1,   400] loss: 94.073\n","0.10314575153894624\n","[3, 1] loss: 88.538\n","Loaded 505726 actions.\n","505726 actions in the training set:\n","UP: 20.24% (102349)\n","DOWN: 25.68% (129878)\n","LEFT: 19.10% (96580)\n","RIGHT: 24.58% (124296)\n","BOMB: 8.64% (43687)\n","WAIT: 1.77% (8936)\n","[4, 1,   200] loss: 104.714\n","0.0503671948841863\n","[4, 1,   400] loss: 93.859\n","0.10098749125020268\n","[4, 1] loss: 85.892\n","Loaded 515840 actions.\n","515840 actions in the training set:\n","UP: 19.77% (101968)\n","DOWN: 25.04% (129166)\n","LEFT: 19.66% (101439)\n","RIGHT: 25.29% (130436)\n","BOMB: 8.61% (44417)\n","WAIT: 1.63% (8414)\n","[5, 1,   200] loss: 101.790\n","0.049379652605459054\n","[5, 1,   400] loss: 92.921\n","0.09900744416873448\n","[5, 1,   600] loss: 81.261\n","0.14863523573200993\n","[5, 1] loss: 67.606\n","Loaded 533534 actions.\n","533534 actions in the training set:\n","UP: 19.89% (106101)\n","DOWN: 25.48% (135950)\n","LEFT: 19.41% (103569)\n","RIGHT: 25.00% (133377)\n","BOMB: 8.67% (46269)\n","WAIT: 1.55% (8268)\n","[6, 1,   200] loss: 101.170\n","0.047742037058556716\n","[6, 1,   400] loss: 90.328\n","0.09572398385107603\n","[6, 1,   600] loss: 85.973\n","0.14370593064359535\n","[6, 1] loss: 83.881\n","Loaded 540207 actions.\n","540207 actions in the training set:\n","UP: 19.76% (106726)\n","DOWN: 25.39% (137163)\n","LEFT: 19.73% (106579)\n","RIGHT: 24.83% (134122)\n","BOMB: 8.70% (46990)\n","WAIT: 1.60% (8627)\n","[7, 1,   200] loss: 97.962\n","0.04715229532382957\n","[7, 1,   400] loss: 87.127\n","0.09454153685531658\n","[7, 1,   600] loss: 82.996\n","0.14193077838680357\n","[7, 1] loss: 78.884\n","Loaded 525402 actions.\n","525402 actions in the training set:\n","UP: 19.32% (101518)\n","DOWN: 25.19% (132330)\n","LEFT: 19.67% (103345)\n","RIGHT: 25.47% (133841)\n","BOMB: 8.66% (45491)\n","WAIT: 1.69% (8877)\n","[8, 1,   200] loss: 100.556\n","0.048480972664740524\n","[8, 1,   400] loss: 83.943\n","0.09720556830769582\n","[8, 1,   600] loss: 77.002\n","0.14593016395065112\n","[8, 1] loss: 73.577\n","Loaded 543787 actions.\n","543787 actions in the training set:\n","UP: 19.88% (108079)\n","DOWN: 25.20% (137030)\n","LEFT: 19.48% (105907)\n","RIGHT: 25.01% (136011)\n","BOMB: 8.78% (47762)\n","WAIT: 1.65% (8998)\n","[9, 1,   200] loss: 97.947\n","0.046841870070450375\n","[9, 1,   400] loss: 83.418\n","0.09391912642266181\n","[9, 1,   600] loss: 78.185\n","0.14099638277487325\n","[9, 1] loss: 74.219\n","Loaded 519096 actions.\n","519096 actions in the training set:\n","UP: 19.70% (102265)\n","DOWN: 25.54% (132576)\n","LEFT: 19.45% (100981)\n","RIGHT: 25.04% (129993)\n","BOMB: 8.52% (44223)\n","WAIT: 1.74% (9058)\n","[10, 1,   200] loss: 94.055\n","0.0490699215559357\n","[10, 1,   400] loss: 79.381\n","0.098386425632253\n","[10, 1,   600] loss: 71.566\n","0.1477029297085703\n","[10, 1] loss: 67.485\n","Finished Training\n"]}],"source":["for run in range(10):\n","    # os.system(\"rm -rf ../rule_based_agent/data\")\n","    os.system(\"rm -rf ../data\")\n","    os.system(\"cd ../..; python main.py play --no-gui --agents rule_based_agent rule_based_agent rule_based_agent rule_based_agent --train 4 --n-rounds 40 --scenario loot-crate\")\n","\n","    trainset = BombermanDataset(data_path)\n","    N = len(trainset)\n","    print(f\"{N} actions in the training set:\")\n","    for i in range(6):\n","        n = np.sum([t == i for t in trainset.actions])\n","        print(f\"{ACTION_MAP_INV[i]}: {n/N*100:.2f}% ({n})\")\n","    # sampler = BatchSampler(WeightedRandomSampler(trainset.weights, len(trainset), replacement=True,), batch_size, False)\n","    # trainloader = torch.utils.data.DataLoader(trainset, batch_sampler=sampler)\n","    trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True)\n","\n","    for epoch in range(num_epoch_per_run):\n","        running_loss = 0.0\n","        running = 0\n","        for i, batch in enumerate(trainloader, 0):\n","        # for i, batch in enumerate(sampler, 0):\n","            print(i, end=\"\\r\")\n","            # get the inputs; data is a list of [inputs, labels]\n","            (channels, features), actions, rewards = batch\n","\n","            channels = channels.to(\"cuda\")\n","            features = features.to(\"cuda\")\n","            actions = actions.to(\"cuda\")\n","            rewards = rewards.to(\"cuda\")\n","\n","            # zero the parameter gradients\n","            optimizer.zero_grad()\n","\n","            # forward + backward + optimize\n","            outputs = net(channels, features)\n","            # loss = criterion(outputs, labels)\n","            log_logits = F.log_softmax(outputs, dim=-1)\n","            log_probs = log_logits.gather(1, actions[:,np.newaxis])\n","\n","            loss = torch.mean(-log_probs * rewards)\n","            loss.backward()\n","            optimizer.step()\n","\n","            # print statistics\n","            running_loss += loss.item()\n","            running += 1\n","            if i % 200 == 199:    # print every 2000 mini-batches\n","                print(f'[{run + 1}, {epoch + 1}, {i + 1:5d}] loss: {running_loss / running:.3f}')\n","                torch.save(net.state_dict(), weight_path)\n","                running_loss = 0.0\n","                running = 0\n","                #print(log_probs)\n","                #print(rewards)\n","                print(i*batch_size/N)\n","            if i*batch_size/N > 0.15:\n","                break\n","        if running > 0:\n","            print(f'[{run + 1}, {epoch + 1}] loss: {running_loss / running:.3f}')\n","\n","    torch.save(net.state_dict(), weight_path)\n","\n","print('Finished Training')"]},{"cell_type":"code","source":["torch.save(net.state_dict(), \"models/model_weights_pre\")"],"metadata":{"id":"TUuLcabGuNBL"},"id":"TUuLcabGuNBL","execution_count":null,"outputs":[]},{"cell_type":"code","source":["net = AgentNet().to(\"cuda\")\n","weight_path = \"models/model_weights.pth\"\n","net.load_state_dict(torch.load(\"models/model_weights_pre\"))\n","optimizer = optim.AdamW(net.parameters(), lr=0.0001)\n","batch_size = 256\n","data_path = f\"/mountpoint/data/\"\n"],"metadata":{"id":"P2AGzxm5R13z"},"id":"P2AGzxm5R13z","execution_count":null,"outputs":[]},{"cell_type":"code","source":["for run in range(10):\n","    # os.system(\"rm -rf ../rule_based_agent/data\")\n","    os.system(\"rm -rf ../data\")\n","    os.system(\"cd ../..; python main.py play --no-gui --agents rule_based_agent rule_based_agent rule_based_agent rule_based_agent --train 4 --n-rounds 40 --scenario loot-crate\")\n","\n","    trainset = BombermanDataset(data_path)\n","    N = len(trainset)\n","    print(f\"{N} actions in the training set:\")\n","    for i in range(6):\n","        n = np.sum([t == i for t in trainset.actions])\n","        print(f\"{ACTION_MAP_INV[i]}: {n/N*100:.2f}% ({n})\")\n","    # sampler = BatchSampler(WeightedRandomSampler(trainset.weights, len(trainset), replacement=True,), batch_size, False)\n","    # trainloader = torch.utils.data.DataLoader(trainset, batch_sampler=sampler)\n","    trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True)\n","\n","    for epoch in range(num_epoch_per_run):\n","        running_loss = 0.0\n","        running = 0\n","        for i, batch in enumerate(trainloader, 0):\n","        # for i, batch in enumerate(sampler, 0):\n","            print(i, end=\"\\r\")\n","            # get the inputs; data is a list of [inputs, labels]\n","            (channels, features), actions, rewards = batch\n","\n","            channels = channels.to(\"cuda\")\n","            features = features.to(\"cuda\")\n","            actions = actions.to(\"cuda\")\n","            rewards = rewards.to(\"cuda\")\n","\n","            # zero the parameter gradients\n","            optimizer.zero_grad()\n","\n","            # forward + backward + optimize\n","            outputs = net(channels, features)\n","            # loss = criterion(outputs, labels)\n","            log_logits = F.log_softmax(outputs, dim=-1)\n","            log_probs = log_logits.gather(1, actions[:,np.newaxis])\n","\n","            loss = torch.mean(-log_probs * rewards)\n","            loss.backward()\n","            optimizer.step()\n","\n","            # print statistics\n","            running_loss += loss.item()\n","            running += 1\n","            if i % 200 == 199:    # print every 2000 mini-batches\n","                print(f'[{run + 1}, {epoch + 1}, {i + 1:5d}] loss: {running_loss / running:.3f}')\n","                torch.save(net.state_dict(), weight_path)\n","                running_loss = 0.0\n","                running = 0\n","                #print(log_probs)\n","                #print(rewards)\n","                print(i*batch_size/N)\n","            if i*batch_size/N > 0.15:\n","                break\n","        if running > 0:\n","            print(f'[{run + 1}, {epoch + 1}] loss: {running_loss / running:.3f}')\n","\n","    torch.save(net.state_dict(), weight_path)\n","\n","print('Finished Training')"],"metadata":{"id":"9b9TP21a4T0m"},"id":"9b9TP21a4T0m","execution_count":null,"outputs":[]},{"cell_type":"code","source":["!ls"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"P2c4rNhQSsI5","executionInfo":{"status":"ok","timestamp":1695829235683,"user_tz":-120,"elapsed":323,"user":{"displayName":"Dominic Holst","userId":"11810639222428697240"}},"outputId":"1a120731-e0a4-4a16-c02d-a4b91eb44fb5"},"id":"P2c4rNhQSsI5","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["callbacks.py  models\t   __pycache__\txxl_lfov_agent.ipynb\n","logs\t      networks.py  train.py\n"]}]},{"cell_type":"code","source":["for run in range(10):\n","    # os.system(\"rm -rf ../rule_based_agent/data\")\n","    os.system(\"rm -rf /mountpoint/data\")\n","    os.system(\"cd ../..; python main.py play --no-gui --agents rule_based_agent rule_based_agent rule_based_agent rule_based_agent --train 4 --n-rounds 50 --scenario loot-crate\")\n","\n","    trainset = BombermanDataset(data_path)\n","    N = len(trainset)\n","    print(f\"{N} actions in the training set:\")\n","    for i in range(6):\n","        n = np.sum([t == i for t in trainset.actions])\n","        print(f\"{ACTION_MAP_INV[i]}: {n/N*100:.2f}% ({n})\")\n","    # sampler = BatchSampler(WeightedRandomSampler(trainset.weights, len(trainset), replacement=True,), batch_size, False)\n","    # trainloader = torch.utils.data.DataLoader(trainset, batch_sampler=sampler)\n","    trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True)\n","\n","    for epoch in range(num_epoch_per_run):\n","        running_loss = 0.0\n","        running = 0\n","        for i, batch in enumerate(trainloader, 0):\n","        # for i, batch in enumerate(sampler, 0):\n","            print(i, end=\"\\r\")\n","            # get the inputs; data is a list of [inputs, labels]\n","            (channels, features), actions, rewards = batch\n","\n","            channels = channels.to(\"cuda\")\n","            features = features.to(\"cuda\")\n","            actions = actions.to(\"cuda\")\n","            rewards = rewards.to(\"cuda\")\n","\n","            # zero the parameter gradients\n","            optimizer.zero_grad()\n","\n","            # forward + backward + optimize\n","            outputs = net(channels, features)\n","            # loss = criterion(outputs, labels)\n","            log_logits = F.log_softmax(outputs, dim=-1)\n","            log_probs = log_logits.gather(1, actions[:,np.newaxis])\n","\n","            loss = torch.mean(-log_probs * rewards)\n","            loss.backward()\n","            optimizer.step()\n","\n","            # print statistics\n","            running_loss += loss.item()\n","            running += 1\n","            if i % 200 == 199:    # print every 2000 mini-batches\n","                print(f'[{run + 1}, {epoch + 1}, {i + 1:5d}] loss: {running_loss / running:.3f}')\n","                torch.save(net.state_dict(), weight_path)\n","                running_loss = 0.0\n","                running = 0\n","                #print(log_probs)\n","                #print(rewards)\n","                print(i*batch_size/N)\n","            if i*batch_size/N > 0.25:\n","                break\n","        if running > 0:\n","            print(f'[{run + 1}, {epoch + 1}] loss: {running_loss / running:.3f}')\n","\n","    torch.save(net.state_dict(), weight_path)\n","\n","print('Finished Training')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"L-DAvFqVR2K6","executionInfo":{"status":"ok","timestamp":1695833704364,"user_tz":-120,"elapsed":4311508,"user":{"displayName":"Dominic Holst","userId":"11810639222428697240"}},"outputId":"289381bc-a72e-4f12-8df3-4be2e4c9023c"},"id":"L-DAvFqVR2K6","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Loaded 804752 actions.\n","804752 actions in the training set:\n","UP: 19.16% (154166)\n","DOWN: 25.12% (202180)\n","LEFT: 19.91% (160196)\n","RIGHT: 25.56% (205703)\n","BOMB: 8.61% (69249)\n","WAIT: 1.65% (13258)\n","[1, 1,   200] loss: 95.510\n","0.06330397439211086\n","[1, 1,   400] loss: 83.552\n","0.1269260592083027\n","[1, 1,   600] loss: 76.160\n","0.1905481440244945\n","[1, 1] loss: 71.195\n","Loaded 813241 actions.\n","813241 actions in the training set:\n","UP: 19.81% (161099)\n","DOWN: 25.74% (209324)\n","LEFT: 19.32% (157092)\n","RIGHT: 24.81% (201764)\n","BOMB: 8.63% (70180)\n","WAIT: 1.69% (13782)\n","[2, 1,   200] loss: 87.619\n","0.06264317711477901\n","[2, 1,   400] loss: 76.648\n","0.12560114406430567\n","[2, 1,   600] loss: 72.246\n","0.1885591110138323\n","[2, 1] loss: 65.978\n","Loaded 798287 actions.\n","798287 actions in the training set:\n","UP: 19.78% (157936)\n","DOWN: 25.76% (205645)\n","LEFT: 19.39% (154825)\n","RIGHT: 24.64% (196660)\n","BOMB: 8.74% (69760)\n","WAIT: 1.69% (13461)\n","[3, 1,   200] loss: 84.072\n","0.06381664739623719\n","[3, 1,   400] loss: 75.739\n","0.12795398146280723\n","[3, 1,   600] loss: 69.143\n","0.19209131552937728\n","[3, 1] loss: 64.381\n","Loaded 784592 actions.\n","784592 actions in the training set:\n","UP: 19.85% (155731)\n","DOWN: 25.63% (201105)\n","LEFT: 19.29% (151354)\n","RIGHT: 25.10% (196922)\n","BOMB: 8.57% (67241)\n","WAIT: 1.56% (12239)\n","[4, 1,   200] loss: 84.104\n","0.06493056263637662\n","[4, 1,   400] loss: 71.015\n","0.1301874095071069\n","[4, 1,   600] loss: 64.211\n","0.19544425637783713\n","[4, 1] loss: 58.138\n","Loaded 783359 actions.\n","783359 actions in the training set:\n","UP: 19.48% (152605)\n","DOWN: 25.27% (197920)\n","LEFT: 19.44% (152262)\n","RIGHT: 25.33% (198414)\n","BOMB: 8.63% (67632)\n","WAIT: 1.85% (14526)\n","[5, 1,   200] loss: 81.594\n","0.06503276275628415\n","[5, 1,   400] loss: 71.873\n","0.1303923233153637\n","[5, 1,   600] loss: 64.796\n","0.19575188387444326\n","[5, 1] loss: 57.397\n","Loaded 800043 actions.\n","800043 actions in the training set:\n","UP: 19.46% (155727)\n","DOWN: 25.13% (201081)\n","LEFT: 19.63% (157053)\n","RIGHT: 25.62% (204997)\n","BOMB: 8.51% (68072)\n","WAIT: 1.64% (13113)\n","[6, 1,   200] loss: 85.042\n","0.06367657738396561\n","[6, 1,   400] loss: 74.145\n","0.12767313756885568\n","[6, 1,   600] loss: 65.190\n","0.19166969775374573\n","[6, 1] loss: 59.524\n","Loaded 802535 actions.\n","802535 actions in the training set:\n","UP: 19.79% (158837)\n","DOWN: 25.46% (204351)\n","LEFT: 19.48% (156326)\n","RIGHT: 25.04% (200951)\n","BOMB: 8.51% (68283)\n","WAIT: 1.72% (13787)\n","[7, 1,   200] loss: 78.654\n","0.06347885138965902\n","[7, 1,   400] loss: 68.595\n","0.12727669198228114\n","[7, 1,   600] loss: 60.542\n","0.19107453257490328\n","[7, 1] loss: 53.841\n","Loaded 829913 actions.\n","829913 actions in the training set:\n","UP: 19.63% (162946)\n","DOWN: 25.33% (210244)\n","LEFT: 19.57% (162425)\n","RIGHT: 25.24% (209501)\n","BOMB: 8.66% (71886)\n","WAIT: 1.56% (12911)\n","[8, 1,   200] loss: 86.343\n","0.06138474755787655\n","[8, 1,   400] loss: 73.556\n","0.12307796118388313\n","[8, 1,   600] loss: 66.351\n","0.18477117480988972\n","[8, 1,   800] loss: 59.236\n","0.2464643884358963\n","[8, 1] loss: 54.631\n","Loaded 839604 actions.\n","839604 actions in the training set:\n","UP: 19.58% (164432)\n","DOWN: 25.33% (212703)\n","LEFT: 19.62% (164709)\n","RIGHT: 25.20% (211554)\n","BOMB: 8.56% (71830)\n","WAIT: 1.71% (14376)\n","[9, 1,   200] loss: 86.047\n","0.06067622355300832\n","[9, 1,   400] loss: 73.773\n","0.12165735275201166\n","[9, 1,   600] loss: 65.455\n","0.182638481951015\n","[9, 1,   800] loss: 57.143\n","0.24361961115001834\n","[9, 1] loss: 52.260\n","Loaded 813922 actions.\n","813922 actions in the training set:\n","UP: 19.38% (157729)\n","DOWN: 25.23% (205374)\n","LEFT: 19.64% (159868)\n","RIGHT: 25.42% (206869)\n","BOMB: 8.66% (70522)\n","WAIT: 1.67% (13560)\n","[10, 1,   200] loss: 83.555\n","0.06259076422556462\n","[10, 1,   400] loss: 70.424\n","0.125496054904524\n","[10, 1,   600] loss: 61.319\n","0.18840134558348343\n","[10, 1] loss: 54.947\n","Finished Training\n"]}]},{"cell_type":"code","execution_count":null,"id":"25463da4","metadata":{"id":"25463da4"},"outputs":[],"source":["torch.save(net.state_dict(), \"models/model_weights_fine.pth\")\n","\n","\n","#weight_path_ref = \"models/model_weights_pretrained_ref.pth\"\n","#torch.save(net.state_dict(),weight_path_ref)"]},{"cell_type":"code","source":["net = AgentNet().to(\"cuda\")\n","weight_path = \"models/model_weights.pth\"\n","net.load_state_dict(torch.load(\"models/model_weights_self.pth\"))\n","\n","optimizer = optim.AdamW(net.parameters(), lr=0.00001)\n","batch_size = 256\n","data_path = f\"/mountpoint/data/\""],"metadata":{"id":"6L0igXRd30pq"},"id":"6L0igXRd30pq","execution_count":null,"outputs":[]},{"cell_type":"code","source":["num_epoch_per_run = 1"],"metadata":{"id":"LGTvRHU76pnn"},"id":"LGTvRHU76pnn","execution_count":null,"outputs":[]},{"cell_type":"code","source":["for run in range(20):\n","    # os.system(\"rm -rf ../rule_based_agent/data\")\n","    os.system(\"rm -rf /mountpoint/data\")\n","    os.system(\"cd ../..; python main.py play --no-gui --agents xxl_lfov_agent rule_based_agent rule_based_agent rule_based_agent --train 4 --n-rounds 60 --scenario loot-crate --save-winner-game True\")\n","\n","    trainset = BombermanDataset(data_path)\n","    N = len(trainset)\n","    print(f\"{N} actions in the training set:\")\n","    for i in range(6):\n","        n = np.sum([t == i for t in trainset.actions])\n","        print(f\"{ACTION_MAP_INV[i]}: {n/N*100:.2f}% ({n})\")\n","    # sampler = BatchSampler(WeightedRandomSampler(trainset.weights, len(trainset), replacement=True,), batch_size, False)\n","    # trainloader = torch.utils.data.DataLoader(trainset, batch_sampler=sampler)\n","    trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True)\n","\n","    for epoch in range(1):\n","        running_loss = 0.0\n","        running = 0\n","        for i, batch in enumerate(trainloader, 0):\n","        # for i, batch in enumerate(sampler, 0):\n","            print(i, end=\"\\r\")\n","            # get the inputs; data is a list of [inputs, labels]\n","            (channels, features), actions, rewards = batch\n","\n","            channels = channels.to(\"cuda\")\n","            features = features.to(\"cuda\")\n","            actions = actions.to(\"cuda\")\n","            rewards = rewards.to(\"cuda\")\n","\n","            # zero the parameter gradients\n","            optimizer.zero_grad()\n","\n","            # forward + backward + optimize\n","            outputs = net(channels, features)\n","            # loss = criterion(outputs, labels)\n","            log_logits = F.log_softmax(outputs, dim=-1)\n","            log_probs = log_logits.gather(1, actions[:,np.newaxis])\n","\n","            loss = torch.mean(-log_probs * rewards)\n","            loss.backward()\n","            optimizer.step()\n","\n","            # print statistics\n","            running_loss += loss.item()\n","            running += 1\n","            if i % 200 == 199:    # print every 2000 mini-batches\n","                print(f'[{run + 1}, {epoch + 1}, {i + 1:5d}] loss: {running_loss / running:.3f}')\n","                torch.save(net.state_dict(), weight_path)\n","                running_loss = 0.0\n","                running = 0\n","                savenet = AgentNet()\n","                savenet.load_state_dict(torch.load(weight_path, map_location = \"cpu\"))\n","                torch.save(savenet.state_dict(), \"models/model_weights.pth\")\n","                #print(log_probs)\n","                #print(rewards)\n","                print(i*batch_size/N)\n","            if i*batch_size/N > 0.5:\n","                break\n","        if running > 0:\n","            print(f'[{run + 1}, {epoch + 1}] loss: {running_loss / running:.3f}')\n","\n","    torch.save(net.state_dict(), weight_path)\n","    savenet = AgentNet()\n","    savenet.load_state_dict(torch.load(weight_path, map_location = \"cpu\"))\n","    torch.save(savenet.state_dict(), \"models/model_weights.pth\")\n","\n","print('Finished Training')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FJXV5ZLE4x4k","executionInfo":{"status":"ok","timestamp":1696190353875,"user_tz":-120,"elapsed":6898461,"user":{"displayName":"Dominic Holst","userId":"11810639222428697240"}},"outputId":"3b90cb71-fba3-4cf5-ae31-b5239af556d7"},"id":"FJXV5ZLE4x4k","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Loaded 285695 actions.\n","285695 actions in the training set:\n","UP: 19.63% (56071)\n","DOWN: 26.16% (74736)\n","LEFT: 19.32% (55203)\n","RIGHT: 25.29% (72250)\n","BOMB: 8.38% (23933)\n","WAIT: 1.23% (3502)\n","[1, 1,   200] loss: 76.926\n","0.17831603633245244\n","[1, 1,   400] loss: 75.326\n","0.35752813314898757\n","[1, 1] loss: 74.392\n","Loaded 291523 actions.\n","291523 actions in the training set:\n","UP: 19.65% (57290)\n","DOWN: 25.84% (75322)\n","LEFT: 19.43% (56629)\n","RIGHT: 25.29% (73732)\n","BOMB: 8.42% (24541)\n","WAIT: 1.38% (4009)\n","[2, 1,   200] loss: 78.573\n","0.1747512203153782\n","[2, 1,   400] loss: 78.185\n","0.3503805874665121\n","[2, 1] loss: 77.113\n","Loaded 301682 actions.\n","301682 actions in the training set:\n","UP: 19.52% (58890)\n","DOWN: 25.51% (76974)\n","LEFT: 19.61% (59157)\n","RIGHT: 25.60% (77241)\n","BOMB: 8.40% (25332)\n","WAIT: 1.36% (4088)\n","[3, 1,   200] loss: 73.909\n","0.16886655484914578\n","[3, 1,   400] loss: 73.350\n","0.33858168535079985\n","[3, 1] loss: 73.306\n","Loaded 300916 actions.\n","300916 actions in the training set:\n","UP: 19.45% (58531)\n","DOWN: 25.63% (77134)\n","LEFT: 19.69% (59257)\n","RIGHT: 25.50% (76726)\n","BOMB: 8.38% (25222)\n","WAIT: 1.34% (4046)\n","[4, 1,   200] loss: 79.356\n","0.16929641494636377\n","[4, 1,   400] loss: 78.534\n","0.33944356564622685\n","[4, 1] loss: 77.719\n","Loaded 290996 actions.\n","290996 actions in the training set:\n","UP: 19.45% (56594)\n","DOWN: 25.80% (75086)\n","LEFT: 19.27% (56086)\n","RIGHT: 25.62% (74544)\n","BOMB: 8.35% (24311)\n","WAIT: 1.50% (4375)\n","[5, 1,   200] loss: 78.814\n","0.17506769852506562\n","[5, 1,   400] loss: 74.165\n","0.3510151342286492\n","[5, 1] loss: 74.265\n","Loaded 297127 actions.\n","297127 actions in the training set:\n","UP: 19.41% (57663)\n","DOWN: 25.46% (75646)\n","LEFT: 19.67% (58433)\n","RIGHT: 25.83% (76735)\n","BOMB: 8.31% (24695)\n","WAIT: 1.33% (3955)\n","[6, 1,   200] loss: 77.420\n","0.1714553036243761\n","[6, 1,   400] loss: 76.085\n","0.3437721916890757\n","[6, 1] loss: 75.726\n","Loaded 297845 actions.\n","297845 actions in the training set:\n","UP: 19.07% (56810)\n","DOWN: 26.26% (78204)\n","LEFT: 19.31% (57520)\n","RIGHT: 25.82% (76892)\n","BOMB: 8.26% (24597)\n","WAIT: 1.28% (3822)\n","[7, 1,   200] loss: 78.873\n","0.1710419849250449\n","[7, 1,   400] loss: 78.689\n","0.34294347731202474\n","[7, 1] loss: 76.859\n","Loaded 300045 actions.\n","300045 actions in the training set:\n","UP: 19.38% (58147)\n","DOWN: 25.04% (75134)\n","LEFT: 19.80% (59412)\n","RIGHT: 25.92% (77773)\n","BOMB: 8.52% (25559)\n","WAIT: 1.34% (4020)\n","[8, 1,   200] loss: 76.656\n","0.16978786515356029\n","[8, 1,   400] loss: 75.824\n","0.34042893565965104\n","[8, 1] loss: 74.593\n","Loaded 300131 actions.\n","300131 actions in the training set:\n","UP: 19.75% (59283)\n","DOWN: 26.04% (78167)\n","LEFT: 19.11% (57343)\n","RIGHT: 25.47% (76450)\n","BOMB: 8.37% (25113)\n","WAIT: 1.26% (3775)\n","[9, 1,   200] loss: 80.219\n","0.1697392138766072\n","[9, 1,   400] loss: 78.699\n","0.3403313886269662\n","[9, 1] loss: 77.769\n","Loaded 302213 actions.\n","302213 actions in the training set:\n","UP: 19.40% (58623)\n","DOWN: 25.61% (77397)\n","LEFT: 19.44% (58744)\n","RIGHT: 25.59% (77333)\n","BOMB: 8.56% (25857)\n","WAIT: 1.41% (4259)\n","[10, 1,   200] loss: 79.537\n","0.16856984974173844\n","[10, 1,   400] loss: 76.166\n","0.33798678415554595\n","[10, 1] loss: 74.378\n","Loaded 295914 actions.\n","295914 actions in the training set:\n","UP: 19.41% (57430)\n","DOWN: 25.83% (76430)\n","LEFT: 19.54% (57832)\n","RIGHT: 25.56% (75635)\n","BOMB: 8.42% (24911)\n","WAIT: 1.24% (3676)\n","[11, 1,   200] loss: 73.617\n","0.17215812702339192\n","[11, 1,   400] loss: 72.976\n","0.3451813702629818\n","[11, 1] loss: 72.105\n","Loaded 286894 actions.\n","286894 actions in the training set:\n","UP: 19.59% (56200)\n","DOWN: 26.17% (75090)\n","LEFT: 19.21% (55110)\n","RIGHT: 25.37% (72786)\n","BOMB: 8.47% (24287)\n","WAIT: 1.19% (3421)\n","[12, 1,   200] loss: 79.208\n","0.1775708101249939\n","[12, 1,   400] loss: 75.742\n","0.3560339358787566\n","[12, 1] loss: 74.465\n","Loaded 294110 actions.\n","294110 actions in the training set:\n","UP: 19.23% (56555)\n","DOWN: 26.00% (76471)\n","LEFT: 19.42% (57112)\n","RIGHT: 25.89% (76131)\n","BOMB: 8.25% (24278)\n","WAIT: 1.21% (3563)\n","[13, 1,   200] loss: 79.591\n","0.17321410356669273\n","[13, 1,   400] loss: 78.338\n","0.34729862976437387\n","[13, 1] loss: 77.046\n","Loaded 291274 actions.\n","291274 actions in the training set:\n","UP: 19.47% (56709)\n","DOWN: 25.77% (75062)\n","LEFT: 19.48% (56736)\n","RIGHT: 25.45% (74122)\n","BOMB: 8.39% (24449)\n","WAIT: 1.44% (4196)\n","[14, 1,   200] loss: 75.171\n","0.17490060904852475\n","[14, 1,   400] loss: 74.999\n","0.35068011562995666\n","[14, 1] loss: 74.455\n","Loaded 299988 actions.\n","299988 actions in the training set:\n","UP: 19.35% (58034)\n","DOWN: 26.09% (78255)\n","LEFT: 19.55% (58645)\n","RIGHT: 25.72% (77161)\n","BOMB: 8.12% (24348)\n","WAIT: 1.18% (3545)\n","[15, 1,   200] loss: 79.848\n","0.16982012613837888\n","[15, 1,   400] loss: 77.712\n","0.3404936197447898\n","[15, 1] loss: 76.291\n","Loaded 304701 actions.\n","304701 actions in the training set:\n","UP: 19.43% (59189)\n","DOWN: 25.83% (78697)\n","LEFT: 19.51% (59456)\n","RIGHT: 25.63% (78085)\n","BOMB: 8.35% (25457)\n","WAIT: 1.25% (3817)\n","[16, 1,   200] loss: 75.402\n","0.167193412558541\n","[16, 1,   400] loss: 75.135\n","0.33522699301938624\n","[16, 1] loss: 72.720\n","Loaded 290710 actions.\n","290710 actions in the training set:\n","UP: 19.56% (56862)\n","DOWN: 26.05% (75733)\n","LEFT: 19.21% (55848)\n","RIGHT: 25.38% (73786)\n","BOMB: 8.39% (24397)\n","WAIT: 1.40% (4084)\n","[17, 1,   200] loss: 75.998\n","0.17523992982697534\n","[17, 1,   400] loss: 73.609\n","0.3513604623163978\n","[17, 1] loss: 74.577\n","Loaded 293892 actions.\n","293892 actions in the training set:\n","UP: 19.31% (56759)\n","DOWN: 25.86% (75992)\n","LEFT: 19.28% (56651)\n","RIGHT: 25.77% (75740)\n","BOMB: 8.46% (24854)\n","WAIT: 1.33% (3896)\n","[18, 1,   200] loss: 76.788\n","0.17334258843384645\n","[18, 1,   400] loss: 74.340\n","0.3475562451512801\n","[18, 1] loss: 73.817\n","Loaded 287802 actions.\n","287802 actions in the training set:\n","UP: 19.70% (56701)\n","DOWN: 26.23% (75487)\n","LEFT: 19.22% (55319)\n","RIGHT: 25.18% (72478)\n","BOMB: 8.42% (24240)\n","WAIT: 1.24% (3577)\n","[19, 1,   200] loss: 76.000\n","0.17701058366515868\n","[19, 1,   400] loss: 75.558\n","0.3549106677507453\n","[19, 1] loss: 72.321\n","Loaded 286290 actions.\n","286290 actions in the training set:\n","UP: 19.57% (56017)\n","DOWN: 25.43% (72801)\n","LEFT: 19.56% (55996)\n","RIGHT: 25.41% (72753)\n","BOMB: 8.56% (24519)\n","WAIT: 1.47% (4204)\n","[20, 1,   200] loss: 72.714\n","0.17794543993852388\n","[20, 1,   400] loss: 70.542\n","0.3567850780676936\n","[20, 1] loss: 70.040\n","Finished Training\n"]}]},{"cell_type":"markdown","source":["**The above losses do not match what we reported because we ran this step again to see whether there would be a further performance gain.**"],"metadata":{"id":"8MXy4-lNtnqL"},"id":"8MXy4-lNtnqL"},{"cell_type":"code","source":["torch.save(net.state_dict(), \"models/model_weights_next.pth\")\n"],"metadata":{"id":"UgeyW1wM5RXz"},"id":"UgeyW1wM5RXz","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"id":"0136c04c-7b35-43b8-a5e1-9a83f30dc43c","metadata":{"id":"0136c04c-7b35-43b8-a5e1-9a83f30dc43c"},"outputs":[],"source":["batch_size = 64\n","data_path = f\"../data/\"\n","weight_path = \"models/model_weights.pth\"\n","pretrained_weight_path = \"models/model_weights_pretrained.pth\"\n","num_epoch_per_run = 1"]},{"cell_type":"code","execution_count":null,"id":"5ab0b0cd-1b41-460c-b3d5-bfb026b34e2c","metadata":{"id":"5ab0b0cd-1b41-460c-b3d5-bfb026b34e2c"},"outputs":[],"source":["# pretrained_net = AgentNet()\n","# pretrained_net.load_state_dict(torch.load(pretrained_weight_path))\n","# torch.save(pretrained_net.state_dict(), weigth_path)"]},{"cell_type":"code","execution_count":null,"id":"7f5db558-2318-4b20-8385-a775144067cd","metadata":{"id":"7f5db558-2318-4b20-8385-a775144067cd"},"outputs":[],"source":["net = AgentNet()\n","pretrained_net = AgentNet()\n","pretrained_net.load_state_dict(torch.load(pretrained_weight_path))\n","net.cnn.load_state_dict(pretrained_net.cnn.state_dict())\n","torch.save(net.state_dict(), weight_path)\n","optimizer = optim.AdamW(net.mlp.parameters(), lr=0.001)"]},{"cell_type":"code","execution_count":null,"id":"4fea4551-3f68-4965-bf24-adca6abf13f8","metadata":{"id":"4fea4551-3f68-4965-bf24-adca6abf13f8"},"outputs":[],"source":["optimizer = optim.AdamW(net.mlp.parameters(), lr=0.001)"]},{"cell_type":"markdown","id":"81e02d0f","metadata":{"id":"81e02d0f"},"source":["EVENT_HORIZON = {\n","    MOVED_LEFT: 1,\n","    MOVED_RIGHT: 1,\n","    MOVED_UP: 1,\n","    MOVED_DOWN: 1,\n","    WAITED: 1,\n","    INVALID_ACTION: 1,\n","    BOMB_DROPPED: 1,\n","    BOMB_EXPLODED: 1,\n","    CRATE_DESTROYED: 5,\n","    COIN_FOUND: 3,\n","    COIN_COLLECTED: 8,\n","    KILLED_OPPONENT: 8,\n","    KILLED_SELF: 4,\n","    GOT_KILLED: 3,\n","    OPPONENT_ELIMINATED: 7,\n","    SURVIVED_ROUND: 5\n","}\n","\n","\n","REWARDS = {\n","    MOVED_LEFT: -0.5,  #-0.2\n","    MOVED_RIGHT: -0.5, #-0.2\n","    MOVED_UP: -0.5,  #-0.2\n","    MOVED_DOWN: -0.5,  #-0.2    \n","    WAITED: 4,\n","    INVALID_ACTION: -2,\n","    BOMB_DROPPED: 0,\n","    BOMB_EXPLODED: 1,\n","    CRATE_DESTROYED: 2,\n","    COIN_FOUND: 0,\n","    COIN_COLLECTED: 6,\n","    KILLED_OPPONENT: 1,\n","    KILLED_SELF: -10,\n","    GOT_KILLED: -1,\n","    OPPONENT_ELIMINATED: 2,\n","    SURVIVED_ROUND: 9\n","}"]},{"cell_type":"code","execution_count":null,"id":"77764d1d","metadata":{"id":"77764d1d"},"outputs":[],"source":["action_probs = np.zeros((6,500))\n"]},{"cell_type":"code","execution_count":null,"id":"7ee713ae-2c19-4c00-9eb0-daf9d05fd540","metadata":{"id":"7ee713ae-2c19-4c00-9eb0-daf9d05fd540","outputId":"4cf9e8b1-a188-4849-9bde-0ff12986f84e"},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 30/30 [00:58<00:00,  1.94s/it]\n"]},{"name":"stdout","output_type":"stream","text":["Loaded 298683 actions.\n","298683 actions in the training set:\n","UP: 15.22% (45452)\n","DOWN: 34.32% (102520)\n","LEFT: 16.40% (48982)\n","RIGHT: 26.48% (79098)\n","BOMB: 6.45% (19274)\n","WAIT: 1.12% (3357)\n","[1, 1,   200] loss: 20.545\n","[1, 1,   400] loss: 17.363\n","[1, 1,   600] loss: 15.881\n","[1, 1,   800] loss: 15.734\n","[1, 1,  1000] loss: 14.239\n","[1, 1,  1200] loss: 13.750\n","[1, 1,  1400] loss: 13.503\n","[1, 1,  1600] loss: 13.106\n","[1, 1,  1800] loss: 12.162\n","[1, 1,  2000] loss: 12.178\n","[1, 1,  2200] loss: 11.820\n","[1, 1,  2400] loss: 11.852\n","[1, 1,  2600] loss: 11.281\n","[1, 1,  2800] loss: 11.148\n","[1, 1,  3000] loss: 10.654\n","[1, 1,  3200] loss: 11.071\n","[1, 1,  3400] loss: 10.446\n","[1, 1,  3600] loss: 10.402\n","[1, 1,  3800] loss: 9.904\n","[1, 1,  4000] loss: 9.635\n","[1, 1,  4200] loss: 9.458\n","[1, 1,  4400] loss: 9.511\n","[1, 1,  4600] loss: 9.639\n","[1, 1] loss: 9.649\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 30/30 [00:58<00:00,  1.94s/it]\n"]},{"name":"stdout","output_type":"stream","text":["Loaded 275019 actions.\n","275019 actions in the training set:\n","UP: 14.84% (40818)\n","DOWN: 35.45% (97483)\n","LEFT: 16.15% (44411)\n","RIGHT: 26.31% (72357)\n","BOMB: 6.03% (16597)\n","WAIT: 1.22% (3353)\n","[2, 1,   200] loss: 15.642\n","[2, 1,   400] loss: 12.522\n","[2, 1,   600] loss: 10.814\n","[2, 1,   800] loss: 10.197\n","[2, 1,  1000] loss: 9.779\n","[2, 1,  1200] loss: 8.961\n","[2, 1,  1400] loss: 8.457\n","[2, 1,  1600] loss: 8.409\n","[2, 1,  1800] loss: 8.007\n","[2, 1,  2000] loss: 7.834\n","[2, 1,  2200] loss: 7.641\n","[2, 1,  2400] loss: 7.661\n","[2, 1,  2600] loss: 7.253\n","[2, 1,  2800] loss: 6.938\n","[2, 1,  3000] loss: 6.859\n","[2, 1,  3200] loss: 7.091\n","[2, 1,  3400] loss: 7.851\n","[2, 1,  3600] loss: 6.596\n","[2, 1,  3800] loss: 7.056\n","[2, 1,  4000] loss: 6.786\n","[2, 1,  4200] loss: 6.336\n","[2, 1] loss: 5.841\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 30/30 [01:02<00:00,  2.07s/it]\n"]},{"name":"stdout","output_type":"stream","text":["Loaded 296943 actions.\n","296943 actions in the training set:\n","UP: 14.74% (43777)\n","DOWN: 34.50% (102442)\n","LEFT: 15.75% (46779)\n","RIGHT: 27.90% (82858)\n","BOMB: 5.79% (17200)\n","WAIT: 1.31% (3887)\n","[3, 1,   200] loss: 13.117\n","[3, 1,   400] loss: 10.529\n","[3, 1,   600] loss: 8.514\n","[3, 1,   800] loss: 7.821\n","[3, 1,  1000] loss: 7.589\n","[3, 1,  1200] loss: 7.152\n","[3, 1,  1400] loss: 6.555\n","[3, 1,  1600] loss: 6.769\n","[3, 1,  1800] loss: 6.426\n","[3, 1,  2000] loss: 6.444\n","[3, 1,  2200] loss: 6.472\n","[3, 1,  2400] loss: 6.159\n","[3, 1,  2600] loss: 5.700\n","[3, 1,  2800] loss: 6.195\n","[3, 1,  3000] loss: 6.146\n","[3, 1,  3200] loss: 6.141\n","[3, 1,  3400] loss: 5.716\n","[3, 1,  3600] loss: 5.535\n","[3, 1,  3800] loss: 5.579\n","[3, 1,  4000] loss: 5.579\n","[3, 1,  4200] loss: 6.154\n","[3, 1,  4400] loss: 5.372\n","[3, 1,  4600] loss: 5.222\n","[3, 1] loss: 6.239\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 30/30 [00:53<00:00,  1.80s/it]\n"]},{"name":"stdout","output_type":"stream","text":["Loaded 288673 actions.\n","288673 actions in the training set:\n","UP: 14.39% (41553)\n","DOWN: 41.43% (119600)\n","LEFT: 15.19% (43854)\n","RIGHT: 22.47% (64863)\n","BOMB: 5.37% (15504)\n","WAIT: 1.14% (3299)\n","[4, 1,   200] loss: 10.355\n","[4, 1,   400] loss: 8.255\n","[4, 1,   600] loss: 7.762\n","[4, 1,   800] loss: 6.706\n","[4, 1,  1000] loss: 6.530\n","[4, 1,  1200] loss: 6.148\n","[4, 1,  1400] loss: 5.509\n","[4, 1,  1600] loss: 5.585\n","[4, 1,  1800] loss: 5.632\n","[4, 1,  2000] loss: 5.073\n","[4, 1,  2200] loss: 4.864\n","[4, 1,  2400] loss: 5.536\n","[4, 1,  2600] loss: 5.197\n","[4, 1,  2800] loss: 4.650\n","[4, 1,  3000] loss: 5.106\n","[4, 1,  3200] loss: 5.531\n","[4, 1,  3400] loss: 4.926\n","[4, 1,  3600] loss: 4.624\n","[4, 1,  3800] loss: 4.378\n","[4, 1,  4000] loss: 4.987\n","[4, 1,  4200] loss: 4.764\n","[4, 1,  4400] loss: 4.636\n","[4, 1] loss: 4.888\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 30/30 [00:52<00:00,  1.76s/it]\n"]},{"name":"stdout","output_type":"stream","text":["Loaded 278948 actions.\n","278948 actions in the training set:\n","UP: 13.04% (36378)\n","DOWN: 39.66% (110624)\n","LEFT: 14.77% (41196)\n","RIGHT: 26.79% (74744)\n","BOMB: 4.77% (13313)\n","WAIT: 0.97% (2693)\n","[5, 1,   200] loss: 11.025\n","[5, 1,   400] loss: 8.949\n","[5, 1,   600] loss: 7.630\n","[5, 1,   800] loss: 7.161\n","[5, 1,  1000] loss: 6.631\n","[5, 1,  1200] loss: 6.287\n","[5, 1,  1400] loss: 6.397\n","[5, 1,  1600] loss: 5.681\n","[5, 1,  1800] loss: 6.000\n","[5, 1,  2000] loss: 6.066\n","[5, 1,  2200] loss: 5.989\n","[5, 1,  2400] loss: 5.975\n","[5, 1,  2600] loss: 5.652\n","[5, 1,  2800] loss: 5.766\n","[5, 1,  3000] loss: 5.277\n","[5, 1,  3200] loss: 5.607\n","[5, 1,  3400] loss: 5.522\n","[5, 1,  3600] loss: 5.495\n","[5, 1,  3800] loss: 5.514\n","[5, 1,  4000] loss: 5.363\n","[5, 1,  4200] loss: 5.473\n","[5, 1] loss: 5.809\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 30/30 [00:51<00:00,  1.71s/it]\n"]},{"name":"stdout","output_type":"stream","text":["Loaded 261623 actions.\n","261623 actions in the training set:\n","UP: 15.45% (40432)\n","DOWN: 39.18% (102493)\n","LEFT: 14.66% (38343)\n","RIGHT: 23.93% (62594)\n","BOMB: 5.39% (14107)\n","WAIT: 1.40% (3654)\n","[6, 1,   200] loss: 10.541\n","[6, 1,   400] loss: 8.960\n","[6, 1,   600] loss: 7.116\n","[6, 1,   800] loss: 6.318\n","[6, 1,  1000] loss: 6.269\n","[6, 1,  1200] loss: 5.886\n","[6, 1,  1400] loss: 6.083\n","[6, 1,  1600] loss: 5.624\n","[6, 1,  1800] loss: 5.623\n","[6, 1,  2000] loss: 5.538\n","[6, 1,  2200] loss: 5.588\n","[6, 1,  2400] loss: 5.382\n","[6, 1,  2600] loss: 5.600\n","[6, 1,  2800] loss: 5.469\n","[6, 1,  3000] loss: 5.517\n","[6, 1,  3200] loss: 5.241\n","[6, 1,  3400] loss: 5.374\n","[6, 1,  3600] loss: 5.240\n","[6, 1,  3800] loss: 5.201\n","[6, 1,  4000] loss: 5.039\n","[6, 1] loss: 5.375\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 30/30 [00:55<00:00,  1.83s/it]\n"]},{"name":"stdout","output_type":"stream","text":["Loaded 266770 actions.\n","266770 actions in the training set:\n","UP: 15.18% (40502)\n","DOWN: 41.63% (111067)\n","LEFT: 13.93% (37161)\n","RIGHT: 22.58% (60247)\n","BOMB: 5.43% (14481)\n","WAIT: 1.24% (3312)\n","[7, 1,   200] loss: 10.172\n","[7, 1,   400] loss: 8.596\n","[7, 1,   600] loss: 7.097\n","[7, 1,   800] loss: 6.640\n","[7, 1,  1000] loss: 6.194\n","[7, 1,  1200] loss: 6.160\n","[7, 1,  1400] loss: 5.763\n","[7, 1,  1600] loss: 5.387\n","[7, 1,  1800] loss: 5.189\n","[7, 1,  2000] loss: 5.195\n","[7, 1,  2200] loss: 5.018\n","[7, 1,  2400] loss: 5.369\n","[7, 1,  2600] loss: 4.919\n","[7, 1,  2800] loss: 5.100\n","[7, 1,  3000] loss: 5.002\n","[7, 1,  3200] loss: 5.368\n","[7, 1,  3400] loss: 4.856\n","[7, 1,  3600] loss: 5.708\n","[7, 1,  3800] loss: 5.186\n","[7, 1,  4000] loss: 4.962\n","[7, 1] loss: 4.603\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 30/30 [00:52<00:00,  1.74s/it]\n"]},{"name":"stdout","output_type":"stream","text":["Loaded 272686 actions.\n","272686 actions in the training set:\n","UP: 14.62% (39861)\n","DOWN: 41.77% (113900)\n","LEFT: 11.12% (30333)\n","RIGHT: 26.36% (71867)\n","BOMB: 4.95% (13500)\n","WAIT: 1.18% (3225)\n","[8, 1,   200] loss: 8.618\n","[8, 1,   400] loss: 7.104\n","[8, 1,   600] loss: 6.021\n","[8, 1,   800] loss: 5.576\n","[8, 1,  1000] loss: 5.072\n","[8, 1,  1200] loss: 4.652\n","[8, 1,  1400] loss: 4.711\n","[8, 1,  1600] loss: 4.287\n","[8, 1,  1800] loss: 4.299\n","[8, 1,  2000] loss: 4.500\n","[8, 1,  2200] loss: 4.388\n","[8, 1,  2400] loss: 4.180\n","[8, 1,  2600] loss: 4.070\n","2624\r"]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_28285/2862909088.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     43\u001b[0m             \"\"\"\n\u001b[1;32m     44\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlog_probs\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    485\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m             )\n\u001b[0;32m--> 487\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    488\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m         )\n","\u001b[0;32m~/.local/lib/python3.10/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    198\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    201\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["for run in range(10):\n","    # os.system(\"rm -rf ../rule_based_agent/data\")\n","    os.system(\"rm -rf ../data\")\n","    #os.system(\"cd ../..; python main.py play --no-gui --agents basic_agent basic_agent basic_agent basic_agent --train 4 --n-rounds 30 --scenario loot-crate\")\n","    os.system(\"cd ../..; python main.py play --no-gui --agents basic_agent basic_agent basic_agent basic_agent  --train 4 --n-rounds 30 --scenario loot-crate\")\n","\n","    # os.system(\"cd ../..; python main.py play --no-gui --agents rule_based_agent rule_based_agent rule_based_agent rule_based_agent --train 4 --n-rounds 10 --scenario classic\")\n","\n","    trainset = BombermanDataset(data_path)\n","    N = len(trainset)\n","    print(f\"{N} actions in the training set:\")\n","    for i in range(6):\n","        n = np.sum([t == i for t in trainset.actions])\n","        print(f\"{ACTION_MAP_INV[i]}: {n/N*100:.2f}% ({n})\")\n","        action_probs[i,run] = n/N\n","\n","\n","    # sampler = BatchSampler(WeightedRandomSampler(trainset.weights, len(trainset), replacement=True,), batch_size, False)\n","    # trainloader = torch.utils.data.DataLoader(trainset, batch_sampler=sampler)\n","    trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True)\n","\n","    for epoch in range(num_epoch_per_run):\n","        running_loss = 0.0\n","        running = 0\n","        for i, batch in enumerate(trainloader, 0):\n","        # for i, batch in enumerate(sampler, 0):\n","            print(i, end=\"\\r\")\n","            # get the inputs; data is a list of [inputs, labels]\n","            (channels, features), actions, rewards = batch\n","\n","            # zero the parameter gradients\n","            optimizer.zero_grad()\n","\n","            # forward + backward + optimize\n","            outputs = net(channels, features)\n","            # loss = criterion(outputs, labels)\n","            log_logits = F.log_softmax(outputs, dim=-1)\n","            log_probs = log_logits.gather(1, actions[:,np.newaxis])\n","            \"\"\"\n","            if i == 0:\n","                print(actions, rewards, log_probs)\n","                print(-log_probs * rewards)\n","            \"\"\"\n","            loss = torch.mean(-log_probs * rewards)\n","            loss.backward()\n","            optimizer.step()\n","\n","            # print statistics\n","            running_loss += loss.item()\n","            running += 1\n","            if i % 200 == 199:    # print every 2000 mini-batches\n","                print(f'[{run + 1}, {epoch + 1}, {i + 1:5d}] loss: {running_loss / running:.3f}')\n","                torch.save(net.state_dict(), weight_path)\n","                running_loss = 0.0\n","                running = 0\n","        if running > 0:\n","            print(f'[{run + 1}, {epoch + 1}] loss: {running_loss / running:.3f}')\n","    torch.save(net.state_dict(), weight_path)\n","\n","print('Finished Training')"]},{"cell_type":"code","execution_count":null,"id":"1e729cf5-51d3-499c-a029-dcc14cc8954c","metadata":{"id":"1e729cf5-51d3-499c-a029-dcc14cc8954c","outputId":"27a64bc7-d85c-4456-dcb7-376ab3503336"},"outputs":[{"name":"stdout","output_type":"stream","text":["[[0. 0. 0. 0. 0. 0. 0.]]\n","[0. 0. 0. 0. 0. 0. 0.]\n","0.0\n"]}],"source":["a = np.zeros((1,7))\n","print(a)\n","print(a[0])\n","print(a[0,0])\n"]},{"cell_type":"code","execution_count":null,"id":"2c53db6b-c671-4001-a638-de64bbee8438","metadata":{"id":"2c53db6b-c671-4001-a638-de64bbee8438","outputId":"f449e07f-8474-468c-eef4-6e8408120a87"},"outputs":[{"name":"stdout","output_type":"stream","text":["[0 1 2 3 4 5 6]\n","[1 2 3 4 5 6]\n"]}],"source":["a = np.array([1,2,3,4,5,6,7])-1\n","print(a)\n","print(a[1:7])"]},{"cell_type":"markdown","id":"bd468b0e","metadata":{"id":"bd468b0e"},"source":["\n","    extensions = np.zeros(4)\n","    if pos[0]+4 <= 16:             #field, box_map, explosion_map, others_map, coin_map\n","        if field[pos[0]+4, pos[1]] == 1:\n","            extensions[0] = 1\n","        if box_map[pos[0]+4, pos[1]] ==1:\n","            extensions[0] = 2\n","        if explosion_map[pos[0]+4, pos[1]] ==1:\n","            extensions[0] = 3\n","        if others_map[pos[0]+4, pos[1]] == 1:\n","            extensions[0] = 4\n","        if coin_map[pos[0]+4, pos[1]] ==1:\n","            extensions[0] =5\n","\n","    if pos[0]-4 >= 0:             #field, box_map, explosion_map, others_map, coin_map\n","        if field[pos[0]-4, pos[1]] == 1:\n","            extensions[1] = 1\n","        if box_map[pos[0]-4, pos[1]] ==1:\n","            extensions[1] = 2\n","        if explosion_map[pos[0]+4, pos[1]] ==1:\n","            extensions[1] = 3\n","        if others_map[pos[0]-4, pos[1]] == 1:\n","            extensions[1] = 4\n","        if coin_map[pos[0]-4, pos[1]] ==1:\n","            extensions[1] =5\n","\n","    if pos[1]+4 <= 16:             #field, box_map, explosion_map, others_map, coin_map\n","        if field[pos[0], pos[1]+4] == 1:\n","            extensions[2] = 1\n","        if box_map[pos[0], pos[1]+4] ==1:\n","            extensions[2] = 2\n","        if explosion_map[pos[0], pos[1]+4] ==1:\n","            extensions[2] = 3\n","        if others_map[pos[0], pos[1]+4] == 1:\n","            extensions[2] = 4\n","        if coin_map[pos[0], pos[1]+4] ==1:\n","            extensions[2] = 5\n","\n","    if pos[1]-4 >= 0:             #field, box_map, explosion_map, others_map, coin_map\n","        if field[pos[0], pos[1]-4] == 1:\n","            extensions[3] = 1\n","        if box_map[pos[0], pos[1]-4] ==1:\n","            extensions[3] = 2\n","        if explosion_map[pos[0], pos[1]-4] ==1:\n","            extensions[3] = 3\n","        if others_map[pos[0], pos[1]-4] == 1:\n","            extensions[3] = 4\n","        if coin_map[pos[0], pos[1]-4] ==1:\n","            extensions[3] = 5\n","\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":5}